
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1/cassandracluster_types.go (68.6%)</option>
				
				<option value="file1">github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1/zz_generated.deepcopy.go (31.1%)</option>
				
				<option value="file2">github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1/zz_generated.openapi.go (0.0%)</option>
				
				<option value="file3">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/cassandra_status.go (46.9%)</option>
				
				<option value="file4">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/cassandra_util.go (33.3%)</option>
				
				<option value="file5">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/cassandracluster_controller.go (63.6%)</option>
				
				<option value="file6">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/deploy_cassandra.go (85.2%)</option>
				
				<option value="file7">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/generator.go (91.7%)</option>
				
				<option value="file8">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/node_operations.go (73.3%)</option>
				
				<option value="file9">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/pod.go (34.6%)</option>
				
				<option value="file10">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/pod_operation.go (12.9%)</option>
				
				<option value="file11">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/poddisruptionbudget.go (42.9%)</option>
				
				<option value="file12">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/pvc.go (0.0%)</option>
				
				<option value="file13">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/reconcile.go (81.3%)</option>
				
				<option value="file14">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/service.go (100.0%)</option>
				
				<option value="file15">github.com/Orange-OpenSource/casskop/pkg/controller/cassandracluster/statefulset.go (37.4%)</option>
				
				<option value="file16">github.com/Orange-OpenSource/casskop/pkg/k8s/client_util.go (0.0%)</option>
				
				<option value="file17">github.com/Orange-OpenSource/casskop/pkg/k8s/util.go (77.4%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package v1alpha1

import (
        "encoding/json"
        "fmt"
        "regexp"
        "strings"

        "github.com/sirupsen/logrus"
        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
        defaultCassandraImage         string        = "cassandra:latest"
        defaultBootstrapImage         string        = "orangeopensource/cassandra-bootstrap:0.1.3"
        InitContainerCmd              string        = "cp -vr /etc/cassandra/* /bootstrap"
        defaultNbMaxConcurrentCleanup               = 2
        defaultMaxPodUnavailable                    = 1
        defaultNumTokens                            = 256
        defaultImagePullPolicy        v1.PullPolicy = v1.PullAlways

        DefaultCassandraDC   string = "dc1"
        DefaultCassandraRack string = "rack1"

        DefaultTerminationGracePeriodSeconds = 1800

        //DefaultDelayWait: wait 20 seconds (2x resyncPeriod) prior to follow status of an operation
        DefaultResyncPeriod = 10
        DefaultDelayWait    = 2 * DefaultResyncPeriod

        //DefaultDelayWaitForDecommission is the time to wait for the decommission to happen on the Pod
        //The operator will start again if it is not the case
        DefaultDelayWaitForDecommission = 120

        //DefaultUserID is the default ID to use in cassandra image (RunAsUser)
        DefaultUserID int64 = 999
)

const (
        AnnotationLastApplied string = "cassandraclusters.db.orange.com/last-applied-configuration"
        //Phase du Cluster
        ClusterPhaseInitial string = "Initializing"
        ClusterPhaseRunning string = "Running"
        ClusterPhasePending string = "Pending"

        StatusOngoing     string = "Ongoing"    // The Action is Ongoing
        StatusDone        string = "Done"       // The Action id Done
        StatusToDo        string = "ToDo"       // The Action is marked as To-Do
        StatusFinalizing  string = "Finalizing" // The Action is between Ongoing and Done
        StatusContinue    string = "Continue"
        StatusConfiguring string = "Configuring"
        StatusManual      string = "Manual"
        StatusError       string = "Error"

        //Available actions
        ActionUpdateConfigMap   string = "UpdateConfigMap"
        ActionUpdateDockerImage string = "UpdateDockerImage"
        ActionUpdateSeedList    string = "UpdateSeedList"
        ActionRollingRestart    string = "RollingRestart"
        ActionUpdateResources   string = "UpdateResources"
        ActionUpdateStatefulSet string = "UpdateStatefulSet"
        ActionScaleUp           string = "ScaleUp"
        ActionScaleDown         string = "ScaleDown"

        ActionDeleteDC   string = "ActionDeleteDC"
        ActionDeleteRack string = "ActionDeleteRack"

        ActionCorrectCRDConfig string = "CorrectCRDConfig" //The Operator has correct a bad CRD configuration

        //List of Pods Operations
        OperationUpgradeSSTables string = "upgradesstables"
        OperationCleanup         string = "cleanup"
        OperationDecommission    string = "decommission"
        OperationRebuild         string = "rebuild"
        OperationRemove          string = "remove"

        BreakResyncLoop    = true
        ContinueResyncLoop = false
)

// CheckDefaults checks that required fields haven't good values
func (cc *CassandraCluster) CheckDefaults() <span class="cov8" title="1">{
        ccs := &amp;cc.Spec

        if len(ccs.CassandraImage) == 0 </span><span class="cov8" title="1">{
                ccs.CassandraImage = defaultCassandraImage
        }</span>

        <span class="cov8" title="1">if len(ccs.ImagePullPolicy) == 0 </span><span class="cov8" title="1">{
                ccs.ImagePullPolicy = defaultImagePullPolicy
        }</span>
        <span class="cov8" title="1">if len(ccs.BootstrapImage) == 0 </span><span class="cov8" title="1">{
                ccs.BootstrapImage = defaultBootstrapImage
        }</span>

        //Init-Container 1 : init-config
        <span class="cov8" title="1">if len(ccs.InitContainerImage) == 0 </span><span class="cov8" title="1">{
                ccs.InitContainerImage = ccs.CassandraImage
        }</span>
        <span class="cov8" title="1">if len(ccs.InitContainerCmd) == 0 </span><span class="cov8" title="1">{
                ccs.InitContainerCmd = InitContainerCmd
        }</span>

        <span class="cov8" title="1">if ccs.RunAsUser == nil </span><span class="cov8" title="1">{
                ccs.RunAsUser = func(i int64) *int64 </span><span class="cov8" title="1">{ return &amp;i }</span>(DefaultUserID)
        }
        <span class="cov8" title="1">if ccs.ReadOnlyRootFilesystem == nil </span><span class="cov8" title="1">{
                ccs.ReadOnlyRootFilesystem = func(b bool) *bool </span><span class="cov8" title="1">{ return &amp;b }</span>(true)
        }
}

// SetDefaults sets the default values for the cassandra spec and returns true if the spec was changed
// SetDefault mus be done only once at startup
func (cc *CassandraCluster) SetDefaults() bool <span class="cov8" title="1">{
        changed := false
        ccs := &amp;cc.Spec
        if ccs.NodesPerRacks == 0 </span><span class="cov8" title="1">{
                ccs.NodesPerRacks = 1
                changed = true
        }</span>
        <span class="cov8" title="1">if len(cc.Status.Phase) == 0 </span><span class="cov8" title="1">{
                cc.Status.Phase = ClusterPhaseInitial
                if cc.InitCassandraRackList() &lt; 1 </span><span class="cov0" title="0">{
                        logrus.Errorf("[%s]: We should have at list One Rack, Please correct the Error", cc.Name)
                }</span>
                <span class="cov8" title="1">if cc.Status.SeedList == nil </span><span class="cov8" title="1">{
                        cc.Status.SeedList = cc.InitSeedList()
                }</span>
                <span class="cov8" title="1">changed = true</span>
        }
        <span class="cov8" title="1">if ccs.MaxPodUnavailable == 0 </span><span class="cov8" title="1">{
                ccs.MaxPodUnavailable = defaultMaxPodUnavailable
        }</span>
        <span class="cov8" title="1">if cc.Spec.Resources.Limits == (CPUAndMem{}) </span><span class="cov8" title="1">{
                cc.Spec.Resources.Limits = cc.Spec.Resources.Requests
                changed = true
        }</span>

        <span class="cov8" title="1">return changed</span>
}

func (cc *CassandraCluster) ComputeLastAppliedConfiguration() ([]byte, error) <span class="cov8" title="1">{
        lastcc := cc.DeepCopy()
        //remove unnecessary fields
        lastcc.Annotations = nil
        lastcc.ResourceVersion = ""
        lastcc.Status = CassandraClusterStatus{}

        lastApplied, err := json.Marshal(lastcc)
        if err != nil </span><span class="cov0" title="0">{
                logrus.Errorf("[%s]: Cannot create last-applied-configuration = %v", cc.Name, err)
        }</span>
        <span class="cov8" title="1">return lastApplied, err</span>
}

//GetDCSize Return the Numbers of declared DC
func (cc *CassandraCluster) GetDCSize() int <span class="cov8" title="1">{
        return len(cc.Spec.Topology.DC)
}</span>

func (cc *CassandraCluster) GetDCRackSize() int <span class="cov8" title="1">{
        var nb int = 0
        dcsize := cc.GetDCSize()
        for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov8" title="1">{
                nb += cc.GetRackSize(dc)
        }</span>
        <span class="cov8" title="1">return nb</span>
}

func (cc *CassandraCluster) GetStatusDCRackSize() int <span class="cov8" title="1">{
        return len(cc.Status.CassandraRackStatus)
}</span>

//GetDCName return the name of the DC a indice dc
//or defaultName
func (cc *CassandraCluster) GetDCName(dc int) string <span class="cov8" title="1">{
        if dc &gt;= cc.GetDCSize() </span><span class="cov0" title="0">{
                return DefaultCassandraDC
        }</span>
        <span class="cov8" title="1">return cc.Spec.Topology.DC[dc].Name</span>
}

func (cc *CassandraCluster) getDCNodesPerRacksFromIndex(dc int) int32 <span class="cov8" title="1">{
        if dc &gt;= cc.GetDCSize() </span><span class="cov0" title="0">{
                return cc.Spec.NodesPerRacks
        }</span>
        <span class="cov8" title="1">storeDC := cc.Spec.Topology.DC[dc]
        if storeDC.NodesPerRacks == nil </span><span class="cov8" title="1">{
                return cc.Spec.NodesPerRacks
        }</span>
        <span class="cov8" title="1">return *storeDC.NodesPerRacks</span>
}

func (cc *CassandraCluster) getDCNumTokensPerRacksFromIndex(dc int) int32 <span class="cov8" title="1">{
        if dc &gt;= cc.GetDCSize() </span><span class="cov0" title="0">{
                return defaultNumTokens
        }</span>
        <span class="cov8" title="1">storeDC := cc.Spec.Topology.DC[dc]
        if storeDC.NumTokens == nil </span><span class="cov0" title="0">{
                return defaultNumTokens
        }</span>
        <span class="cov8" title="1">return *storeDC.NumTokens</span>
}

//GetRackSize return the numbers of the Rack in the DC at indice dc
func (cc *CassandraCluster) GetRackSize(dc int) int <span class="cov8" title="1">{
        if dc &gt;= cc.GetDCSize() </span><span class="cov0" title="0">{
                return 0
        }</span>
        <span class="cov8" title="1">return len(cc.Spec.Topology.DC[dc].Rack)</span>
}

//GetRackName return the Name of the rack for DC at indice dc and Rack at indice rack
func (cc *CassandraCluster) GetRackName(dc int, rack int) string <span class="cov8" title="1">{
        if dc &gt;= cc.GetDCSize() </span><span class="cov0" title="0">{
                return DefaultCassandraRack
        }</span>
        <span class="cov8" title="1">if rack &gt;= cc.GetRackSize(dc) </span><span class="cov0" title="0">{
                return DefaultCassandraRack
        }</span>
        <span class="cov8" title="1">return cc.Spec.Topology.DC[dc].Rack[rack].Name</span>
}

// GetDCRackName compute dcName + RackName to be used in statefulsets, services..
// it return empty if the name don't match with kubernetes domain name validation regexp
func (cc *CassandraCluster) GetDCRackName(dcName string, rackName string) string <span class="cov8" title="1">{
        var dcRackName string
        dcRackName = dcName + "-" + rackName
        var regex_name = regexp.MustCompile("^[a-z]([-a-z0-9]*[a-z0-9])?$")
        if !regex_name.MatchString(dcRackName) </span><span class="cov8" title="1">{
                logrus.Errorf("%s don't match valide name service: a DNS-1035 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character", dcRackName)
                return ""
        }</span>
        <span class="cov8" title="1">return dcRackName</span>
}

//GetDCFromDCRackName send dc name from dcRackName (dc-rack)
func (cc *CassandraCluster) GetDCFromDCRackName(dcRackName string) string <span class="cov0" title="0">{
        dc, _ := cc.GetDCAndRackFromDCRackName(dcRackName)
        return dc
}</span>

//GetDCAndRackFromDCRackName send dc and rack from dcRackName (dc-rack)
func (cc *CassandraCluster) GetDCAndRackFromDCRackName(dcRackName string) (string, string) <span class="cov0" title="0">{
        dc := strings.Split(dcRackName, "-")
        return dc[0], dc[1]
}</span>

// initTopology Initialisation of topology section in CRD
func (cc *CassandraCluster) initTopology(dcName string, rackName string) <span class="cov8" title="1">{
        cc.Spec.Topology = Topology{
                DC: []DC{
                        DC{
                                Name: dcName,
                                Rack: []Rack{
                                        Rack{
                                                Name: rackName,
                                        },
                                },
                        },
                },
        }
}</span>

// InitCassandraRack Initialisation of a CassandraRack Structure which is appended to the CRD status
func (cc *CassandraCluster) initCassandraRack(dcName string, rackName string) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        var rackStatus = CassandraRackStatus{
                Phase: ClusterPhaseInitial,
                CassandraLastAction: CassandraLastAction{
                        Name:   ClusterPhaseInitial,
                        Status: StatusOngoing,
                },
        }

        //The key of each CassandraRackStatus is the name of "&lt;dcName&gt;-&lt;rackName&gt;"
        cc.Status.CassandraRackStatus[dcRackName] = &amp;rackStatus
}</span>

// InitCassandraRack Initialisation of a CassandraRack Structure which is appended to the CRD status
// In this method we create it in status var instead of directly in cc object
// This is because except for init the cc, ca always work with a separate status which updates the cc
// in a defer statement in Reconcile method
func (cc *CassandraCluster) InitCassandraRackinStatus(status *CassandraClusterStatus, dcName string, rackName string) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        var rackStatus CassandraRackStatus = CassandraRackStatus{
                Phase: ClusterPhaseInitial,
                CassandraLastAction: CassandraLastAction{
                        Name:   ClusterPhaseInitial,
                        Status: StatusOngoing,
                },
        }

        //The key of each CassandraRackStatus is the name of "&lt;dcName&gt;-&lt;rackName&gt;"
        status.CassandraRackStatus[dcRackName] = &amp;rackStatus
}</span>

// Initialisation of the Cassandra SeedList
// We want 3 seed nodes for each DC
func (cc *CassandraCluster) InitSeedList() []string <span class="cov8" title="1">{

        var dcName, rackName string
        var nbRack int = 0
        var indice int32
        var seedList []string

        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                dcName = DefaultCassandraDC
                rackName = DefaultCassandraRack
                nbRack++
                for indice = 0; indice &lt; cc.Spec.NodesPerRacks &amp;&amp; indice &lt; 3; indice++ </span><span class="cov0" title="0">{
                        cc.addNewSeed(&amp;seedList, dcName, rackName, indice)
                }</span>
        } else<span class="cov8" title="1"> {
                for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov8" title="1">{
                        dcName = cc.GetDCName(dc)
                        var nbSeedInDC int = 0

                        racksize := cc.GetRackSize(dc)
                        if racksize &lt; 1 </span><span class="cov0" title="0">{
                                rackName = DefaultCassandraRack
                                nbRack++
                                for indice = 0; indice &lt; cc.Spec.NodesPerRacks &amp;&amp; indice &lt; 3; indice++ </span><span class="cov0" title="0">{
                                        cc.addNewSeed(&amp;seedList, dcName, rackName, indice)
                                }</span>
                        } else<span class="cov8" title="1"> {

                                for rack := 0; rack &lt; racksize; rack++ </span><span class="cov8" title="1">{
                                        rackName = cc.GetRackName(dc, rack)
                                        dcRackName := cc.GetDCRackName(dcName, rackName)
                                        nbRack++
                                        nodesPerRacks := cc.GetNodesPerRacks(dcRackName)

                                        switch racksize </span>{
                                        case 1:<span class="cov8" title="1">
                                                for indice = 0; indice &lt; nodesPerRacks &amp;&amp; indice &lt; 3 &amp;&amp; nbSeedInDC &lt; 3; indice++ </span><span class="cov8" title="1">{
                                                        cc.addNewSeed(&amp;seedList, dcName, rackName, indice)
                                                        nbSeedInDC++
                                                }</span>
                                        case 2:<span class="cov8" title="1">
                                                for indice = 0; indice &lt; nodesPerRacks &amp;&amp; indice &lt; 2 &amp;&amp; nbSeedInDC &lt; 3; indice++ </span><span class="cov8" title="1">{
                                                        cc.addNewSeed(&amp;seedList, dcName, rackName, indice)
                                                        nbSeedInDC++
                                                }</span>
                                        default:<span class="cov8" title="1">
                                                if nbSeedInDC &lt; 3 </span><span class="cov8" title="1">{
                                                        cc.addNewSeed(&amp;seedList, dcName, rackName, 0)
                                                        nbSeedInDC++
                                                }</span>
                                        }

                                }
                        }
                }
        }
        <span class="cov8" title="1">return seedList</span>
}

func (cc *CassandraCluster) GetSeedList(seedListTab *[]string) string <span class="cov8" title="1">{
        seedList := strings.Join(*seedListTab, ",")
        return seedList
}</span>

func (cc *CassandraCluster) addNewSeed(seedList *[]string, dcName string, rackName string, indice int32) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        seed := fmt.Sprintf("%s-%s-%d.%s.%s", cc.Name, dcRackName, indice, cc.Name, cc.Namespace)
        *seedList = append(*seedList, seed)
}</span>

func (cc *CassandraCluster) IsPodInSeedList(podName string) bool <span class="cov8" title="1">{
        for i := range cc.Status.SeedList </span><span class="cov8" title="1">{
                if cc.Status.SeedList[i] == podName </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

//FixCassandraRackList will remove additional rack-list that don't exists anymore in Topology
//we recalculate new dcrackStatus from actual topology and we apply diff to original
func (cc *CassandraCluster) FixCassandraRackList(status *CassandraClusterStatus) []string <span class="cov8" title="1">{
        newcc := cc.DeepCopy()
        newcc.InitCassandraRackList()

        rackList := []string{}
        for dcRackName := range cc.Status.CassandraRackStatus </span><span class="cov8" title="1">{
                if _, ok := newcc.Status.CassandraRackStatus[dcRackName]; !ok </span><span class="cov8" title="1">{
                        //The item does not exists anymore
                        //we need to remove it
                        delete(status.CassandraRackStatus, dcRackName)
                        rackList = append(rackList, dcRackName)
                }</span>
        }
        <span class="cov8" title="1">return rackList</span>
}

func (cc *CassandraCluster) GetRemovedDCName(oldCRD *CassandraCluster) string <span class="cov0" title="0">{
        //dcsize := cc.GetDCSize()
        olddcsize := oldCRD.GetDCSize()

        for dc := 0; dc &lt; olddcsize; dc++ </span><span class="cov0" title="0">{
                olddcName := oldCRD.GetDCName(dc)
                dcName := cc.GetDCName(dc)
                if olddcName != dcName </span><span class="cov0" title="0">{
                        return olddcName
                }</span>
        }
        <span class="cov0" title="0">return ""</span>
}

//InitCassandraRackList initiate the Status structure for CassandraRack
func (cc *CassandraCluster) InitCassandraRackList() int <span class="cov8" title="1">{
        var dcName, rackName string
        var nbRack int = 0

        cc.Status.CassandraRackStatus = make(map[string]CassandraRackStatusPtr)
        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov8" title="1">{
                dcName = DefaultCassandraDC
                rackName = DefaultCassandraRack
                nbRack++
                cc.initCassandraRack(dcName, rackName)
                cc.initTopology(dcName, rackName)
        }</span> else<span class="cov8" title="1"> {
                for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov8" title="1">{
                        dcName = cc.GetDCName(dc)
                        racksize := cc.GetRackSize(dc)
                        if racksize &lt; 1 </span><span class="cov8" title="1">{
                                rackName = DefaultCassandraRack
                                nbRack++
                                cc.initCassandraRack(dcName, rackName)
                                cc.initTopology(dcName, rackName)
                        }</span> else<span class="cov8" title="1"> {

                                for rack := 0; rack &lt; racksize; rack++ </span><span class="cov8" title="1">{
                                        rackName = cc.GetRackName(dc, rack)
                                        nbRack++
                                        cc.initCassandraRack(dcName, rackName)
                                }</span>
                        }
                }

        }
        <span class="cov8" title="1">return nbRack</span>
}

// GetNodesPerRacks sends back the number of cassandra nodes to uses for this dc-rack
func (cc *CassandraCluster) GetNodesPerRacks(dcRackName string) int32 <span class="cov8" title="1">{
        nodesPerRacks := cc.GetDCNodesPerRacksFromDCRackName(dcRackName)
        return nodesPerRacks
}</span>

//GetDCNodesPerRacksFromDCRackName send NodesPerRack used for the given dcRackName
func (cc *CassandraCluster) GetDCRackNames() []string <span class="cov0" title="0">{
        dcsize := cc.GetDCSize()

        var dcRackNames = []string{}
        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                return dcRackNames
        }</span>
        <span class="cov0" title="0">for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov0" title="0">{
                dcName := cc.GetDCName(dc)
                racksize := cc.GetRackSize(dc)
                if racksize &lt; 1 </span><span class="cov0" title="0">{
                        return dcRackNames
                }</span>
                <span class="cov0" title="0">for rack := 0; rack &lt; racksize; rack++ </span><span class="cov0" title="0">{
                        rackName := cc.GetRackName(dc, rack)
                        dcRackNames = append(dcRackNames, cc.GetDCRackName(dcName, rackName))
                }</span>
        }
        <span class="cov0" title="0">return dcRackNames</span>
}

//GetDCNodesPerRacksFromDCRackName send NodesPerRack used for the given dcRackName
func (cc *CassandraCluster) GetDCNodesPerRacksFromDCRackName(dcRackName string) int32 <span class="cov8" title="1">{
        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                return cc.Spec.NodesPerRacks
        }</span>
        <span class="cov8" title="1">for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov8" title="1">{
                dcName := cc.GetDCName(dc)
                racksize := cc.GetRackSize(dc)
                if racksize &lt; 1 </span><span class="cov0" title="0">{
                        return cc.Spec.NodesPerRacks
                }</span>
                <span class="cov8" title="1">for rack := 0; rack &lt; racksize; rack++ </span><span class="cov8" title="1">{
                        rackName := cc.GetRackName(dc, rack)
                        if dcRackName == cc.GetDCRackName(dcName, rackName) </span><span class="cov8" title="1">{
                                return cc.getDCNodesPerRacksFromIndex(dc)
                        }</span>
                }
        }
        <span class="cov8" title="1">return cc.Spec.NodesPerRacks</span>
}

// GetNodesPerRacks sends back the number of cassandra nodes to uses for this dc-rack
func (cc *CassandraCluster) GetNumTokensPerRacks(dcRackName string) int32 <span class="cov8" title="1">{
        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                return defaultNumTokens
        }</span>
        <span class="cov8" title="1">for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov8" title="1">{
                dcName := cc.GetDCName(dc)
                racksize := cc.GetRackSize(dc)
                if racksize &lt; 1 </span><span class="cov0" title="0">{
                        return defaultNumTokens
                }</span>
                <span class="cov8" title="1">for rack := 0; rack &lt; racksize; rack++ </span><span class="cov8" title="1">{
                        rackName := cc.GetRackName(dc, rack)
                        if dcRackName == cc.GetDCRackName(dcName, rackName) </span><span class="cov8" title="1">{
                                return cc.getDCNumTokensPerRacksFromIndex(dc)
                        }</span>
                }
        }
        <span class="cov8" title="1">return defaultNumTokens</span>
}

// GetRollingPartitionPerRacks return rollingPartition defined in spec.topology.dc[].rack[].rollingPartition
func (cc *CassandraCluster) GetRollingPartitionPerRacks(dcRackName string) int32 <span class="cov0" title="0">{
        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                return 0
        }</span>
        <span class="cov0" title="0">for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov0" title="0">{
                dcName := cc.GetDCName(dc)
                racksize := cc.GetRackSize(dc)
                if racksize &lt; 1 </span><span class="cov0" title="0">{
                        return 0
                }</span>
                <span class="cov0" title="0">for rack := 0; rack &lt; racksize; rack++ </span><span class="cov0" title="0">{
                        rackName := cc.GetRackName(dc, rack)
                        if dcRackName == cc.GetDCRackName(dcName, rackName) </span><span class="cov0" title="0">{
                                return cc.Spec.Topology.DC[dc].Rack[rack].RollingPartition
                        }</span>
                }
        }
        <span class="cov0" title="0">return 0</span>
}

//GetDCNodesPerRacksFromName send NodesPerRack which is applied for the specified dc name
//return true if we found, and false if not
func (cc *CassandraCluster) GetDCNodesPerRacksFromName(dctarget string) (bool, int32) <span class="cov0" title="0">{
        dcsize := cc.GetDCSize()

        if dcsize &lt; 1 </span><span class="cov0" title="0">{
                return false, cc.Spec.NodesPerRacks
        }</span>
        <span class="cov0" title="0">for dc := 0; dc &lt; dcsize; dc++ </span><span class="cov0" title="0">{
                dcName := cc.GetDCName(dc)
                if dctarget == dcName </span><span class="cov0" title="0">{
                        return true, cc.getDCNodesPerRacksFromIndex(dc)
                }</span>
        }
        <span class="cov0" title="0">return false, cc.Spec.NodesPerRacks</span>
}

//FindDCWithNodesTo0
func (cc *CassandraCluster) FindDCWithNodesTo0() (bool, string, int) <span class="cov0" title="0">{
        for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov0" title="0">{
                if cc.getDCNodesPerRacksFromIndex(dc) == int32(0) </span><span class="cov0" title="0">{
                        dcName := cc.GetDCName(dc)
                        return true, dcName, dc
                }</span>
        }
        <span class="cov0" title="0">return false, "", 0</span>
}

//knownDCs returns list of datacenters
func (cc *CassandraCluster) knownDCs(dcName string) []string <span class="cov0" title="0">{
        var dcList []string
        for _, dc := range cc.Spec.Topology.DC </span><span class="cov0" title="0">{
                dcList = append(dcList, dc.Name)
        }</span>
        <span class="cov0" title="0">return dcList</span>
}

//IsValidDC returns true if dcName is known
func (cc *CassandraCluster) IsValidDC(dcName string) bool <span class="cov0" title="0">{
        for _, dc := range cc.Spec.Topology.DC </span><span class="cov0" title="0">{
                if dc.Name == dcName </span><span class="cov0" title="0">{
                        return true
                }</span>
        }
        <span class="cov0" title="0">return false</span>
}

//Remove elements from DC slice
func (dc *DCSlice) Remove(i int) <span class="cov8" title="1">{
        *dc = append((*dc)[:i], (*dc)[i+1:]...)
}</span>

//Remove elements from Rack slice
func (rack *RackSlice) Remove(i int) <span class="cov0" title="0">{
        *rack = append((*rack)[:i], (*rack)[i+1:]...)
}</span>

//START @TODO Workaround : waiting for PR https://github.com/kubernetes-sigs/controller-tools/pull/317
type CassandraRackStatusPtr *CassandraRackStatus

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraClusterStatus.
func (in *CassandraClusterStatus) DeepCopy() *CassandraClusterStatus <span class="cov8" title="1">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">out := new(CassandraClusterStatus)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraClusterStatus) DeepCopyInto(out *CassandraClusterStatus) <span class="cov8" title="1">{
        *out = *in
        if in.SeedList != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.SeedList, &amp;out.SeedList
                *out = make([]string, len(*in))
                copy(*out, *in)
        }</span>
        <span class="cov8" title="1">if in.CassandraRackStatus != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.CassandraRackStatus, &amp;out.CassandraRackStatus
                *out = make(map[string]CassandraRackStatusPtr, len(*in))
                for key, val := range *in </span><span class="cov8" title="1">{
                        var outVal CassandraRackStatusPtr
                        if val == nil </span><span class="cov0" title="0">{
                                (*out)[key] = nil
                        }</span> else<span class="cov8" title="1"> {
                                in, out := &amp;val, &amp;outVal
                                *out = new(CassandraRackStatus)
                                (**in).DeepCopyInto(*out)
                        }</span>
                        <span class="cov8" title="1">(*out)[key] = outVal</span>
                }
        }
        <span class="cov8" title="1">return</span>
}

//END

// CassandraClusterSpec defines the configuration of CassandraCluster
type CassandraClusterSpec struct {
        // Number of nodes to deploy for a Cassandra deployment in each Racks.
        // Default: 1.
        // If NodesPerRacks = 2 and there is 3 racks, the cluster will have 6 Cassandra Nodes
        NodesPerRacks int32 `json:"nodesPerRacks,omitempty"`

        // Image + version to use for Cassandra
        CassandraImage string `json:"cassandraImage,omitempty"`

        //ImagePullPolicy define the pull policy for C* docker image
        ImagePullPolicy v1.PullPolicy `json:"imagepullpolicy"`

        // Image used for bootstrapping cluster (use the form : base:version)
        BootstrapImage string `json:"bootstrapImage,omitempty"`

        // Command to execute in the initContainer in the targeted image
        InitContainerImage string `json:"initContainerImage,omitempty"`
        // Command to execute in the initContainer in the targeted image
        InitContainerCmd string `json:"initContainerCmd,omitempty"`

        //RunAsUser define the id of the user to run in the Cassandra image
        // +kubebuilder:validation:Minimum=1
        RunAsUser *int64 `json:"runAsUser,omitempty"`

        //Make the pod as Readonly
        ReadOnlyRootFilesystem *bool `json:"readOnlyRootFilesystem,omitempty"`

        // Pod defines the policy for pods owned by cassandra operator.
        // This field cannot be updated once the CR is created.
        //Pod       *PodPolicy         `json:"pod,omitempty"`
        Resources CassandraResources `json:"resources,omitempty"`

        // HardAntiAffinity defines if the PodAntiAffinity of the
        // statefulset has to be hard (it's soft by default)
        HardAntiAffinity bool `json:"hardAntiAffinity,omitempty"`

        Pod *PodPolicy `json:"pod,omitempty"`

        Service *ServicePolicy `json:"service,omitempty"`

        //DeletePVC defines if the PVC must be deleted when the cluster is deleted
        //it is false by default
        DeletePVC bool `json:"deletePVC,omitempty"`

        //Debug is used to surcharge Cassandra pod command to not directly start cassandra but
        //starts an infinite wait to allow user to connect a bash into the pod to make some diagnoses.
        Debug bool `json:"debug,omitempty"`

        //AutoPilot defines if the Operator can fly alone or if we need human action to trigger
        //Actions on specific Cassandra nodes
        //If autoPilot=true, the operator will set labels pod-operation-status=To-Do on Pods which allows him to
        // automatically triggers Action
        //If autoPilot=false, the operator will set labels pod-operation-status=Manual on Pods which won't automatically triggers Action
        AutoPilot          bool `json:"autoPilot,omitempty"`
        NoCheckStsAreEqual bool `json:"noCheckStsAreEqual,omitempty"`

        //GCStdout set the parameter CASSANDRA_GC_STDOUT which configure the JVM -Xloggc: true by default
        GCStdout bool `json:"gcStdout,omitempty" default:"true"`

        //AutoUpdateSeedList defines if the Operator automatically update the SeedList according to new cluster CRD topology
        //by default a boolean is false
        AutoUpdateSeedList bool `json:"autoUpdateSeedList,omitempty"`

        MaxPodUnavailable int32 `json:"maxPodUnavailable"` //Number of MaxPodUnavailable used in the PDB

        //Very special Flag to hack CassKop reconcile loop - use with really good Care
        UnlockNextOperation bool `json:"unlockNextOperation,omitempty"`

        //Define the Capacity for Persistent Volume Claims in the local storage
        // +kubebuilder:validation:Pattern=^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$
        DataCapacity string `json:"dataCapacity,omitempty"`

        //Define StorageClass for Persistent Volume Claims in the local storage.
        DataStorageClass string `json:"dataStorageClass,omitempty"`

        // Deploy or Not Service that provide access to monitoring metrics
        //Exporter bool `json:"exporter,omitempty"`

        // Name of the ConfigMap for Cassandra configuration (cassandra.yaml)
        // If this is empty, operator will uses default cassandra.yaml from the baseImage
        // If this is not empty, operator will uses the cassandra.yaml from the Configmap instead
        ConfigMapName string `json:"configMapName,omitempty"`

        // Name of the secret to uses to authenticate on Docker registries
        // If this is empty, operator do nothing
        // If this is not empty, propagate the imagePullSecrets to the statefulsets
        ImagePullSecret v1.LocalObjectReference `json:"imagePullSecret,omitempty"`

        // JMX Secret if Set is used to set JMX_USER and JMX_PASSWORD
        ImageJolokiaSecret v1.LocalObjectReference `json:"imageJolokiaSecret,omitempty"`

        //Topology to create Cassandra DC and Racks and to target appropriate Kubernetes Nodes
        Topology Topology `json:"topology,omitempty"`
}

// Topology allow to configure the Cassandra Topology according to kubernetes Nodes labels
type Topology struct {
        //List of DC defined in the CassandraCluster
        DC DCSlice `json:"dc,omitempty"`
}

type DCSlice []DC
type RackSlice []Rack

// DC allow to configure Cassandra RC according to kubernetes nodeselector labels
type DC struct {
        //Name of the CassandraDC
        // +kubebuilder:validation:Pattern=^[^-]+$
        Name string `json:"name,omitempty"`
        //Labels used to target Kubernetes nodes
        Labels map[string]string `json:"labels,omitempty"`
        //List of Racks defined in the Cassandra DC
        Rack RackSlice `json:"rack,omitempty"`

        // Number of nodes to deploy for a Cassandra deployment in each Racks.
        // Default: 1.
        // Optional, if not filled, used value define in CassandraClusterSpec
        NodesPerRacks *int32 `json:"nodesPerRacks,omitempty"`

        //NumTokens : configure the CASSANDRA_NUM_TOKENS parameter which can be different for each DD
        NumTokens *int32 `json:"numTokens,omitempty"`

}

// Rack allow to configure Cassandra Rack according to kubernetes nodeselector labels
type Rack struct {
        //Name of the Rack
        // +kubebuilder:validation:Pattern=^[^-]+$
        Name string `json:"name,omitempty"`
        // Flag to tell the operator to trigger a rolling restart of the Rack
        RollingRestart bool `json:"rollingRestart,omitempty"`

        //The Partition to control the Statefulset Upgrade
        RollingPartition int32 `json:"rollingPartition,omitempty"`

        //Labels used to target Kubernetes nodes
        Labels map[string]string `json:"labels,omitempty"`
}

// PodPolicy defines the policy for pods owned by CassKop operator.
type PodPolicy struct {
        // Annotations specifies the annotations to attach to headless service the CassKop operator creates
        Annotations map[string]string `json:"annotations,omitempty"`
        // Tolerations specifies the tolerations to attach to the pods the CassKop operator creates
        Tolerations []v1.Toleration `json:"tolerations,omitempty"`
}

// PodPolicy defines the policy for headless service owned by CassKop operator.
type ServicePolicy struct {
        // Annotations specifies the annotations to attach to headless service the CassKop operator creates
        Annotations map[string]string `json:"annotations,omitempty"`
}

// CassandraClusterResources sets the limits and requests for a container
type CassandraResources struct {
        Requests CPUAndMem `json:"requests,omitempty"`
        Limits   CPUAndMem `json:"limits,omitempty"`
}

// CPUAndMem defines how many cpu and ram the container will request/limit
type CPUAndMem struct {
        // +kubebuilder:validation:Pattern=^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$
        CPU    string `json:"cpu"`
        // +kubebuilder:validation:Pattern=^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$
        Memory string `json:"memory"`
}

//CassandraRackStatus defines states of Cassandra for 1 rack (1 statefulset)
type CassandraRackStatus struct {
        // Phase indicates the state this Cassandra cluster jumps in.
        // Phase goes as one way as below:
        //   Initial -&gt; Running &lt;-&gt; updating
        Phase string `json:"phase,omitempty"`

        // CassandraLastAction is the set of Cassandra State &amp; Actions: Active, Standby..
        CassandraLastAction CassandraLastAction `json:"cassandraLastAction,omitempty"`

        // PodLastOperation manage status for Pod Operation (nodetool cleanup, upgradesstables..)
        PodLastOperation PodLastOperation `json:"podLastOperation,omitempty"`
}

//CassandraClusterStatus defines Global state of CassandraCluster
type CassandraClusterStatus struct {
        // Phase indicates the state this Cassandra cluster jumps in.
        // Phase goes as one way as below:
        //   Initial -&gt; Running &lt;-&gt; updating
        Phase string `json:"phase,omitempty"`

        // Store last action at cluster level
        LastClusterAction       string `json:"lastClusterAction,omitempty"`
        LastClusterActionStatus string `json:"lastClusterActionStatus,omitempty"`

        // Indicates if we need to paused specific actions
        //ActionPaused bool `json:"actionPaused,omitempty"`

        //seeList to be used in Cassandra's Pods (computed by the Operator)
        SeedList []string `json:"seedlist,omitempty"`

        //CassandraRackStatusList list status for each Racks
        CassandraRackStatus map[string]CassandraRackStatusPtr `json:"cassandraRackStatus,omitempty"`
}

// CassandraLastAction defines status of the CassandraStatefulset
type CassandraLastAction struct {
        // Action is the specific actions that can be done on a Cassandra Cluster
        // such as cleanup, upgradesstables..
        Status string `json:"status,omitempty"`

        // Type of action to perform : UpdateVersion, UpdateBaseImage, UpdateConfigMap..
        Name string `json:"name,omitempty"`

        StartTime *metav1.Time `json:"startTime,omitempty"`
        EndTime   *metav1.Time `json:"endTime,omitempty"`

        // PodNames of updated Cassandra nodes. Updated means the Cassandra container image version
        // matches the spec's version.
        UpdatedNodes []string `json:"updatedNodes,omitempty"`
}

// PodLastOperation is managed via labels on Pods set by an administrator
type PodLastOperation struct {
        Name string `json:"name,omitempty"`

        Status string `json:"status,omitempty"`

        StartTime *metav1.Time `json:"startTime,omitempty"`
        EndTime   *metav1.Time `json:"endTime,omitempty"`

        //List of pods running an operation
        Pods []string `json:"pods,omitempty"`
        //List of pods that run an operation successfully
        PodsOK []string `json:"podsOK,omitempty"`
        //List of pods that fail to run an operation
        PodsKO []string `json:"podsKO,omitempty"`

        // Name of operator
        OperatorName string `json:"operatorName,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// CassandraCluster is the Schema for the cassandraclusters API
// +k8s:openapi-gen=true
type CassandraCluster struct {
        metav1.TypeMeta   `json:",inline"`
        metav1.ObjectMeta `json:"metadata,omitempty"`

        Spec   CassandraClusterSpec   `json:"spec,omitempty"`
        Status CassandraClusterStatus `json:"status,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// CassandraClusterList contains a list of CassandraCluster
type CassandraClusterList struct {
        metav1.TypeMeta `json:",inline"`
        metav1.ListMeta `json:"metadata,omitempty"`
        Items           []CassandraCluster `json:"items"`
}

func init() <span class="cov8" title="1">{
        SchemeBuilder.Register(&amp;CassandraCluster{}, &amp;CassandraClusterList{})
}</span>


</pre>
		
		<pre class="file" id="file1" style="display: none">// +build !ignore_autogenerated

// Code generated by operator-sdk. DO NOT EDIT.

package v1alpha1

import (
        v1 "k8s.io/api/core/v1"
        runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CPUAndMem) DeepCopyInto(out *CPUAndMem) <span class="cov0" title="0">{
        *out = *in
        return
}</span>

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CPUAndMem.
func (in *CPUAndMem) DeepCopy() *CPUAndMem <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CPUAndMem)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraCluster) DeepCopyInto(out *CassandraCluster) <span class="cov8" title="1">{
        *out = *in
        out.TypeMeta = in.TypeMeta
        in.ObjectMeta.DeepCopyInto(&amp;out.ObjectMeta)
        in.Spec.DeepCopyInto(&amp;out.Spec)
        in.Status.DeepCopyInto(&amp;out.Status)
        return
}</span>

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraCluster.
func (in *CassandraCluster) DeepCopy() *CassandraCluster <span class="cov8" title="1">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">out := new(CassandraCluster)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *CassandraCluster) DeepCopyObject() runtime.Object <span class="cov0" title="0">{
        if c := in.DeepCopy(); c != nil </span><span class="cov0" title="0">{
                return c
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraClusterList) DeepCopyInto(out *CassandraClusterList) <span class="cov0" title="0">{
        *out = *in
        out.TypeMeta = in.TypeMeta
        in.ListMeta.DeepCopyInto(&amp;out.ListMeta)
        if in.Items != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Items, &amp;out.Items
                *out = make([]CassandraCluster, len(*in))
                for i := range *in </span><span class="cov0" title="0">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
        }
        <span class="cov0" title="0">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraClusterList.
func (in *CassandraClusterList) DeepCopy() *CassandraClusterList <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CassandraClusterList)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *CassandraClusterList) DeepCopyObject() runtime.Object <span class="cov0" title="0">{
        if c := in.DeepCopy(); c != nil </span><span class="cov0" title="0">{
                return c
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraClusterSpec) DeepCopyInto(out *CassandraClusterSpec) <span class="cov8" title="1">{
        *out = *in
        if in.RunAsUser != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.RunAsUser, &amp;out.RunAsUser
                *out = new(int64)
                **out = **in
        }</span>
        <span class="cov8" title="1">if in.ReadOnlyRootFilesystem != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.ReadOnlyRootFilesystem, &amp;out.ReadOnlyRootFilesystem
                *out = new(bool)
                **out = **in
        }</span>
        <span class="cov8" title="1">out.Resources = in.Resources
        if in.Pod != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Pod, &amp;out.Pod
                *out = new(PodPolicy)
                (*in).DeepCopyInto(*out)
        }</span>
        <span class="cov8" title="1">if in.Service != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Service, &amp;out.Service
                *out = new(ServicePolicy)
                (*in).DeepCopyInto(*out)
        }</span>
        <span class="cov8" title="1">out.ImagePullSecret = in.ImagePullSecret
        out.ImageJolokiaSecret = in.ImageJolokiaSecret
        in.Topology.DeepCopyInto(&amp;out.Topology)
        return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraClusterSpec.
func (in *CassandraClusterSpec) DeepCopy() *CassandraClusterSpec <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CassandraClusterSpec)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraLastAction) DeepCopyInto(out *CassandraLastAction) <span class="cov8" title="1">{
        *out = *in
        if in.StartTime != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.StartTime, &amp;out.StartTime
                *out = (*in).DeepCopy()
        }</span>
        <span class="cov8" title="1">if in.EndTime != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.EndTime, &amp;out.EndTime
                *out = (*in).DeepCopy()
        }</span>
        <span class="cov8" title="1">if in.UpdatedNodes != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.UpdatedNodes, &amp;out.UpdatedNodes
                *out = make([]string, len(*in))
                copy(*out, *in)
        }</span>
        <span class="cov8" title="1">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraLastAction.
func (in *CassandraLastAction) DeepCopy() *CassandraLastAction <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CassandraLastAction)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraRackStatus) DeepCopyInto(out *CassandraRackStatus) <span class="cov8" title="1">{
        *out = *in
        in.CassandraLastAction.DeepCopyInto(&amp;out.CassandraLastAction)
        in.PodLastOperation.DeepCopyInto(&amp;out.PodLastOperation)
        return
}</span>

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraRackStatus.
func (in *CassandraRackStatus) DeepCopy() *CassandraRackStatus <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CassandraRackStatus)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CassandraResources) DeepCopyInto(out *CassandraResources) <span class="cov0" title="0">{
        *out = *in
        out.Requests = in.Requests
        out.Limits = in.Limits
        return
}</span>

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CassandraResources.
func (in *CassandraResources) DeepCopy() *CassandraResources <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(CassandraResources)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *DC) DeepCopyInto(out *DC) <span class="cov8" title="1">{
        *out = *in
        if in.Labels != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.Labels, &amp;out.Labels
                *out = make(map[string]string, len(*in))
                for key, val := range *in </span><span class="cov8" title="1">{
                        (*out)[key] = val
                }</span>
        }
        <span class="cov8" title="1">if in.Rack != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.Rack, &amp;out.Rack
                *out = make(RackSlice, len(*in))
                for i := range *in </span><span class="cov8" title="1">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
        }
        <span class="cov8" title="1">if in.NodesPerRacks != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.NodesPerRacks, &amp;out.NodesPerRacks
                *out = new(int32)
                **out = **in
        }</span>
        <span class="cov8" title="1">if in.NumTokens != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.NumTokens, &amp;out.NumTokens
                *out = new(int32)
                **out = **in
        }</span>
        <span class="cov8" title="1">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new DC.
func (in *DC) DeepCopy() *DC <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(DC)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in DCSlice) DeepCopyInto(out *DCSlice) <span class="cov0" title="0">{
        </span><span class="cov0" title="0">{
                in := &amp;in
                *out = make(DCSlice, len(*in))
                for i := range *in </span><span class="cov0" title="0">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
                <span class="cov0" title="0">return</span>
        }
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new DCSlice.
func (in DCSlice) DeepCopy() DCSlice <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(DCSlice)
        in.DeepCopyInto(out)
        return *out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PodLastOperation) DeepCopyInto(out *PodLastOperation) <span class="cov8" title="1">{
        *out = *in
        if in.StartTime != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.StartTime, &amp;out.StartTime
                *out = (*in).DeepCopy()
        }</span>
        <span class="cov8" title="1">if in.EndTime != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.EndTime, &amp;out.EndTime
                *out = (*in).DeepCopy()
        }</span>
        <span class="cov8" title="1">if in.Pods != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Pods, &amp;out.Pods
                *out = make([]string, len(*in))
                copy(*out, *in)
        }</span>
        <span class="cov8" title="1">if in.PodsOK != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.PodsOK, &amp;out.PodsOK
                *out = make([]string, len(*in))
                copy(*out, *in)
        }</span>
        <span class="cov8" title="1">if in.PodsKO != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.PodsKO, &amp;out.PodsKO
                *out = make([]string, len(*in))
                copy(*out, *in)
        }</span>
        <span class="cov8" title="1">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PodLastOperation.
func (in *PodLastOperation) DeepCopy() *PodLastOperation <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(PodLastOperation)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PodPolicy) DeepCopyInto(out *PodPolicy) <span class="cov0" title="0">{
        *out = *in
        if in.Annotations != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Annotations, &amp;out.Annotations
                *out = make(map[string]string, len(*in))
                for key, val := range *in </span><span class="cov0" title="0">{
                        (*out)[key] = val
                }</span>
        }
        <span class="cov0" title="0">if in.Tolerations != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Tolerations, &amp;out.Tolerations
                *out = make([]v1.Toleration, len(*in))
                for i := range *in </span><span class="cov0" title="0">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
        }
        <span class="cov0" title="0">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PodPolicy.
func (in *PodPolicy) DeepCopy() *PodPolicy <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(PodPolicy)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Rack) DeepCopyInto(out *Rack) <span class="cov8" title="1">{
        *out = *in
        if in.Labels != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.Labels, &amp;out.Labels
                *out = make(map[string]string, len(*in))
                for key, val := range *in </span><span class="cov8" title="1">{
                        (*out)[key] = val
                }</span>
        }
        <span class="cov8" title="1">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Rack.
func (in *Rack) DeepCopy() *Rack <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(Rack)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in RackSlice) DeepCopyInto(out *RackSlice) <span class="cov0" title="0">{
        </span><span class="cov0" title="0">{
                in := &amp;in
                *out = make(RackSlice, len(*in))
                for i := range *in </span><span class="cov0" title="0">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
                <span class="cov0" title="0">return</span>
        }
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new RackSlice.
func (in RackSlice) DeepCopy() RackSlice <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(RackSlice)
        in.DeepCopyInto(out)
        return *out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ServicePolicy) DeepCopyInto(out *ServicePolicy) <span class="cov0" title="0">{
        *out = *in
        if in.Annotations != nil </span><span class="cov0" title="0">{
                in, out := &amp;in.Annotations, &amp;out.Annotations
                *out = make(map[string]string, len(*in))
                for key, val := range *in </span><span class="cov0" title="0">{
                        (*out)[key] = val
                }</span>
        }
        <span class="cov0" title="0">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServicePolicy.
func (in *ServicePolicy) DeepCopy() *ServicePolicy <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(ServicePolicy)
        in.DeepCopyInto(out)
        return out</span>
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Topology) DeepCopyInto(out *Topology) <span class="cov8" title="1">{
        *out = *in
        if in.DC != nil </span><span class="cov8" title="1">{
                in, out := &amp;in.DC, &amp;out.DC
                *out = make(DCSlice, len(*in))
                for i := range *in </span><span class="cov8" title="1">{
                        (*in)[i].DeepCopyInto(&amp;(*out)[i])
                }</span>
        }
        <span class="cov8" title="1">return</span>
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Topology.
func (in *Topology) DeepCopy() *Topology <span class="cov0" title="0">{
        if in == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">out := new(Topology)
        in.DeepCopyInto(out)
        return out</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">// +build !ignore_autogenerated

// This file was autogenerated by openapi-gen. Do not edit it manually!

package v1alpha1

import (
        spec "github.com/go-openapi/spec"
        common "k8s.io/kube-openapi/pkg/common"
)

func GetOpenAPIDefinitions(ref common.ReferenceCallback) map[string]common.OpenAPIDefinition <span class="cov0" title="0">{
        return map[string]common.OpenAPIDefinition{
                "./pkg/apis/db/v1alpha1.CassandraCluster": schema_pkg_apis_db_v1alpha1_CassandraCluster(ref),
        }
}</span>

func schema_pkg_apis_db_v1alpha1_CassandraCluster(ref common.ReferenceCallback) common.OpenAPIDefinition <span class="cov0" title="0">{
        return common.OpenAPIDefinition{
                Schema: spec.Schema{
                        SchemaProps: spec.SchemaProps{
                                Description: "CassandraCluster is the Schema for the cassandraclusters API",
                                Type:        []string{"object"},
                                Properties: map[string]spec.Schema{
                                        "kind": {
                                                SchemaProps: spec.SchemaProps{
                                                        Description: "Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds",
                                                        Type:        []string{"string"},
                                                        Format:      "",
                                                },
                                        },
                                        "apiVersion": {
                                                SchemaProps: spec.SchemaProps{
                                                        Description: "APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources",
                                                        Type:        []string{"string"},
                                                        Format:      "",
                                                },
                                        },
                                        "metadata": {
                                                SchemaProps: spec.SchemaProps{
                                                        Ref: ref("k8s.io/apimachinery/pkg/apis/meta/v1.ObjectMeta"),
                                                },
                                        },
                                        "spec": {
                                                SchemaProps: spec.SchemaProps{
                                                        Ref: ref("./pkg/apis/db/v1alpha1.CassandraClusterSpec"),
                                                },
                                        },
                                        "status": {
                                                SchemaProps: spec.SchemaProps{
                                                        Ref: ref("./pkg/apis/db/v1alpha1.CassandraClusterStatus"),
                                                },
                                        },
                                },
                        },
                },
                Dependencies: []string{
                        "./pkg/apis/db/v1alpha1.CassandraClusterSpec", "./pkg/apis/db/v1alpha1.CassandraClusterStatus", "k8s.io/apimachinery/pkg/apis/meta/v1.ObjectMeta"},
        }
}</span>
</pre>
		
		<pre class="file" id="file3" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "reflect"
        "strconv"
        "time"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        "github.com/Orange-OpenSource/casskop/pkg/k8s"
        "github.com/sirupsen/logrus"
        appsv1 "k8s.io/api/apps/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

//Global var used to know if we need to update the CRD
var needUpdate bool

//updateCassandraStatus updates the CRD if the status has changed
//if needUpdate is set that mean that we have updated some fields in the CRD
//This method also stored the annotation cassandraclusters.db.orange.com/last-applied-configuration with last-applied-configuration
func (rcc *ReconcileCassandraCluster) updateCassandraStatus(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus) error <span class="cov8" title="1">{
        // don't update the status if there aren't any changes.
        if cc.Annotations == nil </span><span class="cov8" title="1">{
                cc.Annotations = map[string]string{}
        }</span>

        <span class="cov8" title="1">lastApplied, _ := cc.ComputeLastAppliedConfiguration()

        if !needUpdate &amp;&amp;
                reflect.DeepEqual(cc.Status, *status) &amp;&amp; //Do We need to update Status ?
                reflect.DeepEqual(cc.Annotations[api.AnnotationLastApplied], string(lastApplied)) &amp;&amp; //Do We need to update Annotation ?
                cc.Annotations[api.AnnotationLastApplied] != "" </span><span class="cov8" title="1">{
                return nil
        }</span>
        <span class="cov8" title="1">needUpdate = false
        //make also deepcopy to avoid pointer conflict
        cc.Status = *status.DeepCopy()
        cc.Annotations[api.AnnotationLastApplied] = string(lastApplied)

        err := rcc.client.Update(context.TODO(), cc)
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "err": err}).Errorf("Issue when updating CassandraCluster")
        }</span>
        <span class="cov8" title="1">return err</span>
}

// getNextCassandraClusterStatus goal is to detect some changes in the status between cassandracluster and its statefulset
// We follow only one change at a Time : so this function will return on first changed found
func (rcc *ReconcileCassandraCluster) getNextCassandraClusterStatus(cc *api.CassandraCluster, dc,
        rack int, dcName, rackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) error <span class="cov8" title="1">{

        //UpdateStatusIfUpdateResources(cc, dcRackName, storedStatefulSet, status)
        dcRackName := cc.GetDCRackName(dcName, rackName)

        if needToWaitDelayBeforeCheck(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov8" title="1">if rcc.UpdateStatusIfActionEnded(cc, dcName, rackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                return nil
        }</span>

        //If we set up UnlockNextOperation in CRD we allow to see mode change even last operation didn't ended correctly
        <span class="cov8" title="1">needSpecificChange := false
        if cc.Spec.UnlockNextOperation &amp;&amp;
                rcc.hasUnschedulablePod(cc.Namespace, dcName, rackName) </span><span class="cov0" title="0">{
                needSpecificChange = true
        }</span>
        //Do nothing in Initial phase except if we force it
        <span class="cov8" title="1">if status.CassandraRackStatus[dcRackName].Phase == api.ClusterPhaseInitial </span><span class="cov0" title="0">{
                if !needSpecificChange </span><span class="cov0" title="0">{
                        return nil
                }</span>
                <span class="cov0" title="0">status.CassandraRackStatus[dcRackName].Phase = api.ClusterPhasePending</span>
        }

        <span class="cov8" title="1">lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction

        // Do not check for new action if there is one ongoing or planed
        // Check to discover new changes are not done if action.status is Ongoing or ToDo/Finalizing
        // (a change is already performing)
        // action.status=Continue (which is set when decommission is successful) will be tested to see if we need to
        // decommission more
        // We don't want to check for new operation while there are already ongoing one in order not to break them (ie decommission..)
        // Meanwhile we allow to check for new changes if unlockNextOperation         has been set (to recover from problems)
        if needSpecificChange ||
                (!rcc.thereIsPodDisruption() &amp;&amp;
                        lastAction.Status != api.StatusOngoing &amp;&amp;
                        lastAction.Status != api.StatusToDo &amp;&amp;
                        lastAction.Status != api.StatusFinalizing) </span><span class="cov0" title="0">{

                // Update Status if ConfigMap Has Changed
                if UpdateStatusIfconfigMapHasChanged(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>

                // Update Status if ConfigMap Has Changed
                <span class="cov0" title="0">if UpdateStatusIfDockerImageHasChanged(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>

                // Update Status if There is a ScaleUp or ScaleDown
                <span class="cov0" title="0">if UpdateStatusIfScaling(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>

                // Update Status if Topology for SeedList has changed
                //if lastAction.Status != api.StatusFinalizing {
                <span class="cov0" title="0">if UpdateStatusIfSeedListHasChanged(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>

                <span class="cov0" title="0">if UpdateStatusIfRollingRestart(cc, dc, rack, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>

                <span class="cov0" title="0">if UpdateStatusIfStatefulSetChanged(cc, dcRackName, storedStatefulSet, status) </span><span class="cov0" title="0">{
                        return nil
                }</span>
        } else<span class="cov8" title="1"> {
                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                        "dc-rack": dcRackName}).Info("We don't check for new action before the cluster become stable again")
        }</span>

        <span class="cov8" title="1">if lastAction.Status == api.StatusToDo &amp;&amp; lastAction.Name == api.ActionUpdateResources </span><span class="cov0" title="0">{
                now := metav1.Now()
                lastAction.StartTime = &amp;now
                lastAction.Status = api.StatusOngoing
        }</span>

        <span class="cov8" title="1">return nil</span>
}

//needToWaitDelayBeforeCheck will return if last action start time is &lt; to api.DefaultDelayWait
//that mean start operation is too soon to check to an end operation or other available operations
//this is mostly to let the cassandra cluster and the operator to have the time to correctly stage the action
//DefaultDelayWait is of 2 minutes
func needToWaitDelayBeforeCheck(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet,
        status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{
        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction

        if lastAction.StartTime != nil </span><span class="cov0" title="0">{

                t := *lastAction.StartTime
                now := metav1.Now()

                if t.Add(api.DefaultDelayWait * time.Second).After(now.Time) </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                "rack": dcRackName}).Info("The Operator Waits " + strconv.Itoa(api.
                                DefaultDelayWait) + " seconds for the action to start correctly")
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

/*
func UpdateStatusIfUpdateResources(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet,
        status *api.CassandraClusterStatus) {
        dcRackStatus := status.CassandraRackStatus[dcRackName]
        if dcRackStatus.CassandraLastAction.Status == api.StatusToDo {
                dcRackStatus.CassandraLastAction.Status = api.StatusOngoing
                now := metav1.Now()
                status.CassandraRackStatus[dcRackName].CassandraLastAction.StartTime = &amp;now
        }
}
*/

//UpdateStatusIfconfigMapHasChanged updates CassandraCluster Action Status if it detect a changes :
// - a new configmapName in the CRD
// - or the add or remoove of the configmap in the CRD
func UpdateStatusIfconfigMapHasChanged(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{

        var updateConfigMap bool = false

        if storedStatefulSet.Spec.Template.Spec.Volumes == nil &amp;&amp; cc.Spec.ConfigMapName != "" </span><span class="cov0" title="0">{
                logrus.Infof("[%s][%s]: We ask to change ConfigMap New-CRD:%s -&gt; Old-StatefulSet:%s", cc.Name, dcRackName,
                        cc.Spec.ConfigMapName, "-")
                updateConfigMap = true
        }</span>
        <span class="cov8" title="1">if storedStatefulSet.Spec.Template.Spec.Volumes != nil </span><span class="cov8" title="1">{
                var found bool = false
                for _, volume := range storedStatefulSet.Spec.Template.Spec.Volumes </span><span class="cov8" title="1">{
                        if volume.Name == cassandraConfigMapName </span><span class="cov8" title="1">{
                                found = true
                                if volume.ConfigMap != nil &amp;&amp; volume.ConfigMap.Name != cc.Spec.ConfigMapName </span><span class="cov8" title="1">{
                                        logrus.Infof("[%s][%s]: We ask to change ConfigMap New-CRD:%s -&gt; Old-StatefulSet:%s", cc.Name, dcRackName,
                                                cc.Spec.ConfigMapName, volume.ConfigMap.Name)
                                        updateConfigMap = true
                                }</span>
                                <span class="cov8" title="1">break</span> // we have found the configmap
                        }
                }
                //If volume for configmap don't exist and we ask for a configmap
                <span class="cov8" title="1">if !found &amp;&amp; cc.Spec.ConfigMapName != "" </span><span class="cov8" title="1">{
                        logrus.Infof("[%s][%s]: We ask to change ConfigMap New-CRD:%s -&gt; Old-StatefulSet:%s", cc.Name, dcRackName,
                                cc.Spec.ConfigMapName, "-")
                        updateConfigMap = true
                }</span>
        }

        <span class="cov8" title="1">if updateConfigMap </span><span class="cov8" title="1">{
                lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
                lastAction.Status = api.StatusToDo
                lastAction.Name = api.ActionUpdateConfigMap
                lastAction.StartTime = nil
                lastAction.EndTime = nil
                return true
        }</span>
        <span class="cov8" title="1">return false</span>
}

//UpdateStatusIfDockerImageHasChanged updates CassandraCluster Action Status if it detect a changes in the DockerImage:
func UpdateStatusIfDockerImageHasChanged(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{

        desiredDockerImage := cc.Spec.CassandraImage

        //This needs to be refactor if we load more than 1 container
        if storedStatefulSet.Spec.Template.Spec.Containers != nil </span><span class="cov8" title="1">{
                for _, container := range storedStatefulSet.Spec.Template.Spec.Containers </span><span class="cov8" title="1">{
                        if container.Name == cassandraContainerName &amp;&amp; desiredDockerImage != container.Image </span><span class="cov8" title="1">{
                                </span><span class="cov8" title="1">{
                                        logrus.Infof("[%s][%s]: We ask to change DockerImage CRD:%s -&gt; StatefulSet:%s", cc.Name, dcRackName, desiredDockerImage, storedStatefulSet.Spec.Template.Spec.Containers[0].Image)
                                        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
                                        lastAction.Status = api.StatusToDo
                                        lastAction.Name = api.ActionUpdateDockerImage
                                        lastAction.StartTime = nil
                                        lastAction.EndTime = nil
                                        return true
                                }</span>
                        }
                }
        }
        <span class="cov8" title="1">return false</span>
}

func UpdateStatusIfRollingRestart(cc *api.CassandraCluster, dc,
        rack int, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov0" title="0">{

        if cc.Spec.Topology.DC[dc].Rack[rack].RollingRestart </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                        "dc-rack": dcRackName}).Info("Scoping RollingRestart of the Rack")
                lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
                lastAction.Status = api.StatusToDo
                lastAction.Name = api.ActionRollingRestart
                lastAction.StartTime = nil
                lastAction.EndTime = nil
                cc.Spec.Topology.DC[dc].Rack[rack].RollingRestart = false
                return true
        }</span>
        <span class="cov0" title="0">return false</span>
}

//UpdateStatusIfSeedListHasChanged updates CassandraCluster Action Status if it detect a changes
func UpdateStatusIfSeedListHasChanged(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{

        storedSeedListTab := getStoredSeedListTab(storedStatefulSet)

        //If Automatic Update of SeedList is enabled in the CRD
        if cc.Spec.AutoUpdateSeedList </span><span class="cov8" title="1">{
                //We compute what would be the best SeedList according to CRD Topology
                newSeedListTab := cc.InitSeedList()
                //We check if some nodes of the newSeedList are missing from Actual one
                if !k8s.ContainSlice(storedSeedListTab, newSeedListTab) </span><span class="cov8" title="1">{
                        status.SeedList = k8s.MergeSlice(storedSeedListTab, newSeedListTab)
                        logrus.Infof("[%s][%s]: We may need to update the seedlist (Add Nodes): %v -&gt; %v", cc.Name, dcRackName, storedSeedListTab, status.SeedList)
                }</span>

                //We Check if some nodes disapears from new SeedList (that should be a scale down, ore simply add nodes in another rack ??
                <span class="cov8" title="1">if !k8s.ContainSlice(newSeedListTab, storedSeedListTab) </span><span class="cov8" title="1">{
                        status.SeedList = k8s.MergeSlice(storedSeedListTab, newSeedListTab)
                        logrus.Infof("[%s][%s]: We may need to update the seedlist (Remove Nodes): %v -&gt; %v", cc.Name, dcRackName, storedSeedListTab, status.SeedList)
                }</span>
        }

        // If seed list has changed in the CRD, we have a manual change on the SeedList.
        // We flag the rack with UpdateSeedList Operation Configuring
        // Once all racks will be enabled with UpdateSeedList=Configuring,
        // then we update to ongoing and start the rollUpgrade
        // This is to ensure that we won't do 2 different kind of operations in different racks at the same time (ex:scaling + updateseedlist)
        <span class="cov8" title="1">if !reflect.DeepEqual(status.SeedList, storedSeedListTab) </span><span class="cov8" title="1">{
                logrus.Infof("[%s][%s]: We ask to Change the Cassandra SeedList", cc.Name, dcRackName)
                lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
                lastAction.Status = api.StatusConfiguring
                lastAction.Name = api.ActionUpdateSeedList
                lastAction.StartTime = nil
                lastAction.EndTime = nil
                return true
        }</span>

        <span class="cov8" title="1">return false</span>
}

//UpdateStatusIfScaling will detect any change of replicas
//For Scale Down the operator will need to first Decommission the last node from Cassandra before remooving it from kubernetes.
//For Scale Up some PodOperations may be scheduled if Auto-pilot is activeted.
func UpdateStatusIfScaling(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov0" title="0">{
        nodesPerRacks := cc.GetNodesPerRacks(dcRackName)
        if nodesPerRacks != *storedStatefulSet.Spec.Replicas </span><span class="cov0" title="0">{
                lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
                lastAction.Status = api.StatusToDo
                if nodesPerRacks &gt; *storedStatefulSet.Spec.Replicas </span><span class="cov0" title="0">{
                        lastAction.Name = api.ActionScaleUp
                        logrus.Infof("[%s][%s]: Scaling Cluster : Ask %d and have %d --&gt; ScaleUP", cc.Name, dcRackName, nodesPerRacks, *storedStatefulSet.Spec.Replicas)
                }</span> else<span class="cov0" title="0"> {
                        logrus.Infof("[%s][%s]: Scaling Cluster : Ask %d and have %d --&gt; ScaleDown", cc.Name, dcRackName, nodesPerRacks, *storedStatefulSet.Spec.Replicas)
                        setDecommissionStatus(status, dcRackName)
                }</span>
                <span class="cov0" title="0">lastAction.StartTime = nil
                lastAction.EndTime = nil
                return true</span>
        }
        <span class="cov0" title="0">return false</span>
}

// UpdateStatusIfStatefulSetChanged detects if there is a change in the statefulset which was not already caught
// If we detect a Statefulset change with this method, then the operator won't catch it before the statefulset tells the operator
// that a change is ongoing.
// That mean that all statefulsets may do their rolling upgrade in parallel, so there will be &lt;nbRacks&gt; node down in // in the cluster.
func UpdateStatusIfStatefulSetChanged(cc *api.CassandraCluster, dcRackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov0" title="0">{
        // If We come Here, We have not detected any change with out specific tests
        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
        if storedStatefulSet.Status.CurrentRevision != storedStatefulSet.Status.UpdateRevision </span><span class="cov0" title="0">{

                lastAction.Name = api.ActionUpdateStatefulSet
                lastAction.Status = api.StatusOngoing
                now := metav1.Now()
                lastAction.StartTime = &amp;now
                lastAction.EndTime = nil
                return true
        }</span>
        <span class="cov0" title="0">return false</span>
}

//UpdateStatusIfActionEnded Implement Tests to detect End of Ongoing Actions
func (rcc *ReconcileCassandraCluster) UpdateStatusIfActionEnded(cc *api.CassandraCluster, dcName string,
        rackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
        now := metav1.Now()

        if lastAction.Status == api.StatusOngoing ||
                lastAction.Status == api.StatusContinue </span><span class="cov0" title="0">{

                nodesPerRacks := cc.GetNodesPerRacks(dcRackName)
                switch lastAction.Name </span>{

                case api.ActionScaleUp:<span class="cov0" title="0">

                        //Does the Scaling ended ?
                        if nodesPerRacks == storedStatefulSet.Status.Replicas </span><span class="cov0" title="0">{

                                podsList, err := rcc.ListPods(cc.Namespace, k8s.LabelsForCassandraDCRack(cc, dcName, rackName))
                                nb := len(podsList.Items)
                                if err != nil || nb &lt; 1 </span><span class="cov0" title="0">{
                                        return false
                                }</span>
                                <span class="cov0" title="0">pod := podsList.Items[nodesPerRacks-1]
                                //We need lastPod to be running to consider ScaleUp ended
                                if cassandraPodIsReady(&amp;pod) </span><span class="cov0" title="0">{
                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName}).Info("ScaleUp is Done")
                                        lastAction.Status = api.StatusDone
                                        lastAction.EndTime = &amp;now

                                        labels := map[string]string{"operation-name": api.OperationCleanup}
                                        if cc.Spec.AutoPilot </span><span class="cov0" title="0">{
                                                labels["operation-status"] = api.StatusToDo
                                        }</span> else<span class="cov0" title="0"> {
                                                labels["operation-status"] = api.StatusManual
                                        }</span>
                                        <span class="cov0" title="0">rcc.addPodOperationLabels(cc, dcName, rackName, labels)

                                        return true</span>
                                }
                                <span class="cov0" title="0">return false</span>
                        }

                case api.ActionScaleDown:<span class="cov0" title="0">

                        if nodesPerRacks == storedStatefulSet.Status.Replicas </span><span class="cov0" title="0">{
                                if cc.Status.CassandraRackStatus[dcRackName].PodLastOperation.Name == api.OperationDecommission &amp;&amp;
                                        cc.Status.CassandraRackStatus[dcRackName].PodLastOperation.Status == api.StatusDone </span><span class="cov0" title="0">{
                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName}).Info("ScaleDown is Done")
                                        lastAction.Status = api.StatusDone
                                        lastAction.EndTime = &amp;now
                                        return true
                                }</span>
                                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName}).Info("ScaleDown not yet Completed: Waiting for Pod operation to be Done")</span>
                        }

                case api.ClusterPhaseInitial:<span class="cov0" title="0">
                        //nothing particular here
                        return false</span>

                default:<span class="cov0" title="0">
                        // Do the update has finished on all pods ?
                        if storedStatefulSet.Status.CurrentRevision == storedStatefulSet.Status.UpdateRevision </span><span class="cov0" title="0">{
                                logrus.Infof("[%s][%s]: Update %s is Done", cc.Name, dcRackName, lastAction.Name)
                                lastAction.Status = api.StatusDone
                                now := metav1.Now()
                                lastAction.EndTime = &amp;now
                                return true
                        }</span>

                }

        }
        <span class="cov8" title="1">return false</span>
}

// UpdateCassandraRackStatusPhase goal is to calculate the Cluster Phase according to StatefulSet Status.
// The Phase is: Initializing -&gt; Running &lt;--&gt; Pending
// The Phase is a very high level view of the cluster, for a better view we need to see Actions and Pod Operations
func (rcc *ReconcileCassandraCluster) UpdateCassandraRackStatusPhase(cc *api.CassandraCluster, dcName string,
        rackName string, storedStatefulSet *appsv1.StatefulSet, status *api.CassandraClusterStatus) error <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction

        if status.CassandraRackStatus[dcRackName].Phase == api.ClusterPhaseInitial </span><span class="cov8" title="1">{

                nodesPerRacks := cc.GetNodesPerRacks(dcRackName)
                //If we are stuck in initializing state, we can rollback the add of dc which implies decommissioning nodes
                if nodesPerRacks &lt;= 0 </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                "rack": dcRackName}).Warn("Aborting Initializing..., start ScaleDown")
                        setDecommissionStatus(status, dcRackName)
                        return nil
                }</span>

                //Do we have reach requested number of replicas ?
                <span class="cov8" title="1">if isStatefulSetNotReady(storedStatefulSet) </span><span class="cov0" title="0">{
                        logrus.Infof("[%s][%s]: Initializing StatefulSet: Replicas Number Not OK: %d on %d, ready[%d]",
                                cc.Name, dcRackName, storedStatefulSet.Status.Replicas, *storedStatefulSet.Spec.Replicas,
                                storedStatefulSet.Status.ReadyReplicas)
                }</span> else<span class="cov8" title="1"> {
                        //If yes, just check that lastPod is running
                        podsList, err := rcc.ListPods(cc.Namespace, k8s.LabelsForCassandraDCRack(cc, dcName, rackName))
                        nb := len(podsList.Items)
                        if err != nil || nb &lt; 1 </span><span class="cov0" title="0">{
                                return nil
                        }</span>
                        <span class="cov8" title="1">nodesPerRacks := cc.GetNodesPerRacks(dcRackName)
                        if len(podsList.Items) &lt; int(nodesPerRacks) </span><span class="cov0" title="0">{
                                logrus.Infof("[%s][%s]: StatefulSet is waiting for scaleUp", cc.Name, dcRackName)
                                return nil
                        }</span>
                        <span class="cov8" title="1">pod := podsList.Items[nodesPerRacks-1]
                        if cassandraPodIsReady(&amp;pod) </span><span class="cov8" title="1">{
                                status.CassandraRackStatus[dcRackName].Phase = api.ClusterPhaseRunning
                                now := metav1.Now()
                                lastAction.EndTime = &amp;now
                                lastAction.Status = api.StatusDone
                                logrus.Infof("[%s][%s]: StatefulSet(%s): Replicas Number OK: ready[%d]", cc.Name, dcRackName, lastAction.Name, storedStatefulSet.Status.ReadyReplicas)
                                return nil
                        }</span>
                        <span class="cov0" title="0">return nil</span>

                }

        } else<span class="cov8" title="1"> {

                //We are no more in Initializing state
                if isStatefulSetNotReady(storedStatefulSet) </span><span class="cov0" title="0">{
                        logrus.Infof("[%s][%s]: StatefulSet(%s) Replicas Number Not OK: %d on %d, ready[%d]", cc.Name,
                                dcRackName, lastAction.Name, storedStatefulSet.Status.Replicas, *storedStatefulSet.Spec.Replicas,
                                storedStatefulSet.Status.ReadyReplicas)
                        status.CassandraRackStatus[dcRackName].Phase = api.ClusterPhasePending
                }</span> else<span class="cov8" title="1"> if status.CassandraRackStatus[dcRackName].Phase != api.ClusterPhaseRunning </span><span class="cov0" title="0">{
                        logrus.Infof("[%s][%s]: StatefulSet(%s): Replicas Number OK: ready[%d]", cc.Name, dcRackName,
                                lastAction.Name, storedStatefulSet.Status.ReadyReplicas)
                        status.CassandraRackStatus[dcRackName].Phase = api.ClusterPhaseRunning
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

func setDecommissionStatus(status *api.CassandraClusterStatus, dcRackName string) <span class="cov0" title="0">{
        status.CassandraRackStatus[dcRackName].Phase = api.ClusterPhasePending
        now := metav1.Now()
        lastAction := &amp;status.CassandraRackStatus[dcRackName].CassandraLastAction
        lastAction.StartTime = &amp;now
        lastAction.Status = api.StatusToDo
        lastAction.Name = api.ActionScaleDown
        status.CassandraRackStatus[dcRackName].PodLastOperation.Status = api.StatusToDo
        status.CassandraRackStatus[dcRackName].PodLastOperation.Name = api.OperationDecommission
        status.CassandraRackStatus[dcRackName].PodLastOperation.StartTime = &amp;now
        status.CassandraRackStatus[dcRackName].PodLastOperation.EndTime = nil
        status.CassandraRackStatus[dcRackName].PodLastOperation.Pods = []string{}
        status.CassandraRackStatus[dcRackName].PodLastOperation.PodsOK = []string{}
        status.CassandraRackStatus[dcRackName].PodLastOperation.PodsKO = []string{}
}</span>
</pre>
		
		<pre class="file" id="file4" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"

        "github.com/Orange-OpenSource/casskop/pkg/k8s"
        "github.com/sirupsen/logrus"
        "k8s.io/api/core/v1"
)

//thereIsNoPodDisruption return true if there is no Disruption in the Pods of the cassandra Cluster
func (rcc *ReconcileCassandraCluster) thereIsPodDisruption() bool <span class="cov8" title="1">{
        return rcc.storedPdb.Status.PodDisruptionsAllowed == 0
}</span>

func (rcc *ReconcileCassandraCluster) allowMoreThan1PodDisruption() bool <span class="cov0" title="0">{
        return rcc.storedPdb.Spec.MaxUnavailable.IntVal &gt; 1
}</span>

func (rcc *ReconcileCassandraCluster) hasOneDisruptedPod() bool <span class="cov0" title="0">{
        return (rcc.storedPdb.Status.DesiredHealthy+rcc.storedPdb.Spec.MaxUnavailable.IntVal)-rcc.storedPdb.Status.CurrentHealthy &gt; 0
}</span>

//weAreScalingDown return true if we are Scaling Down the provided dc-rack
func (rcc *ReconcileCassandraCluster) weAreScalingDown(dcRackStatus *api.CassandraRackStatus) bool <span class="cov8" title="1">{
        if dcRackStatus.CassandraLastAction.Name == api.ActionScaleDown &amp;&amp;
                (dcRackStatus.CassandraLastAction.Status == api.StatusToDo ||
                        dcRackStatus.CassandraLastAction.Status == api.StatusOngoing ||
                        dcRackStatus.CassandraLastAction.Status == api.StatusContinue) </span><span class="cov0" title="0">{
                return true
        }</span>
        <span class="cov8" title="1">return false</span>
}

func cassandraPodIsReady(pod *v1.Pod) bool <span class="cov8" title="1">{
        for i := range pod.Status.ContainerStatuses </span><span class="cov8" title="1">{
                if pod.Status.ContainerStatuses[i].Name == "cassandra" &amp;&amp;
                        pod.Status.Phase == "Running" &amp;&amp;
                        pod.Status.ContainerStatuses[i].Ready </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov0" title="0">return false</span>
}

// DeletePVC deletes persistentvolumes of nodes in a rack
func (rcc *ReconcileCassandraCluster) DeletePVCs(cc *api.CassandraCluster, dcName string, rackName string) <span class="cov0" title="0">{
        lpvc, err := rcc.ListPVC(cc.Namespace, k8s.LabelsForCassandraDCRack(cc, dcName, rackName))
        if err != nil </span><span class="cov0" title="0">{
                logrus.Errorf("failed to get cassandra's PVC: %v", err)
        }</span>
        <span class="cov0" title="0">for _, pvc := range lpvc.Items </span><span class="cov0" title="0">{
                err := rcc.deletePVC(&amp;pvc)

                if err != nil </span><span class="cov0" title="0">{
                        logrus.Errorf("[%s]: Error Deleting PVC[%s], Please make manual Actions..", cc.Name, pvc.Name)
                }</span> else<span class="cov0" title="0"> {
                        logrus.Infof("[%s]: Delete PVC[%s] OK", cc.Name, pvc.Name)
                }</span>
        }
}
</pre>
		
		<pre class="file" id="file5" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "time"

        "github.com/sirupsen/logrus"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        appsv1 "k8s.io/api/apps/v1"
        policyv1beta1 "k8s.io/api/policy/v1beta1"
        "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/runtime"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/controller"
        "sigs.k8s.io/controller-runtime/pkg/handler"
        "sigs.k8s.io/controller-runtime/pkg/manager"
        "sigs.k8s.io/controller-runtime/pkg/reconcile"
        logf "sigs.k8s.io/controller-runtime/pkg/runtime/log"
        "sigs.k8s.io/controller-runtime/pkg/source"
)

var log = logf.Log.WithName("controller_cassandracluster")

// Add creates a new CassandraCluster Controller and adds it to the Manager. The Manager will set fields on the Controller
// and Start it when the Manager is Started.
func Add(mgr manager.Manager) error <span class="cov0" title="0">{
        return add(mgr, newReconciler(mgr))
}</span>

// newReconciler returns a new reconcile.Reconciler
func newReconciler(mgr manager.Manager) reconcile.Reconciler <span class="cov0" title="0">{
        return &amp;ReconcileCassandraCluster{client: mgr.GetClient(), scheme: mgr.GetScheme()}
}</span>

// add adds a new Controller to mgr with r as the reconcile.Reconciler
func add(mgr manager.Manager, r reconcile.Reconciler) error <span class="cov0" title="0">{
        // Create a new controller
        c, err := controller.New("cassandracluster-controller", mgr, controller.Options{Reconciler: r})
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Watch for changes to primary resource CassandraCluster
        <span class="cov0" title="0">err = c.Watch(&amp;source.Kind{Type: &amp;api.CassandraCluster{}}, &amp;handler.EnqueueRequestForObject{})
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Modify this to be the types you create that are owned by the primary resource
        /* We currently don't have secondary resource to watch
        // Modify this to be the types you create that are owned by the primary resource
        // Watch for changes to secondary resource Pods and requeue the owner CassandraCluster
        err = c.Watch(&amp;source.Kind{Type: &amp;corev1.Pod{}}, &amp;handler.EnqueueRequestForOwner{
                IsController: true,
                OwnerType:    &amp;dbv1alpha1.CassandraCluster{},
        })
        if err != nil {
                return err
        }
        */

        <span class="cov0" title="0">return nil</span>
}

var _ reconcile.Reconciler = &amp;ReconcileCassandraCluster{}

// ReconcileCassandraCluster reconciles a CassandraCluster object
type ReconcileCassandraCluster struct {
        // This client, initialized using mgr.Client() above, is a split client
        // that reads objects from the cache and writes to the apiserver
        cc     *api.CassandraCluster
        client client.Client
        scheme *runtime.Scheme

        storedPdb         *policyv1beta1.PodDisruptionBudget
        storedStatefulSet *appsv1.StatefulSet
}

// Reconcile reads that state of the cluster for a CassandraCluster object and makes changes based on the state read
// and what is in the CassandraCluster.Spec
// a Pod as an example
// Note:
// The Controller will requeue the Request to be processed again if the returned error is non-nil or
// Result.Requeue is true, otherwise upon completion it will remove the work from the queue.
func (rcc *ReconcileCassandraCluster) Reconcile(request reconcile.Request) (reconcile.Result, error) <span class="cov8" title="1">{
        reqLogger := log.WithValues("Request.Namespace", request.Namespace, "Request.Name", request.Name)
        reqLogger.Info("Reconciling CassandraCluster")

        requeue30 := reconcile.Result{RequeueAfter: 30 * time.Second}
        requeue5 := reconcile.Result{RequeueAfter: 5 * time.Second}
        requeue := reconcile.Result{Requeue: true}
        forget := reconcile.Result{}

        // Fetch the CassandraCluster instance
        rcc.cc = &amp;api.CassandraCluster{}
        cc := rcc.cc
        err := rcc.client.Get(context.TODO(), request.NamespacedName, cc)
        if err != nil </span><span class="cov0" title="0">{
                if errors.IsNotFound(err) </span><span class="cov0" title="0">{
                        // Request object not found, could have been deleted after reconcile request.
                        // Owned objects are automatically garbage collected. For additional cleanup logic use finalizers.
                        // Return and don't requeue
                        return forget, nil
                }</span>
                // Error reading the object - requeue the request.
                <span class="cov0" title="0">return forget, err</span>
        }

        // After first time reconcile, phase will switch to "Initializing".
        <span class="cov8" title="1">if cc.Status.Phase == "" </span><span class="cov8" title="1">{
                // Simulate initializer.
                changed := cc.SetDefaults()
                if changed </span><span class="cov8" title="1">{
                        updateDeletePvcStrategy(cc)
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Info("Initialization: Update CassandraCluster")
                        return requeue, rcc.client.Update(context.TODO(), cc)
                }</span>
        }
        <span class="cov8" title="1">cc.CheckDefaults()

        err = rcc.CheckDeletePVC(cc)
        if err != nil </span><span class="cov0" title="0">{
                return forget, err
        }</span>

        <span class="cov8" title="1">status := cc.Status.DeepCopy()

        //We Update Status at the end
        defer rcc.updateCassandraStatus(cc, status)

        //If non allowed changes on CRD, we return here
        if rcc.CheckNonAllowedChanges(cc, status) </span><span class="cov0" title="0">{
                return requeue30, nil
        }</span>

        <span class="cov8" title="1">if err = rcc.ensureCassandraPodDisruptionBudget(cc); err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Errorf("ensureCassandraPodDisruptionBudget Error: %v", err)
        }</span>

        //ReconcileRack will also add and initiate new racks, we must not go through racks before this method
        <span class="cov8" title="1">err = rcc.ReconcileRack(cc, status)
        if err != nil </span><span class="cov0" title="0">{
                return requeue5, err
        }</span>

        //Do we need to UpdateSeedList
        <span class="cov8" title="1">FlipCassandraClusterUpdateSeedListStatus(cc, status)

        UpdateCassandraClusterStatusPhase(cc, status)

        //We could set different requeue based on current Operation
        return requeue5, nil</span>

}
</pre>
		
		<pre class="file" id="file6" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "fmt"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"

        "github.com/Orange-OpenSource/casskop/pkg/k8s"

        "github.com/sirupsen/logrus"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/util/intstr"
)

const (
        //max 15 char for port names
        cassandraPort                = 9042
        cassandraPortName            = "cql"
        cassandraIntraNodePort       = 7000
        cassandraIntraNodeName       = "intra-node"
        cassandraIntraNodeTLSPort    = 7001
        cassandraIntraNodeTLSName    = "intra-node-tls"
        cassandraJMX                 = 7199 //used for nodetool+istio
        cassandraJMXName             = "jmx-port"
        JolokiaPort                  = 8778
        JolokiaPortName              = "jolokia"
        exporterCassandraJmxPort     = 9500
        exporterCassandraJmxPortName = "promjmx"
)

func (rcc *ReconcileCassandraCluster) ensureCassandraService(cc *api.CassandraCluster) error <span class="cov8" title="1">{
        selector := k8s.LabelsForCassandra(cc)
        svc := generateCassandraService(cc, selector, nil)

        k8s.AddOwnerRefToObject(svc, k8s.AsOwner(cc))
        err := rcc.client.Create(context.TODO(), svc)
        if err != nil &amp;&amp; !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create cassandra service (%v)", err)
        }</span>
        <span class="cov8" title="1">return nil</span>
}

func (rcc *ReconcileCassandraCluster) ensureCassandraServiceMonitoring(cc *api.CassandraCluster,
        dcName string) error <span class="cov8" title="1">{
        selector := k8s.LabelsForCassandra(cc)
        svc := generateCassandraExporterService(cc, selector, nil)

        k8s.AddOwnerRefToObject(svc, k8s.AsOwner(cc))
        err := rcc.client.Create(context.TODO(), svc)
        if err != nil &amp;&amp; !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create cassandra service Monitoring: %v", err)
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// ensureCassandraPodDisruptionBudget generate and apply the PodDisruptionBudget
// take dcName to accordingly named the pdb, and target the pods
func (rcc *ReconcileCassandraCluster) ensureCassandraPodDisruptionBudget(cc *api.CassandraCluster) error <span class="cov8" title="1">{
        labels := k8s.LabelsForCassandra(cc)

        pdb := generatePodDisruptionBudget(cc.Name, cc.Namespace, labels, k8s.AsOwner(cc),
                intstr.FromInt(int(cc.Spec.MaxPodUnavailable)))
        err := rcc.CreateOrUpdatePodDisruptionBudget(pdb)
        if err != nil &amp;&amp; !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                logrus.Errorf("CreateOrUpdatePodDisruptionBudget Error: %v", err)
        }</span>
        <span class="cov8" title="1">return err</span>
}

// ensureCassandraStatefulSet generate and apply the statefulset
// take dcRackName to accordingly named the statefulset
// take dc and rack index of dc and rack in conf to retrieve according  nodeselectors labels
func (rcc *ReconcileCassandraCluster) ensureCassandraStatefulSet(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus, dcName string, dcRackName string, dc int, rack int) (bool, error) <span class="cov8" title="1">{

        labels, nodeSelector := k8s.GetDCRackLabelsAndNodeSelectorForStatefulSet(cc, dc, rack)

        ss := generateCassandraStatefulSet(cc, status, dcName, dcRackName, labels, nodeSelector, nil)
        k8s.AddOwnerRefToObject(ss, k8s.AsOwner(cc))

        breakResyncloop, err := rcc.CreateOrUpdateStatefulSet(ss, status, dcRackName)
        if err != nil &amp;&amp; !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                return breakResyncloop, fmt.Errorf("failed to create cassandra statefulset: %v", err)
        }</span>

        <span class="cov8" title="1">return breakResyncloop, nil</span>
}
</pre>
		
		<pre class="file" id="file7" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "fmt"

        "github.com/banzaicloud/k8s-objectmatcher/patch"

        "github.com/sirupsen/logrus"
        appsv1 "k8s.io/api/apps/v1"
        v1 "k8s.io/api/core/v1"
        policyv1beta1 "k8s.io/api/policy/v1beta1"
        "k8s.io/apimachinery/pkg/api/resource"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/util/intstr"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"

        "github.com/Orange-OpenSource/casskop/pkg/k8s"

        "sort"
        "strconv"
        "strings"
)

/*JvmMemory sets the maximium size of the heap*/
type JvmMemory struct {
        maxHeapSize string
}

/*Bunch of different constants*/
const (
        cassandraContainerName = "cassandra"
        defaultJvmMaxHeap      = "2048M"
        hostnameTopologyKey    = "kubernetes.io/hostname"

        cassandraConfigMapName = "cassandra-config"

        livenessInitialDelaySeconds int32 = 120
        livenessHealthCheckTimeout  int32 = 20
        livenessHealthCheckPeriod   int32 = 10

        readinessInitialDelaySeconds int32 = 60
        readinessHealthCheckTimeout  int32 = 10
        readinessHealthCheckPeriod   int32 = 10
)

func generateCassandraService(cc *api.CassandraCluster, labels map[string]string, ownerRefs []metav1.OwnerReference) *v1.Service <span class="cov8" title="1">{

        var annotations = map[string]string{}
        if cc.Spec.Service != nil </span><span class="cov8" title="1">{
                annotations = cc.Spec.Service.Annotations
        }</span>

        <span class="cov8" title="1">return &amp;v1.Service{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "Service",
                        APIVersion: "v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:            cc.GetName(),
                        Namespace:       cc.GetNamespace(),
                        Labels:          labels,
                        Annotations:     annotations,
                        OwnerReferences: ownerRefs,
                },
                Spec: v1.ServiceSpec{
                        Type:      v1.ServiceTypeClusterIP,
                        ClusterIP: v1.ClusterIPNone,
                        Ports: []v1.ServicePort{
                                v1.ServicePort{
                                        Port:     cassandraPort,
                                        Protocol: v1.ProtocolTCP,
                                        Name:     cassandraPortName,
                                },
                        },
                        Selector:                 labels,
                        PublishNotReadyAddresses: true,
                },
        }</span>
}

func generateCassandraExporterService(cc *api.CassandraCluster, labels map[string]string, ownerRefs []metav1.OwnerReference) *v1.Service <span class="cov8" title="1">{
        name := cc.GetName()
        namespace := cc.Namespace

        mlabels := k8s.MergeLabels(labels, map[string]string{"k8s-app": "exporter-cassandra-jmx"})

        return &amp;v1.Service{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "Service",
                        APIVersion: "v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:            fmt.Sprintf("%s-exporter-jmx", name),
                        Namespace:       namespace,
                        Labels:          mlabels,
                        OwnerReferences: ownerRefs,
                },
                Spec: v1.ServiceSpec{
                        Type:      v1.ServiceTypeClusterIP,
                        ClusterIP: v1.ClusterIPNone,
                        Ports: []v1.ServicePort{
                                v1.ServicePort{
                                        Port:     exporterCassandraJmxPort,
                                        Protocol: v1.ProtocolTCP,
                                        Name:     exporterCassandraJmxPortName,
                                },
                        },
                        Selector: labels,
                },
        }
}</span>

func emptyDir(name string) v1.Volume <span class="cov8" title="1">{
        return v1.Volume{
                Name:         name,
                VolumeSource: v1.VolumeSource{EmptyDir: &amp;v1.EmptyDirVolumeSource{}},
        }
}</span>

func generateCassandraVolumes(cc *api.CassandraCluster) []v1.Volume <span class="cov8" title="1">{
        var v = []v1.Volume{
                emptyDir("bootstrap"),
                emptyDir("extra-lib"),
                //emptyDir("configuration"),
                emptyDir("tmp"),
        }

        if cc.Spec.ConfigMapName != "" </span><span class="cov8" title="1">{
                v = append(v, v1.Volume{
                        Name: cassandraConfigMapName,
                        VolumeSource: v1.VolumeSource{
                                ConfigMap: &amp;v1.ConfigMapVolumeSource{
                                        LocalObjectReference: v1.LocalObjectReference{
                                                Name: cc.Spec.ConfigMapName,
                                        },
                                        DefaultMode: func(i int32) *int32 </span><span class="cov8" title="1">{ return &amp;i }</span>(493), //493 is base10 to 0755 base8
                                },
                        },
                })
        }

        <span class="cov8" title="1">return v</span>
}

// generateCassandraVolumeMount generate volumemounts for cassandra containers
// Volume Claim
//  - /var/lib/cassandra for Cassandra data
// ConfigMap
//  - /tmp/cassandra/configmap for user defined configmap
// EmptyDirs
//   - /bootstrap for Cassandra configuration
//   - /extra-lib for additional jar we want to load
//   - /tmp to work with readonly containers
func generateCassandraVolumeMount(cc *api.CassandraCluster) []v1.VolumeMount <span class="cov8" title="1">{
        var vm []v1.VolumeMount

        if cc.Spec.DataCapacity != "" </span><span class="cov8" title="1">{
                vm = append(vm, v1.VolumeMount{Name: "data", MountPath: "/var/lib/cassandra"})
        }</span>

        <span class="cov8" title="1">if cc.Spec.ConfigMapName != "" </span><span class="cov8" title="1">{
                vm = append(vm, v1.VolumeMount{Name: "cassandra-config", MountPath: "/configmap"})
        }</span>
        <span class="cov8" title="1">vm = append(vm, v1.VolumeMount{Name: "bootstrap", MountPath: "/etc/cassandra"})
        vm = append(vm, v1.VolumeMount{Name: "extra-lib", MountPath: "/extra-lib"})
        vm = append(vm, v1.VolumeMount{Name: "tmp", MountPath: "/tmp"})
        return vm</span>
}

func generateVolumeClaimTemplate(cc *api.CassandraCluster, labels map[string]string) []v1.PersistentVolumeClaim <span class="cov8" title="1">{

        var pvc []v1.PersistentVolumeClaim

        if cc.Spec.DataCapacity == "" </span><span class="cov0" title="0">{
                logrus.Warnf("[%s]: No Spec.DataCapacity was specified -&gt; You Cluster WILL NOT HAVE PERSISTENT DATA!!!!!", cc.Name)
                return pvc
        }</span>

        <span class="cov8" title="1">pvc = []v1.PersistentVolumeClaim{
                v1.PersistentVolumeClaim{
                        ObjectMeta: metav1.ObjectMeta{
                                Name:   "data",
                                Labels: labels,
                        },
                        Spec: v1.PersistentVolumeClaimSpec{
                                AccessModes: []v1.PersistentVolumeAccessMode{
                                        v1.ReadWriteOnce,
                                },

                                Resources: v1.ResourceRequirements{
                                        Requests: v1.ResourceList{
                                                "storage": generateResourceQuantity(cc.Spec.DataCapacity),
                                        },
                                },
                        },
                },
        }

        if cc.Spec.DataStorageClass != "" </span><span class="cov8" title="1">{
                pvc[0].Spec.StorageClassName = &amp;cc.Spec.DataStorageClass
        }</span>

        <span class="cov8" title="1">return pvc</span>
}

func generateCassandraStatefulSet(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        dcName string, dcRackName string,
        labels map[string]string, nodeSelector map[string]string, ownerRefs []metav1.OwnerReference) *appsv1.StatefulSet <span class="cov8" title="1">{
        name := cc.GetName()
        namespace := cc.Namespace
        volumes := generateCassandraVolumes(cc)
        volumeClaimTemplate := generateVolumeClaimTemplate(cc, labels)

        for _, pvc := range volumeClaimTemplate </span><span class="cov8" title="1">{
                k8s.AddOwnerRefToObject(&amp;pvc, k8s.AsOwner(cc))
        }</span>

        <span class="cov8" title="1">nodeAffinity := createNodeAffinity(nodeSelector)
        nodesPerRacks := cc.GetNodesPerRacks(dcRackName)
        rollingPartition := cc.GetRollingPartitionPerRacks(dcRackName)
        terminationPeriod := int64(api.DefaultTerminationGracePeriodSeconds)
        var annotations = map[string]string{}
        var tolerations = []v1.Toleration{}
        if cc.Spec.Pod != nil </span><span class="cov8" title="1">{
                annotations = cc.Spec.Pod.Annotations
                tolerations = cc.Spec.Pod.Tolerations
        }</span>

        <span class="cov8" title="1">ss := &amp;appsv1.StatefulSet{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "StatefulSet",
                        APIVersion: "apps/v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:            name + "-" + dcRackName,
                        Namespace:       namespace,
                        Labels:          labels,
                        OwnerReferences: ownerRefs,
                },
                Spec: appsv1.StatefulSetSpec{
                        ServiceName: name,
                        Replicas:    &amp;nodesPerRacks,
                        UpdateStrategy: appsv1.StatefulSetUpdateStrategy{
                                Type: "RollingUpdate",
                                RollingUpdate: &amp;appsv1.RollingUpdateStatefulSetStrategy{
                                        Partition: &amp;rollingPartition,
                                },
                        },
                        Selector: &amp;metav1.LabelSelector{
                                MatchLabels: labels,
                        },
                        Template: v1.PodTemplateSpec{
                                ObjectMeta: metav1.ObjectMeta{
                                        Labels:      labels,
                                        Annotations: annotations,
                                },
                                Spec: v1.PodSpec{
                                        Affinity: &amp;v1.Affinity{
                                                NodeAffinity:    nodeAffinity,
                                                PodAntiAffinity: createPodAntiAffinity(cc.Spec.HardAntiAffinity, k8s.LabelsForCassandra(cc)),
                                        },
                                        Tolerations: tolerations,
                                        SecurityContext: &amp;v1.PodSecurityContext{
                                                RunAsUser:    cc.Spec.RunAsUser,
                                                RunAsNonRoot: func(b bool) *bool </span><span class="cov8" title="1">{ return &amp;b }</span>(true),
                                                FSGroup:      func(i int64) *int64 <span class="cov8" title="1">{ return &amp;i }</span>(1),
                                        },

                                        InitContainers: []v1.Container{
                                                createInitConfigContainer(cc),
                                                createCassandraBootstrapContainer(cc, status, dcRackName),
                                        },


                                        Containers: []v1.Container{
                                                createCassandraContainer(cc, status, dcRackName),
                                        },
                                        Volumes:                       volumes,
                                        RestartPolicy:                 v1.RestartPolicyAlways,
                                        TerminationGracePeriodSeconds: &amp;terminationPeriod,
                                },
                        },
                        VolumeClaimTemplates: volumeClaimTemplate,
                },
        }

        //Add secrets

        <span class="cov8" title="1">if (cc.Spec.ImagePullSecret != v1.LocalObjectReference{}) </span><span class="cov8" title="1">{
                ss.Spec.Template.Spec.ImagePullSecrets = []v1.LocalObjectReference{cc.Spec.ImagePullSecret}
        }</span>

        <span class="cov8" title="1">if (cc.Spec.ImageJolokiaSecret != v1.LocalObjectReference{}) </span><span class="cov0" title="0">{
                for idx, container := range ss.Spec.Template.Spec.Containers </span><span class="cov0" title="0">{
                        if container.Name == cassandraContainerName </span><span class="cov0" title="0">{
                                ss.Spec.Template.Spec.Containers[idx].Env = append(container.Env,
                                        v1.EnvVar{
                                                Name: "JOLOKIA_USER",
                                                ValueFrom: &amp;v1.EnvVarSource{
                                                        SecretKeyRef: &amp;v1.SecretKeySelector{
                                                                LocalObjectReference: cc.Spec.ImageJolokiaSecret,
                                                                Key:                  "username",
                                                        },
                                                },
                                        },
                                        v1.EnvVar{
                                                Name: "JOLOKIA_PASSWORD",
                                                ValueFrom: &amp;v1.EnvVarSource{
                                                        SecretKeyRef: &amp;v1.SecretKeySelector{
                                                                LocalObjectReference: cc.Spec.ImageJolokiaSecret,
                                                                Key:                  "password",
                                                        },
                                                },
                                        },
                                        v1.EnvVar{
                                                Name:  "CASSANDRA_AUTH_JOLOKIA",
                                                Value: "true"})
                        }</span>
                }
        }

        <span class="cov8" title="1">if err := patch.DefaultAnnotator.SetLastAppliedAnnotation(ss); err != nil </span><span class="cov0" title="0">{
                logrus.Warnf("[%s]: error while applying LastApplied Annotation on Statefulset", cc.Name)
        }</span>
        <span class="cov8" title="1">return ss</span>
}

func generateResourceQuantity(qs string) resource.Quantity <span class="cov8" title="1">{
        q, _ := resource.ParseQuantity(qs)
        return q
}</span>

func defineJvmMemory(resources v1.ResourceRequirements) JvmMemory <span class="cov8" title="1">{

        var mhs string

        if resources.Limits.Memory().IsZero() == false </span><span class="cov8" title="1">{
                m := float64(resources.Limits.Memory().Value()) * float64(0.25) // Maxheapsize should be 1/4 of container Memory Limit
                mi := int(m / float64(1048576))
                mhs = strings.Join([]string{strconv.Itoa(mi), "M"}, "")

        }</span> else<span class="cov0" title="0"> {
                mhs = defaultJvmMaxHeap
        }</span>

        <span class="cov8" title="1">return JvmMemory{
                maxHeapSize: mhs,
        }</span>
}

func generatePodDisruptionBudget(name string, namespace string, labels map[string]string, ownerRefs metav1.OwnerReference, maxUnavailable intstr.IntOrString) *policyv1beta1.PodDisruptionBudget <span class="cov8" title="1">{
        return &amp;policyv1beta1.PodDisruptionBudget{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "PodDisruptionBudget",
                        APIVersion: "policy/v1beta1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:            name,
                        Namespace:       namespace,
                        Labels:          labels,
                        OwnerReferences: []metav1.OwnerReference{ownerRefs},
                },
                Spec: policyv1beta1.PodDisruptionBudgetSpec{
                        MaxUnavailable: &amp;maxUnavailable,
                        Selector: &amp;metav1.LabelSelector{
                                MatchLabels: labels,
                        },
                },
        }
}</span>

func getCassandraResources(spec api.CassandraClusterSpec) v1.ResourceRequirements <span class="cov8" title="1">{
        return v1.ResourceRequirements{
                Requests: getRequests(spec.Resources),
                Limits:   getLimits(spec.Resources),
        }
}</span>

func getLimits(resources api.CassandraResources) v1.ResourceList <span class="cov8" title="1">{
        return generateResourceList(resources.Limits.CPU, resources.Limits.Memory)
}</span>

func getRequests(resources api.CassandraResources) v1.ResourceList <span class="cov8" title="1">{
        return generateResourceList(resources.Requests.CPU, resources.Requests.Memory)
}</span>

func generateResourceList(cpu string, memory string) v1.ResourceList <span class="cov8" title="1">{
        resources := v1.ResourceList{}
        if cpu != "" </span><span class="cov8" title="1">{
                resources[v1.ResourceCPU], _ = resource.ParseQuantity(cpu)
        }</span>
        <span class="cov8" title="1">if memory != "" </span><span class="cov8" title="1">{
                resources[v1.ResourceMemory], _ = resource.ParseQuantity(memory)
        }</span>
        <span class="cov8" title="1">return resources</span>
}

// createNodeAffinity creates NodeAffinity section for the statefulset.
// the selectors will be sorted byt the key name of the labels map before creating the statefulset
func createNodeAffinity(labels map[string]string) *v1.NodeAffinity <span class="cov8" title="1">{

        if len(labels) == 0 </span><span class="cov8" title="1">{
                return &amp;v1.NodeAffinity{}
        }</span>

        <span class="cov8" title="1">var nodeSelectors []v1.NodeSelectorRequirement

        //we make a new map in order to sort becaus a map is random by design
        keys := make([]string, 0, len(labels))
        for key := range labels </span><span class="cov8" title="1">{
                keys = append(keys, key)
        }</span>
        <span class="cov8" title="1">sort.Strings(keys) //sort by key
        for _, key := range keys </span><span class="cov8" title="1">{
                selector := v1.NodeSelectorRequirement{
                        Key:      key,
                        Operator: v1.NodeSelectorOpIn,
                        Values:   []string{labels[key]},
                }
                nodeSelectors = append(nodeSelectors, selector)
        }</span>

        <span class="cov8" title="1">return &amp;v1.NodeAffinity{
                RequiredDuringSchedulingIgnoredDuringExecution: &amp;v1.NodeSelector{
                        NodeSelectorTerms: []v1.NodeSelectorTerm{
                                {
                                        MatchExpressions: nodeSelectors,
                                },
                        },
                },
        }</span>
}

func createPodAntiAffinity(hard bool, labels map[string]string) *v1.PodAntiAffinity <span class="cov8" title="1">{
        podAffinityTerm := v1.PodAffinityTerm{
                TopologyKey: hostnameTopologyKey,
                LabelSelector: &amp;metav1.LabelSelector{
                        MatchLabels: labels,
                },
        }

        if hard </span><span class="cov8" title="1">{
                // Return a HARD anti-affinity (no same pods on one node)
                return &amp;v1.PodAntiAffinity{
                        RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{podAffinityTerm},
                }
        }</span>

        // Return a SOFT anti-affinity
        <span class="cov8" title="1">return &amp;v1.PodAntiAffinity{
                PreferredDuringSchedulingIgnoredDuringExecution: []v1.WeightedPodAffinityTerm{
                        v1.WeightedPodAffinityTerm{
                                Weight:          100,
                                PodAffinityTerm: podAffinityTerm,
                        },
                },
        }</span>
}

func createEnvVarForCassandraContainer(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        resources v1.ResourceRequirements, dcRackName string) []v1.EnvVar <span class="cov8" title="1">{
        name := cc.GetName()
        //in statefulset.go we surcharge this value with conditions
        seedList := cc.GetSeedList(&amp;status.SeedList)
        numTokensPerRacks := cc.GetNumTokensPerRacks(dcRackName)

        return []v1.EnvVar{
                v1.EnvVar{
                        Name:  "CASSANDRA_MAX_HEAP",
                        Value: defineJvmMemory(resources).maxHeapSize,
                },
                v1.EnvVar{
                        Name:  "CASSANDRA_SEEDS",
                        Value: seedList,
                },
                v1.EnvVar{
                        Name:  "CASSANDRA_CLUSTER_NAME",
                        Value: name,
                },
                v1.EnvVar{
                        Name: "POD_IP",
                        ValueFrom: &amp;v1.EnvVarSource{
                                FieldRef: &amp;v1.ObjectFieldSelector{
                                        APIVersion: "v1",
                                        FieldPath:  "status.podIP",
                                },
                        },
                },
                v1.EnvVar{
                        Name: "POD_NAME",
                        ValueFrom: &amp;v1.EnvVarSource{
                                FieldRef: &amp;v1.ObjectFieldSelector{
                                        APIVersion: "v1",
                                        FieldPath:  "metadata.name",
                                },
                        },
                },
                v1.EnvVar{
                        Name:  "CASSANDRA_GC_STDOUT",
                        Value: strconv.FormatBool(cc.Spec.GCStdout),
                },
                v1.EnvVar{
                        Name:  "CASSANDRA_NUM_TOKENS",
                        Value: strconv.Itoa(int(numTokensPerRacks)),
                },
                v1.EnvVar{
                        Name: "NODE_NAME",
                        ValueFrom: &amp;v1.EnvVarSource{
                                FieldRef: &amp;v1.ObjectFieldSelector{
                                        APIVersion: "v1",
                                        FieldPath:  "spec.nodeName",
                                },
                        },
                },
                v1.EnvVar{
                        Name: "CASSANDRA_DC",
                        ValueFrom: &amp;v1.EnvVarSource{
                                FieldRef: &amp;v1.ObjectFieldSelector{
                                        APIVersion: "v1",
                                        FieldPath:  "metadata.labels['cassandraclusters.db.orange.com.dc']",
                                },
                        },
                },
                v1.EnvVar{
                        Name: "CASSANDRA_RACK",
                        ValueFrom: &amp;v1.EnvVarSource{
                                FieldRef: &amp;v1.ObjectFieldSelector{
                                        APIVersion: "v1",
                                        FieldPath:  "metadata.labels['cassandraclusters.db.orange.com.rack']",
                                },
                        },
                },
        }
}</span>

// createInitConfigContainer allows to copy origin config files from docker image to /bootstrap directory
// where it will be surcharged by casskop needs, and by user's configmap changes
func createInitConfigContainer(cc *api.CassandraCluster) v1.Container <span class="cov8" title="1">{
        resources := getCassandraResources(cc.Spec)
        volumeMounts := generateCassandraVolumeMount(cc)

        //we want to mount boostrap volume to backup /etc/cassandra directory
        volumeMounts = deleteVolumeMount(volumeMounts, "bootstrap")
        volumeMounts = append(volumeMounts, v1.VolumeMount{Name: "bootstrap", MountPath: "/bootstrap"})

        return v1.Container{
                Name:            "init-config",
                Image:           cc.Spec.CassandraImage,
                ImagePullPolicy: cc.Spec.ImagePullPolicy,
                Command:         []string{"sh", "-c", cc.Spec.InitContainerCmd},
                VolumeMounts:    volumeMounts,
                Resources:       resources,
        }
}</span>

// createCassandraBootstrapContainer will copy jar from bootstrap image to /extra-lib/ directory.
// configure /etc/cassandra with Env var and with userConfigMap (if enabled) by running the run.sh script
func createCassandraBootstrapContainer(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        dcRackName string) v1.Container <span class="cov8" title="1">{
        resources := getCassandraResources(cc.Spec)
        volumeMounts := generateCassandraVolumeMount(cc)

        return v1.Container{
                Name:            "bootstrap",
                Image:           cc.Spec.BootstrapImage,
                ImagePullPolicy: cc.Spec.ImagePullPolicy,
                Env:             createEnvVarForCassandraContainer(cc, status, resources, dcRackName),
                VolumeMounts:    volumeMounts,
                Resources:       resources,
        }
}</span>

func getPos(slice []v1.VolumeMount, value string) int <span class="cov8" title="1">{
        for i, v := range slice </span><span class="cov8" title="1">{
                if v.Name == value </span><span class="cov8" title="1">{
                        return i
                }</span>
        }
        <span class="cov0" title="0">return -1</span>
}
func deleteVolumeMount(slice []v1.VolumeMount, value string) []v1.VolumeMount <span class="cov8" title="1">{
        if i := getPos(slice, value); i &gt;= 0 </span><span class="cov8" title="1">{
                slice = append(slice[:i], slice[i+1:]...)
                return slice
        }</span>
        <span class="cov0" title="0">return slice</span>
}

/* CreateCassandraContainer create the main container for cassandra
 */
func createCassandraContainer(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        dcRackName string) v1.Container <span class="cov8" title="1">{

        resources := getCassandraResources(cc.Spec)
        volumeMounts := generateCassandraVolumeMount(cc)

        var command = []string{}
        if cc.Spec.Debug </span><span class="cov0" title="0">{
                //debug: keep container running
                command = []string{"sh", "-c", "tail -f /dev/null"}
        }</span> else<span class="cov8" title="1"> {
                command = []string{"cassandra", "-f"}
        }</span>

        <span class="cov8" title="1">cassandraContainer := v1.Container{
                Name:            cassandraContainerName,
                Image:           cc.Spec.CassandraImage,
                ImagePullPolicy: cc.Spec.ImagePullPolicy,
                Command:         command,
                Ports: []v1.ContainerPort{
                        v1.ContainerPort{
                                Name:          cassandraIntraNodeName,
                                ContainerPort: cassandraIntraNodePort,
                                Protocol:      v1.ProtocolTCP,
                        },
                        v1.ContainerPort{
                                Name:          cassandraIntraNodeTLSName,
                                ContainerPort: cassandraIntraNodeTLSPort,
                                Protocol:      v1.ProtocolTCP,
                        },
                        v1.ContainerPort{
                                Name:          cassandraJMXName,
                                ContainerPort: cassandraJMX,
                                Protocol:      v1.ProtocolTCP,
                        },
                        v1.ContainerPort{
                                Name:          cassandraPortName,
                                ContainerPort: cassandraPort,
                                Protocol:      v1.ProtocolTCP,
                        },
                        v1.ContainerPort{
                                Name:          exporterCassandraJmxPortName,
                                ContainerPort: exporterCassandraJmxPort,
                                Protocol:      v1.ProtocolTCP,
                        },
                        v1.ContainerPort{
                                Name:          JolokiaPortName,
                                ContainerPort: JolokiaPort,
                                Protocol:      v1.ProtocolTCP,
                        },
                },

                SecurityContext: &amp;v1.SecurityContext{
                        Capabilities: &amp;v1.Capabilities{
                                Add: []v1.Capability{
                                        "IPC_LOCK",
                                },
                        },
                        ProcMount:              func(s v1.ProcMountType) *v1.ProcMountType </span><span class="cov8" title="1">{ return &amp;s }</span>(v1.DefaultProcMount),
                        ReadOnlyRootFilesystem: cc.Spec.ReadOnlyRootFilesystem,
                },

                Lifecycle: &amp;v1.Lifecycle{
                        PreStop: &amp;v1.Handler{
                                Exec: &amp;v1.ExecAction{
                                        Command: []string{
                                                "/bin/bash",
                                                "-c",
                                                "/etc/cassandra/pre_stop.sh",
                                        },
                                },
                        },
                },
                Env: createEnvVarForCassandraContainer(cc, status, resources, dcRackName),
                ReadinessProbe: &amp;v1.Probe{
                        InitialDelaySeconds: readinessInitialDelaySeconds,
                        TimeoutSeconds:      readinessHealthCheckTimeout,
                        PeriodSeconds:       readinessHealthCheckPeriod,
                        Handler: v1.Handler{
                                Exec: &amp;v1.ExecAction{
                                        Command: []string{
                                                "/bin/bash",
                                                "-c",
                                                "/etc/cassandra/ready-probe.sh",
                                        },
                                },
                        },
                },
                LivenessProbe: &amp;v1.Probe{
                        InitialDelaySeconds: livenessInitialDelaySeconds,
                        TimeoutSeconds:      livenessHealthCheckTimeout,
                        PeriodSeconds:       livenessHealthCheckPeriod,
                        Handler: v1.Handler{
                                Exec: &amp;v1.ExecAction{
                                        Command: []string{
                                                "/bin/bash",
                                                "-c",
                                                "/etc/cassandra/liveness-probe.sh",
                                        },
                                },
                        },
                },
                VolumeMounts: volumeMounts,
                Resources:    resources,
        }

        <span class="cov8" title="1">return cassandraContainer</span>
}
</pre>
		
		<pre class="file" id="file8" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "errors"
        "fmt"
        "regexp"

        "context"

        "github.com/sirupsen/logrus"
        "github.com/swarvanusg/go_jolokia"
        funk "github.com/thoas/go-funk"
        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
)

var localSystemKeyspaces = []string{"system", "system_schema"}

/*JolokiaURL returns the url used to connect to a Jolokia server based on a host and a port*/
func JolokiaURL(host string, port int) string <span class="cov8" title="1">{
        return fmt.Sprintf("http://%s:%d/jolokia/", host, port)
}</span>

// JolokiaClient is a structure that exposes a host and a jolokia client
type JolokiaClient struct {
        client *go_jolokia.JolokiaClient
        host   string
}

func (jolokiaClient *JolokiaClient) executeReadRequest(jolokiaRequest *go_jolokia.JolokiaRequest) (*go_jolokia.JolokiaReadResponse, error) <span class="cov8" title="1">{
        return (*go_jolokia.JolokiaClient)(jolokiaClient.client).ExecuteReadRequest(jolokiaRequest)
}</span>

func (jolokiaClient *JolokiaClient) executeOperation(mBean, operation string,
        arguments interface{}, pattern string) (*go_jolokia.JolokiaReadResponse, error) <span class="cov8" title="1">{
        return (*go_jolokia.JolokiaClient)(jolokiaClient.client).ExecuteOperation(mBean, operation, arguments, pattern)
}</span>

/*NewJolokiaClient returns a new Joloka client for the host name and port provided*/
func NewJolokiaClient(host string, port int, rcc *ReconcileCassandraCluster,
        secretRef v1.LocalObjectReference, namespace string) (*JolokiaClient, error) <span class="cov8" title="1">{
        jolokiaClient := JolokiaClient{go_jolokia.NewJolokiaClient(JolokiaURL(host, port)), host}
        logrus.WithFields(logrus.Fields{"host": host, "port": port,
                "secretRef": secretRef, "namespace": namespace}).Debug("Creating Jolokia connection")
        if (secretRef != v1.LocalObjectReference{}) </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"host": host, "port": port,
                        "secretRef": secretRef, "namespace": namespace}).Debug("Using Secret for Jolokia connection")
                secret := &amp;v1.Secret{
                        TypeMeta: metav1.TypeMeta{
                                Kind:       "Secret",
                                APIVersion: "v1",
                        },
                        ObjectMeta: metav1.ObjectMeta{
                                Name:      secretRef.Name,
                                Namespace: namespace,
                        },
                }
                err := rcc.client.Get(context.TODO(), types.NamespacedName{Name: secretRef.Name, Namespace: namespace}, secret)

                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"host": host, "port": port,
                                "secretRef": secretRef, "namespace": namespace}).Error("Can't get Jolokia secret")
                        return nil, err
                }</span>
                <span class="cov0" title="0">jolokiaClient.client.SetCredential(string(secret.Data["username"]), string(secret.Data["password"]))</span>
        }
        <span class="cov8" title="1">return (*JolokiaClient)(&amp;jolokiaClient), nil</span>
}

func checkJolokiaErrors(resp *go_jolokia.JolokiaReadResponse, err error) (*go_jolokia.JolokiaReadResponse, error) <span class="cov8" title="1">{
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">if resp.Error != "" </span><span class="cov0" title="0">{
                return nil, errors.New(resp.Error)
        }</span>
        <span class="cov8" title="1">return resp, nil</span>
}

func (jolokiaClient *JolokiaClient) leavingNodes() ([]string, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.db:type=StorageService", nil, "LeavingNodes")
        result, err := checkJolokiaErrors(jolokiaClient.client.ExecuteReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("Cannot get list of leaving nodes: %v", err.Error())
        }</span>
        <span class="cov8" title="1">v, isSlice := result.Value.([]interface{})
        if isSlice </span><span class="cov8" title="1">{
                leavingNodes := []string{}
                for _, value := range v </span><span class="cov8" title="1">{
                        str, isString := value.(string)
                        if isString </span><span class="cov8" title="1">{
                                leavingNodes = append(leavingNodes, str)
                        }</span>
                }
                <span class="cov8" title="1">logrus.WithFields(logrus.Fields{"leavingNodes": leavingNodes}).Debug("List of leaving nodes")
                return leavingNodes, nil</span>
        }
        <span class="cov0" title="0">return nil, fmt.Errorf("Value returned by Jolokia is not a slice: %v", result.Value)</span>
}

func (jolokiaClient *JolokiaClient) hostIDMap() (map[string]string, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.db:type=StorageService", nil, "HostIdMap")
        result, err := checkJolokiaErrors(jolokiaClient.client.ExecuteReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("Cannot get host id map: %v", err.Error())
        }</span>
        <span class="cov8" title="1">if m, ok := result.Value.(map[string]interface{}); ok </span><span class="cov8" title="1">{
                hostIDMap := map[string]string{}
                for k, v := range m </span><span class="cov8" title="1">{
                        str, isString := v.(string)
                        if isString </span><span class="cov8" title="1">{
                                hostIDMap[k] = str
                        }</span>
                }
                <span class="cov8" title="1">logrus.WithFields(logrus.Fields{"hostIDMap": hostIDMap}).Debug("Map of hosts IPs and IDs")
                return hostIDMap, nil</span>
        }
        <span class="cov0" title="0">return nil, fmt.Errorf("Value returned by Jolokia is not a map: %v", result.Value)</span>
}

func (jolokiaClient *JolokiaClient) keyspaces() ([]string, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.db:type=StorageService", nil, "Keyspaces")
        result, err := checkJolokiaErrors(jolokiaClient.client.ExecuteReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("Cannot get list of keyspaces: %v", err.Error())
        }</span>
        <span class="cov8" title="1">v, isSlice := result.Value.([]interface{})
        if isSlice </span><span class="cov8" title="1">{
                keyspaces := []string{}
                for _, value := range v </span><span class="cov8" title="1">{
                        str, isString := value.(string)
                        if isString </span><span class="cov8" title="1">{
                                keyspaces = append(keyspaces, str)
                        }</span>
                }
                <span class="cov8" title="1">return keyspaces, nil</span>
        }
        <span class="cov0" title="0">return nil, fmt.Errorf("Value returned by Jolokia is not a slice: %v", result.Value)</span>
}

func (jolokiaClient *JolokiaClient) nonLocalKeyspaces() ([]string, error) <span class="cov8" title="1">{
        keyspaces, err := jolokiaClient.keyspaces()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">nonLocalKeyspaces := []string{}
        for _, keyspace := range keyspaces </span><span class="cov8" title="1">{
                if !funk.Contains(localSystemKeyspaces, keyspace) </span><span class="cov8" title="1">{
                        nonLocalKeyspaces = append(nonLocalKeyspaces, keyspace)
                }</span>
        }
        <span class="cov8" title="1">return nonLocalKeyspaces, nil</span>
}

/*NodeCleanup triggers a cleanup of all keyspaces on the pod using a jolokia client and return the index of the last keyspace accessed and any error*/
func (jolokiaClient *JolokiaClient) NodeCleanup() error <span class="cov8" title="1">{
        keyspaces, err := jolokiaClient.nonLocalKeyspaces()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">return jolokiaClient.NodeCleanupKeyspaces(keyspaces)</span>
}

/*NodeCleanupKeyspaces triggers a cleanup of each keyspaces on the pod using a jolokia client and returns the index of the last keyspace accessed and any error*/
func (jolokiaClient *JolokiaClient) NodeCleanupKeyspaces(keyspaces []string) error <span class="cov8" title="1">{
        for _, keyspace := range keyspaces </span><span class="cov8" title="1">{
                logrus.Infof("[%s]: Cleanup of keyspace %s", jolokiaClient.host, keyspace)
                _, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                        "forceKeyspaceCleanup(java.lang.String,[Ljava.lang.String;)",
                        []interface{}{keyspace, []string{}}, ""))
                if err != nil </span><span class="cov0" title="0">{
                        logrus.Errorf("Cleanup of keyspace %s failed: %v", keyspace, err.Error())
                        return err
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

/*NodeUpgradeSSTables triggers an upgradeSSTables of each keyspaces through a jolokia client and returns any error*/
func (jolokiaClient *JolokiaClient) NodeUpgradeSSTables(threads int) error <span class="cov8" title="1">{
        keyspaces, err := jolokiaClient.keyspaces()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">return jolokiaClient.NodeUpgradeSSTablesKeyspaces(keyspaces, threads)</span>
}

/*NodeUpgradeSSTablesKeyspaces triggers an upgradeSSTables for a list of keyspaces through a jolokia connection and returns any error*/
func (jolokiaClient *JolokiaClient) NodeUpgradeSSTablesKeyspaces(keyspaces []string,
        threads int) error <span class="cov8" title="1">{
        for _, keyspace := range keyspaces </span><span class="cov8" title="1">{
                logrus.Infof("[%s]: Upgrade SSTables of keyspace %s", jolokiaClient.host, keyspace)
                _, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                        "upgradeSSTables(java.lang.String,boolean,int,[Ljava.lang.String;)",
                        []interface{}{keyspace, true, threads, []string{}}, ""))
                if err != nil </span><span class="cov0" title="0">{
                        logrus.Errorf("Upgrade SSTables of keyspace %s failed: %v", keyspace, err.Error())
                        return err
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

/*NodeRebuild triggers a rebuild of all keyspaces on the pod using a jolokia client and returns any error*/
func (jolokiaClient *JolokiaClient) NodeRebuild(dc string) error <span class="cov8" title="1">{
        _, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                "rebuild(java.lang.String)",
                []interface{}{dc}, ""))
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("Cannot rebuild from %s: %v", dc, err.Error())
        }</span>
        <span class="cov8" title="1">return nil</span>
}

/*NodeDecommission decommissions a node using a jolokia client and returns any error*/
func (jolokiaClient *JolokiaClient) NodeDecommission() error <span class="cov8" title="1">{
        _, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                "decommission", []interface{}{}, ""))
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("Cannot decommission: %v", err.Error())
        }</span>
        <span class="cov8" title="1">return nil</span>
}

/*NodeRemove remove node hostid on the pod using a jolokia client and returns any error*/
func (jolokiaClient *JolokiaClient) NodeRemove(hostid string) error <span class="cov0" title="0">{
        _, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                "removeNode",
                []interface{}{hostid}, ""))

        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("Cannot remove node %s: %v", hostid, err.Error())
        }</span>

        <span class="cov0" title="0">return nil</span>
}

/*NodeOperationMode returns OperationMode of a node using a jolokia client and returns any error*/
func (jolokiaClient *JolokiaClient) NodeOperationMode() (string, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.db:type=StorageService", nil, "OperationMode")
        result, err := checkJolokiaErrors(jolokiaClient.executeReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                return "", fmt.Errorf("Cannot get OperationMode: %v", err.Error())
        }</span>
        <span class="cov8" title="1">v, _ := result.Value.(string)
        return v, nil</span>
}

func (jolokiaClient *JolokiaClient) hasStreamingSessions() (bool, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.net:type=StreamManager", nil, "CurrentStreams")
        result, err := checkJolokiaErrors(jolokiaClient.executeReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                return true, fmt.Errorf("Cannot get list of current streams: %v", err.Error())
        }</span>
        <span class="cov8" title="1">val, _ := result.Value.([]interface{})
        return len(val) &gt; 0, nil</span>
}

func (jolokiaClient *JolokiaClient) hasCompactions(name string) (bool, error) <span class="cov8" title="1">{
        request := go_jolokia.NewJolokiaRequest(go_jolokia.READ, "org.apache.cassandra.db:type=CompactionManager", nil, "Compactions")
        result, err := checkJolokiaErrors(jolokiaClient.executeReadRequest(request))
        if err != nil </span><span class="cov0" title="0">{
                logrus.Error(err.Error())
                return true, fmt.Errorf("Cannot get list of current compactions: %v", err.Error())
        }</span>
        <span class="cov8" title="1">compactions, _ := result.Value.([]interface{})
        for _, compaction := range compactions </span><span class="cov8" title="1">{
                c := compaction.(map[string]interface{})
                if c["taskType"] == name </span><span class="cov8" title="1">{
                        return true, nil
                }</span>
        }
        <span class="cov8" title="1">return false, nil</span>
}

func (jolokiaClient *JolokiaClient) hasCleanupCompactions() (bool, error) <span class="cov8" title="1">{
        return jolokiaClient.hasCompactions("Cleanup")
}</span>

func (jolokiaClient *JolokiaClient) hasUpgradeSSTablesCompactions() (bool, error) <span class="cov0" title="0">{
        return jolokiaClient.hasCompactions("Upgrade sstables")
}</span>

func (jolokiaClient *JolokiaClient) hasLeavingNodes() (bool, error) <span class="cov0" title="0">{
        leavingNodes, err := jolokiaClient.leavingNodes()
        if err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>
        <span class="cov0" title="0">return len(leavingNodes) &gt; 0, nil</span>
}

/*HasDataInDC checks partition ranges of all non local keyspaces and ensure no data is replicated to the chosen datacenter*/
func (jolokiaClient *JolokiaClient) HasDataInDC(dc string) ([]string, error) <span class="cov8" title="1">{
        keyspaces, err := jolokiaClient.nonLocalKeyspaces()
        keyspacesWithDataInDC := []string{}
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">for _, keyspace := range keyspaces </span><span class="cov8" title="1">{
                dataFound, err := jolokiaClient.hasKeyspaceDataInDC(keyspace, dc)
                // Returns if there is an error
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov8" title="1">if dataFound </span><span class="cov8" title="1">{
                        keyspacesWithDataInDC = append(keyspacesWithDataInDC, keyspace)
                }</span>
        }
        <span class="cov8" title="1">return keyspacesWithDataInDC, nil</span>
}

func (jolokiaClient *JolokiaClient) hasKeyspaceDataInDC(keyspace, dc string) (bool, error) <span class="cov8" title="1">{
        result, err := checkJolokiaErrors(jolokiaClient.executeOperation("org.apache.cassandra.db:type=StorageService",
                "describeRingJMX", []interface{}{keyspace}, ""))
        if err != nil </span><span class="cov0" title="0">{
                return false, fmt.Errorf("Cannot describe ring using keyspace %s: %v", keyspace, err.Error())
        }</span>
        <span class="cov8" title="1">regexDc := regexp.MustCompile(fmt.Sprintf("datacenter:%s", dc))
        tokenRanges, _ := result.Value.([]interface{})
        for _, tokenRange := range tokenRanges </span><span class="cov8" title="1">{
                // Returns true as soon as we find one token range that is replicated to the chosen datacenter
                if regexDc.MatchString(tokenRange.(string)) </span><span class="cov8" title="1">{
                        return true, nil
                }</span>
        }
        <span class="cov8" title="1">return false, nil</span>
}
</pre>
		
		<pre class="file" id="file9" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "fmt"
        "regexp"
        "sort"
        "strconv"

        "github.com/Orange-OpenSource/casskop/pkg/k8s"

        v1 "k8s.io/api/core/v1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/client"
)

const (
        last  = true
        first = false
)

var reEndingNumber = regexp.MustCompile("[0-9]+$")

// PodContainersReady returns true if all container in the Pod are ready
func PodContainersReady(pod *v1.Pod) bool <span class="cov8" title="1">{
        if pod.Status.ContainerStatuses != nil &amp;&amp; len(pod.Status.ContainerStatuses) &gt; 0 </span><span class="cov8" title="1">{
                for _, c := range pod.Status.ContainerStatuses </span><span class="cov8" title="1">{
                        if c.Ready == false </span><span class="cov8" title="1">{
                                return false
                        }</span>
                }
                <span class="cov8" title="1">return true</span>
        }
        <span class="cov8" title="1">return false</span>
}

func (rcc *ReconcileCassandraCluster) GetPod(namespace, name string) (*v1.Pod, error) <span class="cov8" title="1">{

        pod := &amp;v1.Pod{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "Pod",
                        APIVersion: "v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return pod, rcc.client.Get(context.TODO(), types.NamespacedName{Name: name, Namespace: namespace}, pod)
}</span>

// GetLastOrFirstPod returns the last or first pod satisfying the selector and being in the namespace
func GetLastOrFirstPod(podsList *v1.PodList, last bool) (*v1.Pod, error) <span class="cov8" title="1">{
        nb := len(podsList.Items)

        if nb &lt; 1 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("there is no pod")
        }</span>

        <span class="cov8" title="1">idx := 0
        if last </span><span class="cov8" title="1">{
                idx = nb - 1
        }</span>

        <span class="cov8" title="1">items := podsList.Items[:]

        // Sort pod list using ending number in field ObjectMeta.Name
        sort.Slice(items, func(i, j int) bool </span><span class="cov8" title="1">{
                id1, _ := strconv.Atoi(reEndingNumber.FindString(items[i].ObjectMeta.Name))
                id2, _ := strconv.Atoi(reEndingNumber.FindString(items[j].ObjectMeta.Name))
                return id1 &lt; id2
        }</span>)

        <span class="cov8" title="1">pod := podsList.Items[idx]

        return &amp;pod, nil</span>
}

// GetFirstPod returns the first pod satisfying the selector and being in the namespace
func (rcc *ReconcileCassandraCluster) GetFirstPod(namespace string, selector map[string]string) (*v1.Pod, error) <span class="cov0" title="0">{
        podsList, err := rcc.ListPods(namespace, selector)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to get cassandra's pods: %v", err)
        }</span>
        <span class="cov0" title="0">return GetLastOrFirstPod(podsList, first)</span>
}

// GetLastPod returns the last pod satisfying the selector and being in the namespace
func (rcc *ReconcileCassandraCluster) GetLastPod(namespace string, selector map[string]string) (*v1.Pod, error) <span class="cov0" title="0">{
        podsList, err := rcc.ListPods(namespace, selector)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to get cassandra's pods: %v", err)
        }</span>
        <span class="cov0" title="0">return GetLastOrFirstPod(podsList, last)</span>
}

func (rcc *ReconcileCassandraCluster) UpdatePodLabel(pod *v1.Pod, label map[string]string) error <span class="cov0" title="0">{
        podToUpdate, err := rcc.GetPod(pod.Namespace, pod.Name)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">labels := k8s.MergeLabels(podToUpdate.GetLabels(), label)
        podToUpdate.SetLabels(labels)
        return rcc.UpdatePod(podToUpdate)</span>
}

//hasUnschedulablePod goal is to detect if Pods are unschedulable
// - for lake of resources cpu/memory
// - with bad docker image (imagepullbackoff)
// - or else to add
func (rcc *ReconcileCassandraCluster) hasUnschedulablePod(namespace string, dcName, rackName string) bool <span class="cov0" title="0">{
        podsList, err := rcc.ListPods(rcc.cc.Namespace, k8s.LabelsForCassandraDCRack(rcc.cc, dcName, rackName))
        if err != nil || len(podsList.Items) &lt; 1 </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">for _, pod := range podsList.Items </span><span class="cov0" title="0">{
                if pod.Status.Phase != v1.PodRunning &amp;&amp; pod.Status.Conditions != nil </span><span class="cov0" title="0">{
                        for _, cs := range pod.Status.ContainerStatuses </span><span class="cov0" title="0">{
                                if cs.Ready == false &amp;&amp; cs.State.Waiting != nil &amp;&amp; cs.State.Waiting.Reason == "ImagePullBackOff" </span><span class="cov0" title="0">{
                                        //TODO: delete Pod in this case so that it can be scheduled again if image in spec and image in
                                        // pod have changd
                                        return true
                                }</span>
                        }
                        <span class="cov0" title="0">for _, cond := range pod.Status.Conditions </span><span class="cov0" title="0">{
                                if (cond.Reason == v1.PodReasonUnschedulable) ||
                                        //try catch non ready pods
                                        (cond.Type == v1.PodReady &amp;&amp; cond.Status == v1.ConditionFalse &amp;&amp; cond.Reason == "ContainersNotReady") </span><span class="cov0" title="0">{
                                        return true
                                }</span>
                        }
                }
        }
        <span class="cov0" title="0">return false</span>
}

func (rcc *ReconcileCassandraCluster) ListPods(namespace string, selector map[string]string) (*v1.PodList, error) <span class="cov8" title="1">{

        clientOpt := &amp;client.ListOptions{
                Namespace:     namespace,
                LabelSelector: labels.SelectorFromSet(selector),
        }

        opt := []client.ListOption{
                clientOpt,
        }

        pl := &amp;v1.PodList{}
        return pl, rcc.client.List(context.TODO(), pl, opt...)
}</span>

func (rcc *ReconcileCassandraCluster) CreatePod(pod *v1.Pod) error <span class="cov8" title="1">{
        err := rcc.client.Create(context.TODO(), pod)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("pod already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to create cassandra pod: %cc", err)</span>
                //return err
        }
        <span class="cov8" title="1">return nil</span>
}

func (rcc *ReconcileCassandraCluster) UpdatePod(pod *v1.Pod) error <span class="cov0" title="0">{
        err := rcc.client.Update(context.TODO(), pod)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("pod already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to update cassandra pod: %cc", err)</span>
                //return err
        }
        <span class="cov0" title="0">return nil</span>
}

func (rcc *ReconcileCassandraCluster) CreateOrUpdatePod(namespace string, name string, pod *v1.Pod) error <span class="cov0" title="0">{
        storedPod, err := rcc.GetPod(namespace, pod.Name)
        if err != nil </span><span class="cov0" title="0">{
                // If no resource we need to create.
                if apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        return rcc.CreatePod(pod)
                }</span>
                <span class="cov0" title="0">return err</span>
        }

        // Already exists, need to Update.
        <span class="cov0" title="0">pod.ResourceVersion = storedPod.ResourceVersion
        return rcc.UpdatePod(pod)</span>
}

//DeletePod delete a pod
func (rcc *ReconcileCassandraCluster) DeletePod(pod *v1.Pod) error <span class="cov0" title="0">{
        err := rcc.client.Delete(context.TODO(), pod)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to delete Pod: %cc", err)
        }</span>
        <span class="cov0" title="0">return nil</span>
}

//ForceDeletePod delete a pod with a grace period of 0 seconds
func (rcc *ReconcileCassandraCluster) ForceDeletePod(pod *v1.Pod) error <span class="cov0" title="0">{
        err := rcc.client.Delete(context.TODO(), pod, client.GracePeriodSeconds(0))
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to delete Pod: %cc", err)
        }</span>
        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file10" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "errors"
        "fmt"
        "math/rand"
        "net"
        "os"
        "strings"

        "time"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        "github.com/Orange-OpenSource/casskop/pkg/k8s"
        "github.com/sirupsen/logrus"
        v1 "k8s.io/api/core/v1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/wait"
)

type finalizedOp struct {
        err           error
        dcRackName    string
        pod           v1.Pod
        operationName string
}

type op struct {
        Action     func(*ReconcileCassandraCluster, string, *api.CassandraCluster, string, v1.Pod) error
        Monitor    func(*JolokiaClient) (bool, error)
        PostAction func(*ReconcileCassandraCluster, *api.CassandraCluster, string, v1.Pod) error
}

var podOperationMap = map[string]op{
        api.OperationCleanup:         op{(*ReconcileCassandraCluster).runCleanup, (*JolokiaClient).hasCleanupCompactions, nil},
        api.OperationRebuild:         op{(*ReconcileCassandraCluster).runRebuild, (*JolokiaClient).hasStreamingSessions, nil},
        api.OperationUpgradeSSTables: op{(*ReconcileCassandraCluster).runUpgradeSSTables, (*JolokiaClient).hasUpgradeSSTablesCompactions, nil},
        api.OperationRemove: op{(*ReconcileCassandraCluster).runRemove, (*JolokiaClient).hasLeavingNodes,
                (*ReconcileCassandraCluster).postRunRemove}}

const breakResyncLoop bool = true
const continueResyncLoop bool = false
const monitorSleepDelay = 10 * time.Second
const deletedPvcTimeout = 30 * time.Second

var chanRunningOp = make(chan finalizedOp, 100)

func randomPodOperationKey() string <span class="cov8" title="1">{
        r := rand.Intn(len(podOperationMap))
        for k := range podOperationMap </span><span class="cov8" title="1">{
                if r == 0 </span><span class="cov8" title="1">{
                        return k
                }</span>
                <span class="cov8" title="1">r--</span>
        }
        <span class="cov0" title="0">return ""</span> // will never happen but make the compiler happy \_()_/
}

//executePodOperation will ensure that all Pod Operations which needed to be performed are done accordingly.
//It may return a breakResyncloop order meaning that the Operator won't update the statefulset until
//PodOperations are finishing gracefully.
func (rcc *ReconcileCassandraCluster) executePodOperation(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus) (bool, error) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        dcRackStatus := status.CassandraRackStatus[dcRackName]
        var breakResyncloop = false
        var err error

        // If we ask a ScaleDown, We can't update the Statefulset before the nodetool decommission has finished
        if rcc.weAreScalingDown(dcRackStatus) </span><span class="cov0" title="0">{
                //If a Decommission is Ongoing, we want to break the Resyncloop until the Decommission is succeed
                breakResyncloop, err = rcc.ensureDecommission(cc, dcName, rackName, status)
                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "dc": dcName, "rack": rackName,
                                "err": err}).Error("Error with decommission")
                }</span>
                <span class="cov0" title="0">return breakResyncloop, err</span>
        }

        // If LastClusterAction was a ScaleUp and It is Done then
        // Execute Cleanup On labeled Pods
        <span class="cov8" title="1">if status.LastClusterActionStatus == api.StatusDone </span><span class="cov8" title="1">{
                // If I enable test on ScaleUp then it may be too restrictive :
                // we won't be able to label pods to execute an action outside of a scaleup
                // &amp;&amp; status.LastClusterAction == api.ActionScaleUp {

                // Finalize operations that are done
                rcc.finalizeOperations(cc)

                // We run approximately a different operation each time
                rcc.ensureOperation(cc, dcName, rackName, status, randomPodOperationKey())
        }</span>

        <span class="cov8" title="1">return breakResyncloop, err</span>
}

//addPodOperationLabels will add Pod Labels labels on all Pod in the Current dcRackName
func (rcc *ReconcileCassandraCluster) addPodOperationLabels(cc *api.CassandraCluster, dcName string,
        rackName string, labels map[string]string) <span class="cov0" title="0">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        //Select all Pods in the Rack
        selector := k8s.MergeLabels(k8s.LabelsForCassandraDCRack(cc, dcName, rackName))

        podsList, err := rcc.ListPods(cc.Namespace, selector)

        if err != nil || len(podsList.Items) &lt; 1 </span><span class="cov0" title="0">{
                return
        }</span>

        <span class="cov0" title="0">for _, pod := range podsList.Items </span><span class="cov0" title="0">{
                if pod.Status.Phase != v1.PodRunning || pod.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov0" title="0">newlabels := k8s.MergeLabels(pod.GetLabels(), labels)

                pod.SetLabels(newlabels)
                err = rcc.UpdatePod(&amp;pod)
                if err != nil </span><span class="cov0" title="0">{
                        logrus.Errorf("[%s][%s]:[%s] UpdatePod Error: %v", cc.Name, dcRackName, pod.Name, err)
                }</span>

                <span class="cov0" title="0">logrus.Infof("[%s][%s]:[%s] UpdatePod Labels: %v", cc.Name, dcRackName, pod.Name, labels)</span>

        }
}

// initOperation finds pods waiting for operation to run
func (rcc *ReconcileCassandraCluster) initOperation(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        dcName, rackName, operationName string) []v1.Pod <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        selector := k8s.MergeLabels(k8s.LabelsForCassandraDCRack(cc, dcName, rackName),
                map[string]string{"operation-name": operationName,
                        "operation-status": api.StatusToDo})

        podsList, err := rcc.ListPods(cc.Namespace, selector)
        now := metav1.Now()

        podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation

        if err != nil || len(podsList.Items) &lt; 1 </span><span class="cov8" title="1">{

                if podLastOperation.Name == operationName &amp;&amp; podLastOperation.Status == api.StatusOngoing &amp;&amp; len(podLastOperation.Pods) &lt; 1 </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "operation": strings.Title(operationName)}).Debug("Set podLastOperation to Done as there is no more Pod to work on")
                        podLastOperation.Status = api.StatusDone
                        podLastOperation.EndTime = &amp;now

                        //We want dynamic view of status on CassandraCluster
                        rcc.updateCassandraStatus(cc, status)
                }</span>
                <span class="cov8" title="1">return nil</span>
        }

        <span class="cov0" title="0">if podLastOperation.Status != api.StatusOngoing </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "operation": strings.Title(operationName)}).Debug("Reset podLastOperation attributes")
                podLastOperation.Name = operationName
                podLastOperation.Status = api.StatusOngoing
                podLastOperation.StartTime = &amp;now
                podLastOperation.EndTime = nil
                podLastOperation.PodsOK = []string{}
                podLastOperation.PodsKO = []string{}
                podLastOperation.Pods = []string{}

                //We want dynamic view of status on CassandraCluster
                rcc.updateCassandraStatus(cc, status)
        }</span>

        <span class="cov0" title="0">return func(podsList *v1.PodList) []v1.Pod </span><span class="cov0" title="0">{
                podsSlice := make([]v1.Pod, 0)
                for _, pod := range podsList.Items </span><span class="cov0" title="0">{
                        if pod.Status.Phase != v1.PodRunning || pod.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                continue</span>
                        }
                        <span class="cov0" title="0">podsSlice = append(podsSlice, pod)</span>
                }
                <span class="cov0" title="0">return podsSlice</span>
        }(podsList)
}

func (rcc *ReconcileCassandraCluster) startOperation(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        pod v1.Pod, dcRackName, operationName string) error <span class="cov0" title="0">{
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "operation": strings.Title(operationName)}).Info("Start operation")
        labels := map[string]string{"operation-status": api.StatusOngoing,
                "operation-start": k8s.LabelTime(), "operation-end": ""}

        err := rcc.UpdatePodLabel(&amp;pod, labels)
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": pod.Name, "err": err.Error(), "labels": labels}).Debug("Failed to add labels to pod")
                return err
        }</span>

        <span class="cov0" title="0">podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation
        podLastOperation.Pods = append(podLastOperation.Pods, pod.Name)
        podLastOperation.PodsOK = k8s.RemoveString(podLastOperation.PodsOK, pod.Name)
        podLastOperation.PodsKO = k8s.RemoveString(podLastOperation.PodsKO, pod.Name)

        rcc.updateCassandraStatus(cc, status)

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "pod": pod.Name, "operation": strings.Title(operationName),
                "podLastOperation.OperatorName": podLastOperation.OperatorName,
                "podLastOperation.Pods":         podLastOperation.Pods}).Debug("Display information about pods")
        return nil</span>
}

// ensureOperation goal is to find pods with Labels :
//  - operation-name=xxxx and operation-status=To-Do
func (rcc *ReconcileCassandraCluster) ensureOperation(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus, operationName string) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        podsSlice, checkOnly := rcc.getPodsToWorkOn(cc, dcName, rackName, status, operationName)

        // For each pod where we need to run the operation on
        for _, pod := range podsSlice </span><span class="cov0" title="0">{
                hostName := fmt.Sprintf("%s.%s", pod.Spec.Hostname, pod.Spec.Subdomain)
                // We check if an operation is running
                if checkOnly </span><span class="cov0" title="0">{
                        go rcc.monitorOperation(hostName, cc, dcRackName, pod, operationName)
                        continue</span>
                }
                // Add the operatorName to the last pod operation in case the operator pod is replaced
                <span class="cov0" title="0">status.CassandraRackStatus[dcRackName].PodLastOperation.OperatorName = os.Getenv("POD_NAME")
                err := rcc.startOperation(cc, status, pod, dcRackName, operationName)
                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "pod": pod.Name, "err": err}).Debug("Failed to start operation on pod")
                        continue</span>
                }
                <span class="cov0" title="0">go rcc.runOperation(operationName, hostName, cc, dcRackName, pod, status)</span>
        }
}

func (rcc *ReconcileCassandraCluster) finalizeOperations(cc *api.CassandraCluster) <span class="cov8" title="1">{
        // Finalize all operations here to avoid update conflicts
        for chanOp := 0; chanOp &lt; len(chanRunningOp); chanOp++ </span><span class="cov0" title="0">{
                op := &lt;-chanRunningOp
                rcc.finalizeOperation(op.err, cc, op.dcRackName, op.pod, &amp;rcc.cc.Status,
                        strings.Title(op.operationName))
        }</span>
}

func (rcc *ReconcileCassandraCluster) runOperation(operationName, hostName string, cc *api.CassandraCluster, dcRackName string, pod v1.Pod,
        status *api.CassandraClusterStatus) <span class="cov0" title="0">{
        err := podOperationMap[operationName].Action(rcc, hostName, cc, dcRackName, pod)

        // If there is an error we finalize the operation but skip any existing post action
        if err != nil </span><span class="cov0" title="0">{
                chanRunningOp &lt;- finalizedOp{err, dcRackName, pod, operationName}
                return
        }</span>
        <span class="cov0" title="0">postAction := podOperationMap[operationName].PostAction
        if postAction != nil </span><span class="cov0" title="0">{
                err = postAction(rcc, cc, dcRackName, pod)
        }</span>
        <span class="cov0" title="0">chanRunningOp &lt;- finalizedOp{err, dcRackName, pod, operationName}</span>
}

/* ensureDecommission will ensure that the Last Pod of the StatefulSet will be decommissionned
        - If pod.status=To-DO then executeDecommission in the Pod and flag pod.status as **Ongoing**
        - If pod.status=Ongoing then if pod is not running then flag its status as **Done**
        - If pod.status=Done then delete Pod PVC and ChangeActionStatus to **Continue**

  it return breakResyncloop=true is we need to bypass update of the Statefulset.
  it return breakResyncloop=false if we want to call the ensureStatefulset method. */
func (rcc *ReconcileCassandraCluster) ensureDecommission(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus) (bool, error) <span class="cov0" title="0">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation

        if podLastOperation.Name != api.OperationDecommission </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "lastOperation": podLastOperation.Name}).Warnf("We should decommission only if pod.Operation == decommission, not the case here")
                return continueResyncLoop, nil
        }</span>

        <span class="cov0" title="0">switch podLastOperation.Status </span>{

        case api.StatusToDo:<span class="cov0" title="0">

                return rcc.ensureDecommissionToDo(cc, dcName, rackName, status)</span>

        case api.StatusOngoing,
                api.StatusFinalizing:<span class="cov0" title="0">

                if podLastOperation.Pods == nil || podLastOperation.Pods[0] == "" </span><span class="cov0" title="0">{
                        return breakResyncLoop, fmt.Errorf("For Status Ongoing we should have a PodLastOperation Pods item")
                }</span>
                <span class="cov0" title="0">lastPod, err := rcc.GetPod(cc.Namespace, podLastOperation.Pods[0])
                if err != nil </span><span class="cov0" title="0">{
                        if !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                                return breakResyncLoop, fmt.Errorf("failed to get last cassandra's pods '%s': %v",
                                        podLastOperation.Pods[0], err)
                        }</span>
                }

                //If Node is already Gone, We Delete PVC
                <span class="cov0" title="0">if apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        return rcc.ensureDecommissionFinalizing(cc, dcName, rackName, status, lastPod)
                }</span>

                //LastPod Still Exists
                <span class="cov0" title="0">if !PodContainersReady(lastPod) &amp;&amp; lastPod.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "lastPod": lastPod.Name}).Infof("We already asked Statefulset to scaleDown, waiting..")
                        return breakResyncLoop, nil

                }</span>

                <span class="cov0" title="0">hostName := fmt.Sprintf("%s.%s", lastPod.Spec.Hostname, lastPod.Spec.Subdomain)
                jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                        cc.Spec.ImageJolokiaSecret, cc.Namespace)

                if err != nil </span><span class="cov0" title="0">{
                        return breakResyncLoop, err
                }</span>

                <span class="cov0" title="0">operationMode, err := jolokiaClient.NodeOperationMode()

                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "hostName": hostName, "err": err}).Error("Jolokia call failed")
                        return breakResyncLoop, err
                }</span>

                <span class="cov0" title="0">if operationMode == "NORMAL" </span><span class="cov0" title="0">{
                        t, err := k8s.LabelTime2Time(lastPod.Labels["operation-start"])
                        if err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"operation-start": lastPod.Labels["operation-start"]}).Debugf("Can't parse time")
                        }</span>
                        <span class="cov0" title="0">now, _ := k8s.LabelTime2Time(k8s.LabelTime())

                        if t.Add(api.DefaultDelayWaitForDecommission * time.Second).After(now) </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                        "pod": lastPod.Name, "operationMode": operationMode,
                                        "DefaultDelayWaitForDecommission": api.DefaultDelayWaitForDecommission}).Info("Decommission was applied less " +
                                        "than DefaultDelayWaitForDecommission seconds, waiting")
                        }</span> else<span class="cov0" title="0"> {
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": lastPod.Name,
                                        "operationMode": operationMode}).Info("Seems that decommission has not correctly been applied, trying again..")
                                status.CassandraRackStatus[dcRackName].PodLastOperation.Status = api.StatusToDo
                        }</span>
                        <span class="cov0" title="0">return breakResyncLoop, nil</span>
                }

                <span class="cov0" title="0">if operationMode == "DECOMMISSIONED" || operationMode == "" </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "lastPod": lastPod.Name, "operationMode": operationMode}).Infof("Node has left the ring, " +
                                "waiting for statefulset Scaledown")
                        podLastOperation.Status = api.StatusFinalizing
                        return continueResyncLoop, nil
                }</span>

                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": lastPod.Name,
                        "operationMode": operationMode}).Info("Cassandra Node is decommissioning, we need to wait")
                return breakResyncLoop, nil</span>

                //In case of PodLastOperation Done we set LastAction to Continue to see if we need to decommission more
        case api.StatusDone:<span class="cov0" title="0">
                if podLastOperation.PodsOK == nil || podLastOperation.PodsOK[0] == "" </span><span class="cov0" title="0">{
                        return breakResyncLoop, fmt.Errorf("For Status Done we should have a PodLastOperation.PodsOK item")
                }</span>
                <span class="cov0" title="0">status.CassandraRackStatus[dcRackName].CassandraLastAction.Status = api.StatusContinue
                return breakResyncLoop, nil</span>

        default:<span class="cov0" title="0">
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "status": podLastOperation.Status}).Errorf("Error this should not happened: unknown status")</span>
        }

        <span class="cov0" title="0">return continueResyncLoop, nil</span>
}

//ensureDecommissionToDo
// State To-DO -&gt; Ongoing
// set podLastOperation.Pods and label targeted pod (lastPod)
func (rcc *ReconcileCassandraCluster) ensureDecommissionToDo(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus) (bool, error) <span class="cov0" title="0">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        var list []string
        podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation

        // We Get LastPod From StatefulSet
        lastPod, err := rcc.GetLastPod(cc.Namespace, k8s.LabelsForCassandraDCRack(cc, dcName, rackName))
        if err != nil </span><span class="cov0" title="0">{
                return breakResyncLoop, fmt.Errorf("Failed to get last cassandra's pods: %v", err)
        }</span>
        //If Pod is unschedulable, we bypass decommission (cassandra is not running)
        <span class="cov0" title="0">if lastPod.Status.Phase == v1.PodPending &amp;&amp;
                lastPod.Status.Conditions != nil &amp;&amp;
                lastPod.Status.Conditions[0].Reason == "Unschedulable" </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": lastPod.Name}).Warn("ScaleDown detected on a pending Pod. we don't launch decommission")
                podLastOperation.Status = api.StatusFinalizing
                podLastOperation.PodsOK = []string{}
                podLastOperation.Pods = append(list, lastPod.Name)
                podLastOperation.PodsKO = []string{}
                status.CassandraRackStatus[dcRackName].CassandraLastAction.Status = api.StatusContinue
                return continueResyncLoop, nil
        }</span>
        <span class="cov0" title="0">if lastPod.Status.Phase != v1.PodRunning || lastPod.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                return breakResyncLoop, fmt.Errorf("Pod is not running")
        }</span>
        <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "pod": lastPod.Name}).Info("ScaleDown detected, we launch decommission")

        //Ensure node is not leaving or absent from the ring
        hostName := fmt.Sprintf("%s.%s", lastPod.Spec.Hostname, lastPod.Spec.Subdomain)
        jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                cc.Spec.ImageJolokiaSecret, cc.Namespace)

        if err != nil </span><span class="cov0" title="0">{
                return breakResyncLoop, err
        }</span>

        <span class="cov0" title="0">operationMode, err := jolokiaClient.NodeOperationMode()

        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "hostName": hostName, "err": err}).Error("Jolokia call failed")
                return breakResyncLoop, err
        }</span>

        <span class="cov0" title="0">if operationMode == "DECOMMISSIONED" || operationMode == "" || operationMode == "LEAVING" </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": lastPod.Name}).Info("Node is leaving or has already been decommissioned")
                return breakResyncLoop, nil
        }</span>

        <span class="cov0" title="0">err = rcc.UpdatePodLabel(lastPod, map[string]string{
                "operation-status": api.StatusOngoing,
                "operation-start":  k8s.LabelTime(),
                "operation-name":   api.OperationDecommission})
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": lastPod.Name, "err": err}).Debug("Error updating pod")
        }</span>
        <span class="cov0" title="0">podLastOperation.Status = api.StatusOngoing
        podLastOperation.Pods = append(list, lastPod.Name)
        podLastOperation.PodsOK = []string{}
        podLastOperation.PodsKO = []string{}

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "pod": lastPod.Name}).Debug("Decommissioning cassandra node")

        go func() </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": lastPod.Name}).Debug("Node decommission starts")
                err = jolokiaClient.NodeDecommission()
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": lastPod.Name}).Debug("Node decommission ended")
                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "pod": lastPod.Name, "err": err}).Debug("Node decommission failed")
                }</span>
        }()
        <span class="cov0" title="0">return breakResyncLoop, nil</span>
}

//ensureDecommissionFinalizing
// State To-DO -&gt; Ongoing
func (rcc *ReconcileCassandraCluster) ensureDecommissionFinalizing(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus, lastPod *v1.Pod) (bool, error) <span class="cov0" title="0">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation

        pvcName := "data-" + podLastOperation.Pods[0]
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "pvc": pvcName}).Info("Decommission done -&gt; we delete PVC")
        pvc, err := rcc.GetPVC(cc.Namespace, pvcName)
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pvc": pvcName}).Error("Cannot get PVC")
        }</span>
        <span class="cov0" title="0">if err == nil </span><span class="cov0" title="0">{
                err = rcc.deletePVC(pvc)
                if err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "pvc": pvcName}).Error("Error deleting PVC, Please make manual Actions..")
                }</span> else<span class="cov0" title="0"> {
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "pvc": pvcName}).Info("PVC deleted")
                }</span>
        }

        <span class="cov0" title="0">podLastOperation.Status = api.StatusDone
        podLastOperation.PodsOK = []string{lastPod.Name}
        now := metav1.Now()
        podLastOperation.EndTime = &amp;now
        podLastOperation.Pods = []string{}
        //Important, We must break loop if multipleScaleDown has been asked
        return breakResyncLoop, nil</span>
}

func (rcc *ReconcileCassandraCluster) podsSlice(cc *api.CassandraCluster, status *api.CassandraClusterStatus,
        podLastOperation api.PodLastOperation, dcRackName, operationName, operatorName string) ([]v1.Pod, bool) <span class="cov8" title="1">{
        checkOnly := false
        podsSlice := make([]v1.Pod, 0)
        // Operator is different from when the previous operation was started
        // Set checkOnly to restart the monitoring function to wait until the operation is done
        if podLastOperation.Name == operationName &amp;&amp; podLastOperation.Status == api.StatusOngoing &amp;&amp;
                podLastOperation.OperatorName != "" &amp;&amp; podLastOperation.OperatorName != operatorName </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "podLastOperation.OperatorName": podLastOperation.OperatorName, "operatorName": operatorName,
                        "operation": strings.Title(operationName)}).Info("Operator's name is different, we enable checking routines")
                podLastOperation.OperatorName = operatorName

                for _, podName := range podLastOperation.Pods </span><span class="cov8" title="1">{
                        p, err := rcc.GetPod(cc.Namespace, podName)
                        if err != nil || p.Status.Phase != v1.PodRunning || p.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                continue</span>
                        }
                        <span class="cov8" title="1">podsSlice = append(podsSlice, *p)</span>
                }
                <span class="cov8" title="1">checkOnly = true
                return podsSlice, checkOnly</span>
        }
        <span class="cov8" title="1">dcName, rackName := cc.GetDCAndRackFromDCRackName(dcRackName)
        podsSlice = rcc.initOperation(cc, status, dcName, rackName, operationName)
        return podsSlice, checkOnly</span>
}

// Get pods that need an operation to run on
// Returns if checking is needed (can happen if the operator has been killed during an operation)
func (rcc *ReconcileCassandraCluster) getPodsToWorkOn(cc *api.CassandraCluster, dcName, rackName string,
        status *api.CassandraClusterStatus, operationName string) ([]v1.Pod, bool) <span class="cov8" title="1">{
        dcRackName := cc.GetDCRackName(dcName, rackName)
        var checkOnly bool
        podsSlice := make([]v1.Pod, 0)

        operatorName := os.Getenv("POD_NAME")
        if len(operatorName) == 0 </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName}).Info("POD_NAME is not defined and is mandatory")
                return podsSlice, checkOnly
        }</span>

        // Every time we update this variable we have to run updateCassandraStatus
        <span class="cov8" title="1">podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "podLastOperation.OperatorName": podLastOperation.OperatorName,
                "podLastOperation.Pods":         podLastOperation.Pods}).Debug("Display information about pods")

        podsSlice, checkOnly = rcc.podsSlice(cc, status, *podLastOperation, dcRackName, operationName, operatorName)

        if checkOnly </span><span class="cov0" title="0">{
                if len(podsSlice) == 0 </span><span class="cov0" title="0">{
                        // If previous running pods are done or cannot be found, we update the operator status
                        podLastOperation.Status = api.StatusDone
                        now := metav1.Now()
                        podLastOperation.EndTime = &amp;now
                }</span>
                <span class="cov0" title="0">rcc.updateCassandraStatus(cc, status)</span>
        }
        <span class="cov8" title="1">return podsSlice, checkOnly</span>
}

func (rcc *ReconcileCassandraCluster) updatePodLastOperation(clusterName, dcRackName, podName, operation string,
        status *api.CassandraClusterStatus, err error) <span class="cov0" title="0">{
        podLastOperation := &amp;status.CassandraRackStatus[dcRackName].PodLastOperation
        if err != nil </span><span class="cov0" title="0">{
                // We set the operation-status to Error on failing pods
                logrus.WithFields(logrus.Fields{"cluster": clusterName, "rack": dcRackName, "pod": podName,
                        "operation": operation, "err": err.Error()}).Error("Error in updatePodLastOperation")
                podLastOperation.PodsKO = append(podLastOperation.PodsKO, podName)
        }</span> else<span class="cov0" title="0"> {
                podLastOperation.PodsOK = append(podLastOperation.PodsOK, podName)
        }</span>
        // We remove the pod from the list of pods running the operation
        <span class="cov0" title="0">podLastOperation.Pods = k8s.RemoveString(podLastOperation.Pods, podName)</span>
}

/* finalizeOperation sets the labels on the pod where ran an operation depending on the error status
   It also updates status.CassandraRackStatus[dcRackName].PodLastOperation
*/
func (rcc *ReconcileCassandraCluster) finalizeOperation(err error, cc *api.CassandraCluster, dcRackName string,
        pod v1.Pod, status *api.CassandraClusterStatus, operationName string) <span class="cov0" title="0">{
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "status": status, "operation": operationName}).Debug("Finalize operation")
        labels := map[string]string{"operation-status": api.StatusDone, "operation-end": k8s.LabelTime()}

        if err != nil </span><span class="cov0" title="0">{
                labels["operation-status"] = api.StatusError
        }</span>

        <span class="cov0" title="0">ccRefreshed := cc.DeepCopy()

        rcc.updatePodLastOperation(cc.Name, dcRackName, pod.Name, strings.Title(operationName), status, err)

        for </span><span class="cov0" title="0">{
                if err = rcc.UpdatePodLabel(&amp;pod, labels); err != nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                                "labels": labels, "error": err}).Error("Can't update labels")
                        continue</span>
                }
                <span class="cov0" title="0">if err = rcc.updateCassandraStatus(ccRefreshed, status); err == nil </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                        "status": status, "error": err}).Debug("Got an error. Getting a new version of Cassandra Cluster")
                if rcc.client.Get(context.TODO(), types.NamespacedName{Name: cc.Name, Namespace: cc.Namespace}, ccRefreshed) == nil </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                                "status": status}).Debug("Got a new version of Cassandra Cluster")
                        continue</span>
                }
                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                        "status": status}).Debug("Can't get new version of Cassandra Cluster. Will try again")
                time.Sleep(retryInterval)</span>
        }
}

func (rcc *ReconcileCassandraCluster) monitorOperation(hostName string, cc *api.CassandraCluster, dcRackName string,
        pod v1.Pod, operationName string) <span class="cov0" title="0">{
        // Wait until there are no more cleanup compactions
        for </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": pod.Name, "host": hostName, "operation": operationName}).Info("Checking if operation is still running on node")
                jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                        cc.Spec.ImageJolokiaSecret, cc.Namespace)
                if err == nil </span><span class="cov0" title="0">{
                        operationIsRunning, err := podOperationMap[operationName].Monitor(jolokiaClient)
                        // When there is an error it returns true to try again during the next loop
                        if err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                        "pod": pod.Name, "host": hostName, "operation": operationName, "err": err}).Error("Got an error from Jolokia")
                                operationIsRunning = true
                        }</span>
                        <span class="cov0" title="0">if operationIsRunning != true </span><span class="cov0" title="0">{
                                break</span>
                        }
                }
                <span class="cov0" title="0">time.Sleep(monitorSleepDelay)</span>
        }
        <span class="cov0" title="0">postAction := podOperationMap[operationName].PostAction
        var err error
        if postAction != nil </span><span class="cov0" title="0">{
                err = postAction(rcc, cc, dcRackName, pod)
        }</span>
        <span class="cov0" title="0">chanRunningOp &lt;- finalizedOp{err, dcRackName, pod, operationName}</span>
}

func (rcc *ReconcileCassandraCluster) runUpgradeSSTables(hostName string, cc *api.CassandraCluster, dcRackName string,
        pod v1.Pod) error <span class="cov0" title="0">{
        var err error
        operation := strings.Title(api.OperationUpgradeSSTables)

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "hostName": hostName, "operation": operation}).Info("Operation start")

        jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                cc.Spec.ImageJolokiaSecret, cc.Namespace)
        if err == nil </span><span class="cov0" title="0">{
                err = jolokiaClient.NodeUpgradeSSTables(0)
        }</span>
        <span class="cov0" title="0">return err</span>
}

func (rcc *ReconcileCassandraCluster) runRebuild(hostName string, cc *api.CassandraCluster, dcRackName string, pod v1.Pod) error <span class="cov0" title="0">{
        var err error
        var rebuildFrom, labelSet = pod.GetLabels()["operation-argument"]
        operation := strings.Title(api.OperationRebuild)

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "hostName": hostName, "operation": operation}).Info("Operation start")

        if labelSet != true </span><span class="cov0" title="0">{
                err = errors.New("operation-argument is needed to get the datacenter name to rebuild from")
        }</span> else<span class="cov0" title="0"> if cc.IsValidDC(rebuildFrom) == false </span><span class="cov0" title="0">{
                err = fmt.Errorf("%s is not an existing datacenter", rebuildFrom)
        }</span>

        // In case of an error set the status on the pod and skip it
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "datacenter": rebuildFrom, "operation": operation}).Info("Execute the Jolokia Operation")

        jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                cc.Spec.ImageJolokiaSecret, cc.Namespace)
        if err == nil </span><span class="cov0" title="0">{
                err = jolokiaClient.NodeRebuild(rebuildFrom)
        }</span>
        <span class="cov0" title="0">return err</span>
}

func (rcc *ReconcileCassandraCluster) runRemove(hostName string, cc *api.CassandraCluster, dcRackName string, pod v1.Pod) error <span class="cov0" title="0">{
        operation := strings.Title(api.OperationRemove)

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "hostName": hostName, "operation": operation}).Info("Operation start")

        var label, labelSet = pod.GetLabels()["operation-argument"]
        if labelSet != true </span><span class="cov0" title="0">{
                return errors.New("operation-argument is needed to get the pod name to remove from the cluster")
        }</span>

        <span class="cov0" title="0">val := strings.Split(label, "_")
        podToRemove := val[0]
        var podIPToRemove string
        if len(val) == 2 </span><span class="cov0" title="0">{
                podIPToRemove = val[1]
        }</span>

        <span class="cov0" title="0">if podToRemove == "" &amp;&amp; podIPToRemove == "" </span><span class="cov0" title="0">{
                return fmt.Errorf("Expected format is `[Name][_IP]` with at least one value but none was found")
        }</span>
        // Name can be omitted in case the pod has already been deleted but then IP must be provided
        // When an IP is provided it will be used by the removeNode operation
        <span class="cov0" title="0">if podIPToRemove != "" &amp;&amp; net.ParseIP(podIPToRemove) == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("%s is not an IP address", podIPToRemove)
        }</span>

        <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "nodeToRemove": podToRemove, "operation": operation}).Info("Execute the Jolokia Operation")

        var lostPod *v1.Pod
        var err error
        if podToRemove != "" </span><span class="cov0" title="0">{
                // We delete the pod that is no longer part of the cluster
                lostPod, err = rcc.GetPod(cc.Namespace, podToRemove)
                if err != nil </span><span class="cov0" title="0">{
                        if !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                                return fmt.Errorf("Failed to get pod '%s': %v", podToRemove, err)
                        }</span>
                        // If we can't find it, it means it has already been deleted somehow. That's okay as long as we got its IP
                        <span class="cov0" title="0">if podIPToRemove == "" </span><span class="cov0" title="0">{
                                return fmt.Errorf("Pod %s not found. You need to provide its old IP to remove it from the cluster", podToRemove)
                        }</span>
                }
        }

        // If no IP is not provided, we grab it from the existing pod
        <span class="cov0" title="0">if podIPToRemove == "" </span><span class="cov0" title="0">{
                podIPToRemove = lostPod.Status.PodIP
                if podIPToRemove == "" </span><span class="cov0" title="0">{
                        return fmt.Errorf("Can't find an IP assigned to pod %s. You need to provide its old IP to remove it from the cluster", podToRemove)
                }</span>
        }

        <span class="cov0" title="0">jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc, cc.Spec.ImageJolokiaSecret, cc.Namespace)

        if err == nil </span><span class="cov0" title="0">{
                var hostIDMap map[string]string
                // Get hostID from internal map and pass it to removeNode function
                if hostIDMap, err = jolokiaClient.hostIDMap(); err == nil </span><span class="cov0" title="0">{
                        if hostID, keyFound := hostIDMap[podIPToRemove]; keyFound != true </span><span class="cov0" title="0">{
                                err = fmt.Errorf("Host with IP '%s' not found in hostIdMap", podIPToRemove)
                        }</span> else<span class="cov0" title="0"> {
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                                        "nodeToRemove": podToRemove, "operation": operation}).Info("Jolokia Remove node operation")
                                err = jolokiaClient.NodeRemove(hostID)
                        }</span>
                }
        }

        <span class="cov0" title="0">return err</span>
}

func (rcc *ReconcileCassandraCluster) waitUntilPvcIsDeleted(namespace, pvcName string) error <span class="cov0" title="0">{
        err := wait.Poll(retryInterval, deletedPvcTimeout, func() (done bool, err error) </span><span class="cov0" title="0">{
                _, err = rcc.GetPVC(namespace, pvcName)
                if err != nil &amp;&amp; apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"namespace": namespace,
                                "pvc": pvcName}).Info("PVC no longer exists")
                        return true, nil
                }</span>
                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"namespace": namespace,
                        "pvc": pvcName}).Info("Waiting for PVC to be deleted")
                return false, nil</span>
        })
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">return nil</span>
}

func (rcc *ReconcileCassandraCluster) postRunRemove(cc *api.CassandraCluster, dcRackName string, pod v1.Pod) error <span class="cov0" title="0">{
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name}).Info("Post operation start")

        var label, labelSet = pod.GetLabels()["operation-argument"]
        if labelSet != true </span><span class="cov0" title="0">{
                return errors.New("operation-argument is needed to get the pod name to remove from the cluster")
        }</span>
        <span class="cov0" title="0">podToRemove := strings.Split(label, "_")[0]

        if podToRemove == "" </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                        "rack": dcRackName}).Info("RemoveNode done. No pod was provided so we're done'")
                return nil
        }</span>

        // We delete the attached PVC
        <span class="cov0" title="0">pvcName := "data-" + podToRemove
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                "pvc": pvcName}).Info("RemoveNode done. We now delete its PVC")

        pvc, err := rcc.GetPVC(cc.Namespace, pvcName)
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pvc": pvcName}).Error("Cannot get PVC")
        }</span> else<span class="cov0" title="0"> {
                err = rcc.deletePVC(pvc)
                if err != nil &amp;&amp; !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                                "pvc": pvcName}).Error("Error deleting PVC, manual actions required...")
                        return err
                }</span>
                <span class="cov0" title="0">_ = rcc.waitUntilPvcIsDeleted(cc.Namespace, pvcName)
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pvc": pvcName}).Info("PVC deleted")</span>
        }

        // We delete the pod that is no longer part of the cluster
        <span class="cov0" title="0">lostPod, err := rcc.GetPod(cc.Namespace, podToRemove)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("Failed to get pod '%s': %v", podToRemove, err)
                }</span>
        }
        <span class="cov0" title="0">err = rcc.ForceDeletePod(lostPod)

        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": podToRemove}).Error("Error deleting Pod, manual actions required...")
        }</span> else<span class="cov0" title="0"> {
                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName,
                        "pod": podToRemove}).Info("Pod deleted")
        }</span>
        <span class="cov0" title="0">return err</span>
}

func (rcc *ReconcileCassandraCluster) runCleanup(hostName string, cc *api.CassandraCluster, dcRackName string, pod v1.Pod) error <span class="cov0" title="0">{
        var err error
        operation := strings.Title(api.OperationCleanup)

        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "hostName": hostName, "operation": operation}).Info("Operation start")

        // In case of an error set the status on the pod and skip it
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackName, "pod": pod.Name,
                "operation": operation}).Info("Execute the Jolokia Operation")

        jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                cc.Spec.ImageJolokiaSecret, cc.Namespace)

        if err == nil </span><span class="cov0" title="0">{
                err = jolokiaClient.NodeCleanup()
        }</span>
        <span class="cov0" title="0">return err</span>
}
</pre>
		
		<pre class="file" id="file11" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "fmt"

        policyv1beta1 "k8s.io/api/policy/v1beta1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
)

//GetPodDisruptionBudget return the PodDisruptionBudget name from the cluster in the namespace
func (rcc *ReconcileCassandraCluster) GetPodDisruptionBudget(namespace,
        name string) (*policyv1beta1.PodDisruptionBudget, error) <span class="cov8" title="1">{

        pdb := &amp;policyv1beta1.PodDisruptionBudget{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "PodDisruptionBudget",
                        APIVersion: "policy/v1beta1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return pdb, rcc.client.Get(context.TODO(), types.NamespacedName{Name: name, Namespace: namespace}, pdb)
}</span>

//CreatePodDisruptionBudget create a new PodDisruptionBudget pdb
func (rcc *ReconcileCassandraCluster) CreatePodDisruptionBudget(pdb *policyv1beta1.PodDisruptionBudget) error <span class="cov8" title="1">{
        err := rcc.client.Create(context.TODO(), pdb)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("PodDisruptionBudget already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to create cassandra PodDisruptionBudget: %cc", err)</span>
        }
        <span class="cov8" title="1">return nil</span>
}

//DeletePodDisruptionBudget delete a new PodDisruptionBudget pdb
func (rcc *ReconcileCassandraCluster) DeletePodDisruptionBudget(pdb *policyv1beta1.PodDisruptionBudget) error <span class="cov0" title="0">{
        err := rcc.client.Delete(context.TODO(), pdb)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to delete cassandra PodDisruptionBudget: %cc", err)
        }</span>
        <span class="cov0" title="0">return nil</span>
}

//UpdatePodDisruptionBudget updates an existing PodDisruptionBudget pdb
func (rcc *ReconcileCassandraCluster) UpdatePodDisruptionBudget(pdb *policyv1beta1.PodDisruptionBudget) error <span class="cov0" title="0">{
        err := rcc.client.Update(context.TODO(), pdb)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("PodDisruptionBudget already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to update cassandra PodDisruptionBudget: %cc", err)</span>
        }
        <span class="cov0" title="0">return nil</span>
}

//CreateOrUpdatePodDisruptionBudget Create PodDisruptionBudget if not existing, or update it if existing.
func (rcc *ReconcileCassandraCluster) CreateOrUpdatePodDisruptionBudget(pdb *policyv1beta1.PodDisruptionBudget) error <span class="cov8" title="1">{
        var err error
        rcc.storedPdb, err = rcc.GetPodDisruptionBudget(pdb.Namespace, pdb.Name)
        if err != nil </span><span class="cov8" title="1">{
                // If no resource we need to create.
                if apierrors.IsNotFound(err) </span><span class="cov8" title="1">{
                        return rcc.CreatePodDisruptionBudget(pdb)
                }</span>
                <span class="cov0" title="0">return err</span>
        }

        <span class="cov8" title="1">if *rcc.storedPdb.Spec.MaxUnavailable != *pdb.Spec.MaxUnavailable </span><span class="cov0" title="0">{
                rcc.DeletePodDisruptionBudget(pdb)
                //rcc.storedPdb = pdb
                return rcc.CreatePodDisruptionBudget(pdb)
        }</span>
        //We can't Update PorDisruptionBudget
        <span class="cov8" title="1">return nil</span>
        /*
                // Already exists, need to Update.
                pdb.ResourceVersion = rcc.storedPdb.ResourceVersion

                return rcc.UpdatePodDisruptionBudget(pdb)
        */
}
</pre>
		
		<pre class="file" id="file12" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"

        "k8s.io/api/core/v1"
        "sigs.k8s.io/controller-runtime/pkg/client"

        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/types"
)

func (rcc *ReconcileCassandraCluster) GetPVC(namespace, name string) (*v1.PersistentVolumeClaim, error) <span class="cov0" title="0">{

        o := &amp;v1.PersistentVolumeClaim{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "PersistentVolumeClaim",
                        APIVersion: "v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return o, rcc.client.Get(context.TODO(), types.NamespacedName{Name: name, Namespace: namespace}, o)
}</span>

func (rcc *ReconcileCassandraCluster) ListPVC(namespace string,
        selector map[string]string) (*v1.PersistentVolumeClaimList, error) <span class="cov0" title="0">{

        clientOpt := &amp;client.ListOptions{Namespace: namespace, LabelSelector: labels.SelectorFromSet(selector)}
        opt := []client.ListOption{
                clientOpt,
        }

        o := &amp;v1.PersistentVolumeClaimList{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "PersistentVolumeClaim",
                        APIVersion: "v1",
                },
        }

        return o, rcc.client.List(context.TODO(), o, opt...)
}</span>

func (rcc *ReconcileCassandraCluster) deletePVC(pvc *v1.PersistentVolumeClaim) error <span class="cov0" title="0">{

        return rcc.client.Delete(context.TODO(), pvc)

}</span>
</pre>
		
		<pre class="file" id="file13" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "encoding/json"
        "fmt"
        "reflect"
        "regexp"
        "strings"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        "github.com/Orange-OpenSource/casskop/pkg/k8s"
        "github.com/r3labs/diff"
        "github.com/sirupsen/logrus"
        v1 "k8s.io/api/core/v1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const topologyChangeRefused = "The Operator has refused the Topology change. "

func preventClusterDeletion(cc *api.CassandraCluster, value bool) <span class="cov8" title="1">{
        if value </span><span class="cov8" title="1">{
                cc.SetFinalizers([]string{"kubernetes.io/pvc-to-delete"})
                return
        }</span>
        <span class="cov8" title="1">cc.SetFinalizers([]string{})</span>
}

func updateDeletePvcStrategy(cc *api.CassandraCluster) <span class="cov8" title="1">{
        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "deletePVC": cc.Spec.DeletePVC,
                "finalizers": cc.Finalizers}).Debug("updateDeletePvcStrategy called")
        // Remove Finalizers if DeletePVC is not enabled
        if !cc.Spec.DeletePVC &amp;&amp; len(cc.Finalizers) &gt; 0 </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Info("Won't delete PVCs when nodes are removed")
                preventClusterDeletion(cc, false)
        }</span>
        // Add Finalizer if DeletePVC is enabled
        <span class="cov8" title="1">if cc.Spec.DeletePVC &amp;&amp; len(cc.Finalizers) == 0 </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Info("Will delete PVCs when nodes are removed")
                preventClusterDeletion(cc, true)
        }</span>
}

// CheckDeletePVC checks if DeletePVC is updated and update DeletePVC strategy
func (rcc *ReconcileCassandraCluster) CheckDeletePVC(cc *api.CassandraCluster) error <span class="cov8" title="1">{
        var oldCRD api.CassandraCluster
        if cc.Annotations[api.AnnotationLastApplied] == "" </span><span class="cov8" title="1">{
                return nil
        }</span>

        //We retrieved our last-applied-configuration stored in the CRD
        <span class="cov8" title="1">err := json.Unmarshal([]byte(cc.Annotations[api.AnnotationLastApplied]), &amp;oldCRD)
        if err != nil </span><span class="cov0" title="0">{
                logrus.Errorf("[%s]: Can't get Old version of CRD", cc.Name)
                return nil
        }</span>

        <span class="cov8" title="1">if cc.Spec.DeletePVC != oldCRD.Spec.DeletePVC </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Debug("DeletePVC has been updated")
                updateDeletePvcStrategy(cc)
                return rcc.client.Update(context.TODO(), cc)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// CheckNonAllowedChanges - checks if there are some changes on CRD that are not allowed on statefulset
// If a non Allowed Changed is Find we won't Update associated kubernetes objects, but we will put back the old value
// and Patch the CRD with correct values
func (rcc *ReconcileCassandraCluster) CheckNonAllowedChanges(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus) bool <span class="cov8" title="1">{
        var oldCRD api.CassandraCluster
        if cc.Annotations[api.AnnotationLastApplied] == "" </span><span class="cov8" title="1">{
                return false
        }</span>

        <span class="cov8" title="1">if lac, _ := cc.ComputeLastAppliedConfiguration(); string(lac) == cc.Annotations[api.AnnotationLastApplied] </span><span class="cov8" title="1">{
                //there are no changes to take care about
                return false
        }</span>

        //We retrieved our last-applied-configuration stored in the CRD
        <span class="cov8" title="1">err := json.Unmarshal([]byte(cc.Annotations[api.AnnotationLastApplied]), &amp;oldCRD)
        if err != nil </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Error("Can't get Old version of CRD")
                return false
        }</span>

        //Global scaleDown to 0 is forbidden
        <span class="cov8" title="1">if cc.Spec.NodesPerRacks == 0 </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).
                        Warningf("The Operator has refused the change on NodesPerRack=0 restore to OldValue[%d]",
                                oldCRD.Spec.NodesPerRacks)
                cc.Spec.NodesPerRacks = oldCRD.Spec.NodesPerRacks
                needUpdate = true
        }</span>
        //DataCapacity change is forbidden
        <span class="cov8" title="1">if cc.Spec.DataCapacity != oldCRD.Spec.DataCapacity </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).
                        Warningf("The Operator has refused the change on DataCapacity from [%s] to NewValue[%s]",
                                oldCRD.Spec.DataCapacity, cc.Spec.DataCapacity)
                cc.Spec.DataCapacity = oldCRD.Spec.DataCapacity
                needUpdate = true
        }</span>
        //DataStorage
        <span class="cov8" title="1">if cc.Spec.DataStorageClass != oldCRD.Spec.DataStorageClass </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).
                        Warningf("The Operator has refused the change on DataStorageClass from [%s] to NewValue[%s]",
                                oldCRD.Spec.DataStorageClass, cc.Spec.DataStorageClass)
                cc.Spec.DataStorageClass = oldCRD.Spec.DataStorageClass
                needUpdate = true
        }</span>

        <span class="cov8" title="1">if needUpdate </span><span class="cov8" title="1">{
                status.LastClusterAction = api.ActionCorrectCRDConfig
                return true
        }</span>

        <span class="cov8" title="1">var updateStatus string
        if needUpdate, updateStatus = CheckTopologyChanges(rcc, cc, status, &amp;oldCRD); needUpdate </span><span class="cov8" title="1">{
                if updateStatus != "" </span><span class="cov8" title="1">{
                        status.LastClusterAction = updateStatus
                }</span>
                <span class="cov8" title="1">if updateStatus == api.ActionCorrectCRDConfig </span><span class="cov8" title="1">{
                        cc.Spec.Topology = (&amp;oldCRD).Spec.Topology
                }</span>

                <span class="cov8" title="1">return true</span>
        }

        <span class="cov8" title="1">if updateStatus == api.ActionDeleteRack </span><span class="cov0" title="0">{
                return true
        }</span>

        <span class="cov8" title="1">if needUpdate, updateStatus = rcc.CheckNonAllowedScaleDown(cc, status, &amp;oldCRD); needUpdate </span><span class="cov8" title="1">{
                if updateStatus != "" </span><span class="cov8" title="1">{
                        status.LastClusterAction = updateStatus
                }</span>
                <span class="cov8" title="1">return true</span>
        }

        //What if we ask to changes Pod ressources ?
        // It is authorized, but the operator needs to detect it to prevent multiple statefulsets updates in the same time
        // the operator must handle thoses updates sequentially, so we flag each dcrackname with this information
        <span class="cov8" title="1">if !reflect.DeepEqual(cc.Spec.Resources, oldCRD.Spec.Resources) </span><span class="cov8" title="1">{
                logrus.Infof("[%s]: We ask to Change Pod Resources from %v to %v", cc.Name, oldCRD.Spec.Resources, cc.Spec.Resources)

                for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov8" title="1">{
                        dcName := cc.GetDCName(dc)
                        for rack := 0; rack &lt; cc.GetRackSize(dc); rack++ </span><span class="cov8" title="1">{

                                rackName := cc.GetRackName(dc, rack)
                                dcRackName := cc.GetDCRackName(dcName, rackName)
                                dcRackStatus := status.CassandraRackStatus[dcRackName]

                                logrus.Infof("[%s][%s]: Update Rack Status UpdateResources=Ongoing", cc.Name, dcRackName)
                                dcRackStatus.CassandraLastAction.Name = api.ActionUpdateResources
                                dcRackStatus.CassandraLastAction.Status = api.StatusToDo
                                now := metav1.Now()
                                status.CassandraRackStatus[dcRackName].CassandraLastAction.StartTime = &amp;now
                                status.CassandraRackStatus[dcRackName].CassandraLastAction.EndTime = nil
                        }</span>
                }

        }

        <span class="cov8" title="1">return false</span>
}

func generatePaths(s string) []string <span class="cov8" title="1">{
        return strings.Split(s, ".")
}</span>

// lookForFilter checks if filters are found in path and add the information to filtersFound if that's the case
func lookForFilter(path []string, filters [][]string, filtersFound *map[string]bool) <span class="cov8" title="1">{
        for _, filter := range filters </span><span class="cov8" title="1">{
                if 2*len(filter)+1 == len(path) </span><span class="cov8" title="1">{
                        currentPath := path[0]
                        for i := 2; i &lt; len(path)-1; i += 2 </span><span class="cov8" title="1">{
                                currentPath += "." + path[i]
                        }</span>
                        <span class="cov8" title="1">if currentPath == strings.Join(filter, ".") </span><span class="cov8" title="1">{
                                if _, ok := (*filtersFound)[currentPath]; !ok </span><span class="cov8" title="1">{
                                        (*filtersFound)[currentPath] = true
                                }</span>
                        }
                }
        }
}

// hasChange returns if there is a change with the type provided and matching all paths
// paths can be prepended with a - to specify  that it should not be found
// for instance ('DC', '-DC.Rack') means a DC change without a DC.Rack change
// changes of property NodesPerRacks are skipped
func hasChange(changelog diff.Changelog, changeType string, paths ...string) bool <span class="cov8" title="1">{
        regexPath := regexp.MustCompile("^\\-([^\\+]*)$")
        if len(changelog) == 0 </span><span class="cov8" title="1">{
                return false
        }</span>
        <span class="cov8" title="1">noPaths := len(paths) == 0
        includeFilters := [][]string{}
        excludeFilters := [][]string{}
        for _, path := range paths </span><span class="cov8" title="1">{
                if match := regexPath.FindStringSubmatch(path); len(match) &gt; 0 </span><span class="cov8" title="1">{
                        excludeFilters = append(excludeFilters, generatePaths(match[1]))
                        continue</span>
                }
                <span class="cov8" title="1">includeFilters = append(includeFilters, generatePaths(path))</span>
        }
        <span class="cov8" title="1">idx := "-1"
        var includedFiltersFound, excludedFiltersFound map[string]bool
        for _, cl := range changelog </span><span class="cov8" title="1">{
                // Only scan changes on Name/NumTokens
                if cl.Type == changeType &amp;&amp;
                        // DC Changes
                        (cl.Path[2] == "Name" || cl.Path[2] == "NumTokens" ||
                                // Rack changes
                                (len(cl.Path) &gt; 4 &amp;&amp; cl.Path[4] == "Name")) </span><span class="cov8" title="1">{
                        if noPaths </span><span class="cov8" title="1">{
                                return true
                        }</span>

                        // We reset counters when it's a new index
                        <span class="cov8" title="1">if cl.Path[1] != idx </span><span class="cov8" title="1">{
                                idx = cl.Path[1]
                                includedFiltersFound = map[string]bool{}
                                excludedFiltersFound = map[string]bool{}
                        }</span>

                        // We look for all matching filters
                        <span class="cov8" title="1">lookForFilter(cl.Path, includeFilters, &amp;includedFiltersFound)

                        // We look for all excluding filters
                        lookForFilter(cl.Path, excludeFilters, &amp;excludedFiltersFound)

                        if len(includedFiltersFound) == len(includeFilters) &amp;&amp; len(excludedFiltersFound) == 0 </span><span class="cov8" title="1">{
                                return true
                        }</span>
                }
        }
        <span class="cov8" title="1">return false</span>
}

//CheckTopologyChanges checks to see if the Operator accepts or refuses the CRD changes
func CheckTopologyChanges(rcc *ReconcileCassandraCluster, cc *api.CassandraCluster,
        status *api.CassandraClusterStatus, oldCRD *api.CassandraCluster) (bool, string) <span class="cov8" title="1">{

        changelog, _ := diff.Diff(oldCRD.Spec.Topology, cc.Spec.Topology)

        if hasChange(changelog, diff.UPDATE) ||
                hasChange(changelog, diff.DELETE, "DC.Rack", "-DC") ||
                hasChange(changelog, diff.CREATE, "DC.Rack", "-DC") </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                        topologyChangeRefused+"No change other than adding/removing a DC can happen: %v restored to %v",
                        cc.Spec.Topology, oldCRD.Spec.Topology)
                return true, api.ActionCorrectCRDConfig
        }</span>

        <span class="cov8" title="1">if cc.GetDCSize() &lt; oldCRD.GetDCSize()-1 </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                        topologyChangeRefused+"You can only remove 1 DC at a time, "+
                                "not only a Rack: %v restored to %v", cc.Spec.Topology, oldCRD.Spec.Topology)
                return true, api.ActionCorrectCRDConfig

        }</span>

        <span class="cov8" title="1">if cc.GetDCRackSize() &lt; oldCRD.GetDCRackSize() </span><span class="cov8" title="1">{

                if cc.Status.LastClusterAction == api.ActionScaleDown &amp;&amp;
                        cc.Status.LastClusterActionStatus != api.StatusDone </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name}).
                                Warningf(topologyChangeRefused +
                                        "You must wait to the end of ScaleDown to 0 before deleting a DC")
                        return true, api.ActionCorrectCRDConfig

                }</span>

                <span class="cov8" title="1">dcName := cc.GetRemovedDCName(oldCRD)

                //We need to check how many nodes were in the old CRD (before the user delete it)
                if found, nbNodes := oldCRD.GetDCNodesPerRacksFromName(dcName); found &amp;&amp; nbNodes &gt; 0 </span><span class="cov8" title="1">{
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name}).
                                Warningf(topologyChangeRefused+
                                        "You must scale down the DC %s to 0 before deleting it", dcName)
                        return true, api.ActionCorrectCRDConfig
                }</span>

                <span class="cov8" title="1">logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf("Removing DC %s", dcName)

                //We apply this change to the Cluster status
                return rcc.deleteDCObjects(cc, status, oldCRD)</span>
        }

        <span class="cov8" title="1">return false, ""</span>
}

func (rcc *ReconcileCassandraCluster) deleteDCObjects(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus, oldCRD *api.CassandraCluster) (bool, string) <span class="cov8" title="1">{

        dcRackNameToDeleteList := cc.FixCassandraRackList(status)

        if len(dcRackNameToDeleteList) &gt; 0 </span><span class="cov8" title="1">{

                for _, dcRackNameToDelete := range dcRackNameToDeleteList </span><span class="cov8" title="1">{

                        err := rcc.DeleteStatefulSet(cc.Namespace, cc.Name+"-"+dcRackNameToDelete)
                        if err != nil &amp;&amp; !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackNameToDelete}).Warnf(
                                        "Can't Delete Statefulset: %v", err)
                        }</span>
                        <span class="cov8" title="1">names := []string{
                                cc.Name + "-" + cc.GetDCFromDCRackName(dcRackNameToDelete),                   //name-dc
                                cc.Name + "-" + dcRackNameToDelete,                                           //name-dc-rack
                                cc.Name + "-" + cc.GetDCFromDCRackName(dcRackNameToDelete) + "-exporter-jmx", //name-dc-exporter-jmx
                        }
                        for i := range names </span><span class="cov8" title="1">{
                                err = rcc.DeleteService(cc.Namespace, names[i])
                                if err != nil &amp;&amp; !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "rack": dcRackNameToDelete}).Warnf(
                                                "Can't Delete Service: %v", err)
                                }</span>
                        }

                }
                <span class="cov8" title="1">return true, api.ActionDeleteDC</span>
        }
        <span class="cov0" title="0">return false, ""</span>
}

//CheckNonAllowedScaleDown goal is to discard the scaleDown to 0 is there is still replicated data towards the
// corresponding DC
func (rcc *ReconcileCassandraCluster) CheckNonAllowedScaleDown(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus,
        oldCRD *api.CassandraCluster) (bool, string) <span class="cov8" title="1">{

        if ok, dcName, dc := cc.FindDCWithNodesTo0(); ok </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Infof("Ask ScaleDown to 0 for dc %s", dcName)

                //We take the first Rack
                rackName := cc.GetRackName(dc, 0)

                selector := k8s.MergeLabels(k8s.LabelsForCassandraDCRack(cc, dcName, rackName))
                podsList, err := rcc.ListPods(cc.Namespace, selector)
                if err != nil || len(podsList.Items) &lt; 1 </span><span class="cov0" title="0">{
                        if err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                                        "The Operator has refused the ScaleDown (no pod found). "+
                                                "topology %v restored to %v", cc.Spec.Topology, oldCRD.Spec.Topology)
                                cc.Spec.Topology = oldCRD.Spec.Topology
                                return true, api.ActionCorrectCRDConfig
                        }</span>
                        //else there is already no pods so it's ok
                        <span class="cov0" title="0">return false, ""</span>
                }

                //We take the first available Pod
                <span class="cov8" title="1">for _, pod := range podsList.Items </span><span class="cov8" title="1">{
                        if pod.Status.Phase != v1.PodRunning || pod.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                continue</span>
                        }
                        <span class="cov8" title="1">hostName := fmt.Sprintf("%s.%s", pod.Spec.Hostname, pod.Spec.Subdomain)
                        logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Debugf("The Operator will ask node %s", hostName)
                        jolokiaClient, err := NewJolokiaClient(hostName, JolokiaPort, rcc,
                                cc.Spec.ImageJolokiaSecret, cc.Namespace)
                        var keyspacesWithData []string
                        if err == nil </span><span class="cov8" title="1">{
                                keyspacesWithData, err = jolokiaClient.HasDataInDC(dcName)
                        }</span>
                        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                                        "The Operator has refused the ScaleDown (HasDataInDC failed %s). ", err)
                                cc.Spec.Topology = oldCRD.Spec.Topology
                                return true, api.ActionCorrectCRDConfig
                        }</span>
                        <span class="cov8" title="1">if len(keyspacesWithData) != 0 </span><span class="cov8" title="1">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                                        "The Operator has refused the ScaleDown. Keyspaces still having data %v", keyspacesWithData)
                                cc.Spec.Topology = oldCRD.Spec.Topology
                                return true, api.ActionCorrectCRDConfig
                        }</span>
                        <span class="cov8" title="1">logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Warningf(
                                "Cassandra has no more replicated data on dc %s, we can scale Down to 0", dcName)
                        return false, ""</span>
                }
        }
        <span class="cov8" title="1">return false, ""</span>
}

//ReconcileRack will try to reconcile cassandra for each of the couple DC/Rack defined in the topology
func (rcc *ReconcileCassandraCluster) ReconcileRack(cc *api.CassandraCluster,
        status *api.CassandraClusterStatus) (err error) <span class="cov8" title="1">{

        for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov8" title="1">{
                dcName := cc.GetDCName(dc)
                for rack := 0; rack &lt; cc.GetRackSize(dc); rack++ </span><span class="cov8" title="1">{

                        rackName := cc.GetRackName(dc, rack)
                        dcRackName := cc.GetDCRackName(dcName, rackName)
                        if dcRackName == "" </span><span class="cov0" title="0">{
                                return fmt.Errorf("Name uses for DC and/or Rack are not good")
                        }</span>

                        //If we have added a dc/rack to the CRD, we add it to the Status
                        <span class="cov8" title="1">if _, ok := status.CassandraRackStatus[dcRackName]; !ok </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Infof("the DC(%s) and Rack(%s) does not exist, "+
                                        "initialize it in status", dcName, rackName)
                                cc.InitCassandraRackinStatus(status, dcName, rackName)
                                //Return will stop operator reconcile loop until next one
                                //used here to write CassandraClusterStatus properly
                                return nil
                        }</span>
                        <span class="cov8" title="1">dcRackStatus := status.CassandraRackStatus[dcRackName]

                        if cc.DeletionTimestamp != nil &amp;&amp; cc.Spec.DeletePVC </span><span class="cov0" title="0">{
                                rcc.DeletePVCs(cc, dcName, rackName)
                                //Go to next rack
                                continue</span>
                        }
                        <span class="cov8" title="1">Name := cc.Name + "-" + dcRackName
                        storedStatefulSet, err := rcc.GetStatefulSet(cc.Namespace, Name)
                        if err != nil </span><span class="cov8" title="1">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                        "dc-rack": dcRackName}).Infof("failed to get cassandra's statefulset (%s) %v", Name, err)
                        }</span> else<span class="cov8" title="1"> {

                                //Update CassandraClusterPhase
                                rcc.UpdateCassandraRackStatusPhase(cc, dcName, rackName, storedStatefulSet, status)

                                //Find if there is an Action to execute or to end
                                rcc.getNextCassandraClusterStatus(cc, dc, rack, dcName, rackName, storedStatefulSet, status)

                                //If Not in +Initial State
                                // Find if we have some Pod Operation to Execute, and execute thees
                                if dcRackStatus.Phase != api.ClusterPhaseInitial </span><span class="cov8" title="1">{
                                        breakResyncloop, err := rcc.executePodOperation(cc, dcName, rackName, status)
                                        if err != nil </span><span class="cov0" title="0">{
                                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "dc-rack": dcRackName,
                                                        "err": err}).Error("Executing pod operation failed")
                                        }</span>
                                        //For some Operations, we must NOT update the statefulset until Done.
                                        //So we block until OK
                                        <span class="cov8" title="1">if breakResyncloop </span><span class="cov0" title="0">{
                                                // If an Action is ongoing on the current Rack,
                                                // we don't want to check or start actions on Next Rack
                                                if dcRackStatus.Phase != api.ClusterPhaseRunning ||
                                                        dcRackStatus.CassandraLastAction.Status == api.StatusToDo ||
                                                        dcRackStatus.CassandraLastAction.Status == api.StatusOngoing ||
                                                        dcRackStatus.CassandraLastAction.Status == api.StatusContinue </span><span class="cov0" title="0">{
                                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name, "dc-rack": dcRackName,
                                                                "err": err}).Debug("Waiting Rack to be running before continuing, " +
                                                                "we break ReconcileRack Without Updating Statefulset")
                                                        return nil
                                                }</span>
                                                <span class="cov0" title="0">logrus.WithFields(logrus.Fields{"cluster": cc.Name, "dc-rack": dcRackName,
                                                        "LastActionName":   dcRackStatus.CassandraLastAction.Name,
                                                        "LastActionStatus": dcRackStatus.CassandraLastAction.Status}).Warning(
                                                        "Should Not see this message ;)" +
                                                                " Waiting Rack to be running before continuing, we loop on Next Rack, maybe we don't want that")
                                                continue</span>

                                        }
                                }
                        }
                        <span class="cov8" title="1">if err = rcc.ensureCassandraService(cc); err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Errorf("ensureCassandraService Error: %v", err)
                        }</span>

                        <span class="cov8" title="1">if err = rcc.ensureCassandraServiceMonitoring(cc, dcName); err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                        "dc-rack": dcRackName}).Errorf("ensureCassandraServiceMonitoring Error: %v", err)
                        }</span>

                        <span class="cov8" title="1">breakLoop, err := rcc.ensureCassandraStatefulSet(cc, status, dcName, dcRackName, dc, rack)
                        if err != nil </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                        "dc-rack": dcRackName}).Errorf("ensureCassandraStatefulSet Error: %v", err)
                        }</span>
                        <span class="cov8" title="1">if cc.Spec.UnlockNextOperation </span><span class="cov0" title="0">{
                                //If we enter specific change we remove unlockNextOperation from Spec
                                cc.Spec.UnlockNextOperation = false
                                needUpdate = true
                        }</span>
                        <span class="cov8" title="1">if breakLoop </span><span class="cov8" title="1">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name, "dc-rack": dcRackName,
                                        "err": err}).Debug("We just update Statefulset " +
                                        "we break ReconcileRack")
                                return nil
                        }</span>

                        //If the Phase is not running Then we won't check on Next Racks so we return
                        //We don't want to make change in 2 racks in a same time
                        <span class="cov8" title="1">if dcRackStatus.Phase != api.ClusterPhaseRunning ||
                                (dcRackStatus.CassandraLastAction.Status == api.StatusOngoing ||
                                        dcRackStatus.CassandraLastAction.Status == api.StatusFinalizing) </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                        "dc-rack": dcRackName}).Infof("Waiting Rack to be running before continuing, " +
                                        "we break ReconcileRack after updated statefulset")
                                return nil
                        }</span>
                }

        }

        //If cluster is deleted and DeletePVC is set, we can now stop preventing the cluster from being deleted
        //cause PVCs have been deleted
        <span class="cov8" title="1">if cc.DeletionTimestamp != nil &amp;&amp; cc.Spec.DeletePVC </span><span class="cov0" title="0">{
                preventClusterDeletion(cc, false)
                return rcc.client.Update(context.TODO(), cc)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// UpdateCassandraClusterStatusPhase goal is to calculate the Cluster Phase according to StatefulSet Status.
func UpdateCassandraClusterStatusPhase(cc *api.CassandraCluster, status *api.CassandraClusterStatus) <span class="cov8" title="1">{
        var setLastClusterActionStatus bool
        for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov8" title="1">{
                dcName := cc.GetDCName(dc)
                for rack := 0; rack &lt; cc.GetRackSize(dc); rack++ </span><span class="cov8" title="1">{

                        rackName := cc.GetRackName(dc, rack)
                        dcRackName := cc.GetDCRackName(dcName, rackName)
                        dcRackStatus, exist := status.CassandraRackStatus[dcRackName]
                        if !exist </span><span class="cov0" title="0">{
                                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Infof("the DC(%s) and Rack(%s) does not exist, "+
                                        "the rack status will be updated in next reconcile", dcName, rackName)
                                continue</span>
                        }

                        // If there is a lastAction ongoing in a Rack we update cluster lastaction accordingly
                        <span class="cov8" title="1">if dcRackStatus.CassandraLastAction.Status != api.StatusDone </span><span class="cov8" title="1">{
                                status.LastClusterActionStatus = dcRackStatus.CassandraLastAction.Status
                                status.LastClusterAction = dcRackStatus.CassandraLastAction.Name
                                setLastClusterActionStatus = true
                        }</span>

                        //If a rack is not running we return
                        <span class="cov8" title="1">if dcRackStatus.Phase != api.ClusterPhaseRunning </span><span class="cov8" title="1">{
                                status.Phase = dcRackStatus.Phase

                                if _, ok := cc.Status.CassandraRackStatus[dcRackName]; !ok ||
                                        cc.Status.CassandraRackStatus[dcRackName].Phase != dcRackStatus.Phase </span><span class="cov0" title="0">{
                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                                "dc-rack": dcRackName}).Infof("Update Rack Status: %s", dcRackStatus.Phase)
                                }</span>
                                <span class="cov8" title="1">return</span>
                        }

                }

        }
        //If there is no more action in racks, we update cluster
        <span class="cov8" title="1">if !setLastClusterActionStatus &amp;&amp;
                status.LastClusterActionStatus != api.StatusDone </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Infof("Action %s is done!", status.LastClusterAction)
                status.LastClusterActionStatus = api.StatusDone
                status.Phase = api.ClusterPhaseRunning
        }</span>
        //If cluster phase id not running, we update it
        <span class="cov8" title="1">if status.Phase != api.ClusterPhaseRunning &amp;&amp; status.LastClusterActionStatus == api.StatusDone </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": cc.Name}).Infof("Cluster is running")
                status.Phase = api.ClusterPhaseRunning
        }</span>

        <span class="cov8" title="1">return</span>
}

//FlipCassandraClusterUpdateSeedListStatus checks if all racks has the status UpdateSeedList=To-do
//It then sets UpdateSeedList to Ongoing to start the operation
func FlipCassandraClusterUpdateSeedListStatus(cc *api.CassandraCluster, status *api.CassandraClusterStatus) <span class="cov8" title="1">{

        //if global status is not yet  "Configuring", we skip this one
        if status.LastClusterAction == api.ActionUpdateSeedList &amp;&amp;
                status.LastClusterActionStatus == api.StatusConfiguring </span><span class="cov8" title="1">{
                var setOperationOngoing = true

                //Check if We need to start operation
                //all status of all racks must be "configuring"
                for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov8" title="1">{
                        dcName := cc.GetDCName(dc)
                        for rack := 0; rack &lt; cc.GetRackSize(dc); rack++ </span><span class="cov8" title="1">{

                                rackName := cc.GetRackName(dc, rack)
                                dcRackName := cc.GetDCRackName(dcName, rackName)
                                dcRackStatus := status.CassandraRackStatus[dcRackName]

                                //If not all racks are in "configuring", then we don't flip status to to-do except for initializing rack
                                if !(dcRackStatus.CassandraLastAction.Name == api.ActionUpdateSeedList &amp;&amp;
                                        dcRackStatus.CassandraLastAction.Status == api.StatusConfiguring) </span><span class="cov0" title="0">{
                                        //if rack is initializing we allow it to Flip
                                        if dcRackStatus.CassandraLastAction.Name != api.ClusterPhaseInitial </span><span class="cov0" title="0">{
                                                setOperationOngoing = false
                                        }</span>

                                        <span class="cov0" title="0">break</span>
                                }
                        }
                }

                //If all racks are in "configuring" state, we set all status to ToDo to trigger the operator actions
                <span class="cov8" title="1">if setOperationOngoing </span><span class="cov8" title="1">{
                        for dc := 0; dc &lt; cc.GetDCSize(); dc++ </span><span class="cov8" title="1">{
                                dcName := cc.GetDCName(dc)
                                for rack := 0; rack &lt; cc.GetRackSize(dc); rack++ </span><span class="cov8" title="1">{

                                        rackName := cc.GetRackName(dc, rack)
                                        dcRackName := cc.GetDCRackName(dcName, rackName)
                                        dcRackStatus := status.CassandraRackStatus[dcRackName]

                                        logrus.WithFields(logrus.Fields{"cluster": cc.Name,
                                                "dc-rack": dcRackName}).Infof("Update Rack Status UpdateSeedList=ToDo")
                                        dcRackStatus.CassandraLastAction.Name = api.ActionUpdateSeedList
                                        dcRackStatus.CassandraLastAction.Status = api.StatusToDo
                                }</span>
                        }
                }
        }
        <span class="cov8" title="1">return</span>
}
</pre>
		
		<pre class="file" id="file14" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"

        "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func (rcc *ReconcileCassandraCluster) DeleteService(namespace, name string) error <span class="cov8" title="1">{

        svc := &amp;v1.Service{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "Service",
                        APIVersion: "v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return rcc.client.Delete(context.TODO(), svc)
}</span>
</pre>
		
		<pre class="file" id="file15" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package cassandracluster

import (
        "context"
        "fmt"
        "strings"
        "time"

        "k8s.io/api/core/v1"

        "k8s.io/apimachinery/pkg/util/wait"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        "github.com/Orange-OpenSource/casskop/pkg/k8s"
        "github.com/allamand/godebug/pretty"
        "github.com/sirupsen/logrus"
        appsv1 "k8s.io/api/apps/v1"
        apiequality "k8s.io/apimachinery/pkg/api/equality"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        //patch "github.com/banzaicloud/k8s-objectmatcher/patch"
)

var (
        retryInterval = time.Second
        timeout       = time.Second * 5
)

//GetStatefulSet return the Statefulset name from the cluster in the namespace
func (rcc *ReconcileCassandraCluster) GetStatefulSet(namespace, name string) (*appsv1.StatefulSet, error) <span class="cov8" title="1">{

        ss := &amp;appsv1.StatefulSet{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "StatefulSet",
                        APIVersion: "apps/v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return ss, rcc.client.Get(context.TODO(), types.NamespacedName{Name: name, Namespace: namespace}, ss)
}</span>

func (rcc *ReconcileCassandraCluster) DeleteStatefulSet(namespace, name string) error <span class="cov8" title="1">{

        ss := &amp;appsv1.StatefulSet{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "StatefulSet",
                        APIVersion: "apps/v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
        }
        return rcc.client.Delete(context.TODO(), ss)
}</span>

//CreateStatefulSet create a new statefulset ss
func (rcc *ReconcileCassandraCluster) CreateStatefulSet(statefulSet *appsv1.StatefulSet) error <span class="cov8" title="1">{
        err := rcc.client.Create(context.TODO(), statefulSet)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("statefulset already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to create cassandra statefulset: %cc", err)</span>
                //return err
        }
        <span class="cov8" title="1">return nil</span>
}

//UpdateStatefulSet updates an existing statefulset ss
func (rcc *ReconcileCassandraCluster) UpdateStatefulSet(statefulSet *appsv1.StatefulSet) error <span class="cov8" title="1">{
        err := rcc.client.Update(context.TODO(), statefulSet)
        if err != nil </span><span class="cov0" title="0">{
                if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        return fmt.Errorf("statefulset already exists: %cc", err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("failed to update cassandra statefulset: %cc", err)</span>
        }
        //Check that the new revision of statefulset has been taken into account
        <span class="cov8" title="1">err = wait.Poll(retryInterval, timeout, func() (done bool, err error) </span><span class="cov8" title="1">{
                newSts, err := rcc.GetStatefulSet(statefulSet.Namespace, statefulSet.Name)
                if err != nil &amp;&amp; !apierrors.IsNotFound(err) </span><span class="cov0" title="0">{
                        return false, fmt.Errorf("failed to get cassandra statefulset: %cc", err)
                }</span>
                <span class="cov8" title="1">if statefulSet.ResourceVersion != newSts.ResourceVersion </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name, "statefulset": statefulSet.Name}).Info(
                                "Statefulset has new revision, we continue")
                        return true, nil
                }</span>
                <span class="cov8" title="1">logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name, "statefulset": statefulSet.Name}).Info(
                        "Waiting for new version of statefulset")
                return false, nil</span>
        })
        <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name, "statefulset": statefulSet.Name}).Info(
                        "Error Waiting for sts change")
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// sts1 = stored statefulset and sts2 = new generated statefulset
func statefulSetsAreEqual(sts1, sts2 *appsv1.StatefulSet) bool <span class="cov0" title="0">{

        //updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden.

        //Things we won't check :
        //TODO: we should rely on library to ease the comparison between 2 statefulset and to know If we need to apply
        // change or not cf: https://github.com/banzaicloud/k8s-objectmatcher

        /*
                patchResult, err := patch.DefaultPatchMaker.Calculate(sts1, sts2)
                if err != nil {
                        logrus.Infof("Template is different: " + pretty.Compare(sts1.Spec, sts2.Spec))
                        return false
                }
                if !patchResult.IsEmpty() {
                        logrus.Infof("Template is different: " + pretty.Compare(sts1.Spec, sts2.Spec))
                        return false
                }
        */
        sts1.Spec.Template.Spec.SchedulerName = sts2.Spec.Template.Spec.SchedulerName
        sts1.Spec.Template.Spec.DNSPolicy = sts2.Spec.Template.Spec.DNSPolicy // ClusterFirst
        for i := 0; i &lt; len(sts1.Spec.Template.Spec.Containers); i++ </span><span class="cov0" title="0">{
                sts1.Spec.Template.Spec.Containers[i].LivenessProbe = sts2.Spec.Template.Spec.Containers[i].LivenessProbe
                sts1.Spec.Template.Spec.Containers[i].ReadinessProbe = sts2.Spec.Template.Spec.Containers[i].ReadinessProbe

                sts1.Spec.Template.Spec.Containers[i].TerminationMessagePath = sts2.Spec.Template.Spec.Containers[i].TerminationMessagePath
                sts1.Spec.Template.Spec.Containers[i].TerminationMessagePolicy = sts2.Spec.Template.Spec.Containers[i].TerminationMessagePolicy
        }</span>

        <span class="cov0" title="0">for i := 0; i &lt; len(sts1.Spec.Template.Spec.InitContainers); i++ </span><span class="cov0" title="0">{
                sts1.Spec.Template.Spec.InitContainers[i].LivenessProbe = sts2.Spec.Template.Spec.InitContainers[i].LivenessProbe
                sts1.Spec.Template.Spec.InitContainers[i].ReadinessProbe = sts2.Spec.Template.Spec.InitContainers[i].ReadinessProbe

                sts1.Spec.Template.Spec.InitContainers[i].TerminationMessagePath = sts2.Spec.Template.Spec.InitContainers[i].TerminationMessagePath
                sts1.Spec.Template.Spec.InitContainers[i].TerminationMessagePolicy = sts2.Spec.Template.Spec.InitContainers[i].TerminationMessagePolicy
        }</span>

        //some defaultMode changes make falsepositif, so we bypass this, we already have check on configmap changes
        <span class="cov0" title="0">sts1.Spec.VolumeClaimTemplates = sts2.Spec.VolumeClaimTemplates
        sts1.Spec.PodManagementPolicy = sts2.Spec.PodManagementPolicy
        sts1.Spec.RevisionHistoryLimit = sts2.Spec.RevisionHistoryLimit

        if !apiequality.Semantic.DeepEqual(sts1.Spec, sts2.Spec) </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"statefulset": sts1.Name,
                        "namespace": sts1.Namespace}).Info("Template is different: " + pretty.CompareAndPrintDiff(sts1.Spec, sts2.Spec))

                return false
        }</span>

        <span class="cov0" title="0">return true</span>
}

//CreateOrUpdateStatefulSet Create statefulset if not existing, or update it if existing.
func (rcc *ReconcileCassandraCluster) CreateOrUpdateStatefulSet(statefulSet *appsv1.StatefulSet,
        status *api.CassandraClusterStatus, dcRackName string) (bool, error) <span class="cov8" title="1">{
        dcRackStatus := status.CassandraRackStatus[dcRackName]
        var err error
        now := metav1.Now()

        rcc.storedStatefulSet, err = rcc.GetStatefulSet(statefulSet.Namespace, statefulSet.Name)
        if err != nil </span><span class="cov8" title="1">{
                // If no resource we need to create.
                if apierrors.IsNotFound(err) </span><span class="cov8" title="1">{
                        return api.BreakResyncLoop, rcc.CreateStatefulSet(statefulSet)
                }</span>
                <span class="cov0" title="0">return api.ContinueResyncLoop, err</span>
        }

        //We will not Update the Statefulset
        // if there is existing disruptions on Pods
        // Or if we are not scaling Down the current statefulset
        <span class="cov8" title="1">if rcc.thereIsPodDisruption() </span><span class="cov8" title="1">{
                if rcc.weAreScalingDown(dcRackStatus) &amp;&amp; rcc.hasOneDisruptedPod() </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name,
                                "dc-rack": dcRackName}).Info("Cluster has 1 Pod Disrupted" +
                                "but that may be normal as we are decommissioning")
                }</span> else<span class="cov8" title="1"> if rcc.cc.Spec.UnlockNextOperation </span><span class="cov0" title="0">{
                        logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name,
                                "dc-rack": dcRackName}).Warn("Cluster has 1 Pod Disrupted" +
                                "but we have unlock the next operation")
                }</span> else<span class="cov8" title="1"> {
                        logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name,
                                "dc-rack": dcRackName}).Info("Cluster has Disruption on Pods, " +
                                "we wait before applying any change to statefulset")
                        return api.ContinueResyncLoop, nil
                }</span>
        }

        // Already exists, need to Update.
        <span class="cov0" title="0">statefulSet.ResourceVersion = rcc.storedStatefulSet.ResourceVersion
        // We grab the existing labels and add them back to the generated StatefulSet
        statefulSet.Spec.Template.SetLabels(rcc.storedStatefulSet.Spec.Template.GetLabels())

        //If UpdateSeedList=Ongoing, we allow the new SeedList to be propagated into the Statefulset
        //and change the status to Finalizing (it start a RollingUpdate)
        if dcRackStatus.CassandraLastAction.Name == api.ActionUpdateSeedList &amp;&amp;
                dcRackStatus.CassandraLastAction.Status == api.StatusToDo </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name, "dc-rack": dcRackName}).Info("Update SeedList on Rack")
                dcRackStatus.CassandraLastAction.Status = api.StatusOngoing
                dcRackStatus.CassandraLastAction.StartTime = &amp;now
        }</span> else<span class="cov0" title="0"> {

                //We need to keep the SeedList from the stored statefulset
                //we retrieve it in the Env CASSANDRA_SEEDS of the bootstrap container
                ic := getBootstrapContainerFromStatefulset(statefulSet)
                oldIc := getBootstrapContainerFromStatefulset(rcc.storedStatefulSet)
                for i, env := range ic.Env </span><span class="cov0" title="0">{
                        if env.Name == "CASSANDRA_SEEDS" </span><span class="cov0" title="0">{
                                for _, oldenv := range oldIc.Env </span><span class="cov0" title="0">{
                                        if oldenv.Name == "CASSANDRA_SEEDS" &amp;&amp; env.Value != oldenv.Value </span><span class="cov0" title="0">{
                                                ic.Env[i].Value = oldenv.Value
                                        }</span>
                                }
                        }
                }
        }

        //Hack for ScaleDown:
        //because before applying a scaledown at Kubernetes (statefulset) level we need to execute a cassandra decommission
        //we want the statefulset to only perform one scaledown at a time.
        //we have some call which will block the call of this method until the decommission is not OK, so here
        //we just need to change the scaledown value if more than 1 at a time.
        <span class="cov0" title="0">if *rcc.storedStatefulSet.Spec.Replicas-*statefulSet.Spec.Replicas &gt; 1 </span><span class="cov0" title="0">{
                *statefulSet.Spec.Replicas = *rcc.storedStatefulSet.Spec.Replicas - 1
        }</span>

        <span class="cov0" title="0">if dcRackStatus.CassandraLastAction.Name == api.ActionRollingRestart &amp;&amp;
                dcRackStatus.CassandraLastAction.Status == api.StatusToDo </span><span class="cov0" title="0">{
                statefulSet.Spec.Template.SetLabels(k8s.MergeLabels(statefulSet.Spec.Template.GetLabels(), map[string]string{
                        "rolling-restart": k8s.LabelTime()}))
                dcRackStatus.CassandraLastAction.Status = api.StatusOngoing
                dcRackStatus.CassandraLastAction.StartTime = &amp;now
        }</span>

        //Except for RollingRestart we check If Statefulset has changed
        <span class="cov0" title="0">if !rcc.cc.Spec.NoCheckStsAreEqual &amp;&amp;
                statefulSetsAreEqual(rcc.storedStatefulSet.DeepCopy(), statefulSet.DeepCopy()) </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name,
                        "dc-rack": dcRackName}).Debug("Statefulsets Are Equal: No Update")
                return api.ContinueResyncLoop, nil
        }</span>

        //If the Status is To-Do, then the Action will be Ongoing once we update the statefulset
        <span class="cov0" title="0">if dcRackStatus.CassandraLastAction.Status == api.StatusToDo </span><span class="cov0" title="0">{
                dcRackStatus.CassandraLastAction.Status = api.StatusOngoing
                dcRackStatus.CassandraLastAction.StartTime = &amp;now
                dcRackStatus.CassandraLastAction.EndTime = nil
        }</span>

        <span class="cov0" title="0">if !rcc.cc.Spec.NoCheckStsAreEqual &amp;&amp;
                dcRackStatus.CassandraLastAction.Status == api.StatusDone </span><span class="cov0" title="0">{
                logrus.WithFields(logrus.Fields{"cluster": rcc.cc.Name,
                        "dc-rack": statefulSet.Labels["dc-rack"]}).Debug("Start Updating Statefulset")
                dcRackStatus.CassandraLastAction.Status = api.StatusOngoing
                dcRackStatus.CassandraLastAction.Name = api.ActionUpdateStatefulSet
                dcRackStatus.CassandraLastAction.StartTime = &amp;now
                dcRackStatus.CassandraLastAction.EndTime = nil
        }</span>

        <span class="cov0" title="0">return api.BreakResyncLoop, rcc.UpdateStatefulSet(statefulSet)</span>

}

func getBootstrapContainerFromStatefulset(sts *appsv1.StatefulSet) *v1.Container <span class="cov8" title="1">{
        for _, ic := range sts.Spec.Template.Spec.InitContainers </span><span class="cov8" title="1">{
                if ic.Name == "bootstrap" </span><span class="cov8" title="1">{
                        return &amp;ic
                }</span>
        }
        <span class="cov0" title="0">return nil</span>
}

func getStoredSeedListTab(storedStatefulSet *appsv1.StatefulSet) []string <span class="cov8" title="1">{
        ic := getBootstrapContainerFromStatefulset(storedStatefulSet)
        //TODO: check if this test is necessary
        if ic != nil </span><span class="cov8" title="1">{
                for _, env := range ic.Env </span><span class="cov8" title="1">{
                        if env.Name == "CASSANDRA_SEEDS" </span><span class="cov8" title="1">{
                                return strings.Split(env.Value, ",")
                        }</span>
                }
        }
        <span class="cov0" title="0">return []string{}</span>
}

func isStatefulSetNotReady(storedStatefulSet *appsv1.StatefulSet) bool <span class="cov8" title="1">{
        if storedStatefulSet.Status.Replicas != *storedStatefulSet.Spec.Replicas ||
                storedStatefulSet.Status.ReadyReplicas != *storedStatefulSet.Spec.Replicas </span><span class="cov0" title="0">{
                return true
        }</span>
        <span class="cov8" title="1">return false</span>
}
</pre>
		
		<pre class="file" id="file16" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

// Provide K8S Client to be able to operate directly with K8S. For example to do exec cmd on a pod.
// Usage sample :
//   clientset, cfg := k8s.MustNewKubeClientAndConfig()
//         sdtout, stderr,err := k8s.ExecPodFromName(clientset.(*kubernetes.Clientset),cfg,capi.Namespace,nodename,killCmd)
//         if err!=nil {
//                logrus.Errorf("Error when run cmd to pod %s", nodename)
//                return fmt.Errorf("failed execute command on pods: %v", err)
//         }
package k8s

import (
        "bytes"
        "fmt"
        "net"
        "os"

        "github.com/operator-framework/operator-sdk/pkg/k8sutil"
        corev1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/client-go/kubernetes"
        "k8s.io/client-go/kubernetes/scheme"
        "k8s.io/client-go/rest"
        "k8s.io/client-go/tools/clientcmd"
        "k8s.io/client-go/tools/remotecommand"
)

//var clientset *kubernetes.Clientset
var clientset kubernetes.Interface
var cfg *rest.Config

//InitClient allow to setup an additional client to kubernetes API while operator-sdk don't gives us access to oit
func InitClient() <span class="cov0" title="0">{
        if clientset == nil </span><span class="cov0" title="0">{
                clientset, cfg = MustNewKubeClientAndConfig()
        }</span>
}

// Copy from k8sclient because not yet public
// MustNewKubeClientAndConfig returns the in-cluster config and kubernetes client
// or if KUBERNETES_CONFIG is given an out of cluster config and client
func MustNewKubeClientAndConfig() (kubernetes.Interface, *rest.Config) <span class="cov0" title="0">{
        //var cfg *rest.Config
        var err error
        if os.Getenv(k8sutil.KubeConfigEnvVar) != "" </span><span class="cov0" title="0">{
                cfg, err = outOfClusterConfig()
        }</span> else<span class="cov0" title="0"> {
                cfg, err = inClusterConfig()
        }</span>
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                panic(err)</span>
        }
        <span class="cov0" title="0">return kubernetes.NewForConfigOrDie(cfg), cfg</span>
}

// inClusterConfig returns the in-cluster config accessible inside a pod
func inClusterConfig() (*rest.Config, error) <span class="cov0" title="0">{
        // Work around https://github.com/kubernetes/kubernetes/issues/40973
        // See https://github.com/coreos/etcd-operator/issues/731#issuecomment-283804819
        if len(os.Getenv("KUBERNETES_SERVICE_HOST")) == 0 </span><span class="cov0" title="0">{
                addrs, err := net.LookupHost("kubernetes.default.svc")
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov0" title="0">os.Setenv("KUBERNETES_SERVICE_HOST", addrs[0])</span>
        }
        <span class="cov0" title="0">if len(os.Getenv("KUBERNETES_SERVICE_PORT")) == 0 </span><span class="cov0" title="0">{
                os.Setenv("KUBERNETES_SERVICE_PORT", "443")
        }</span>
        <span class="cov0" title="0">return rest.InClusterConfig()</span>
}

func outOfClusterConfig() (*rest.Config, error) <span class="cov0" title="0">{
        kubeconfig := os.Getenv(k8sutil.KubeConfigEnvVar)
        config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
        return config, err
}</span>

//inspiration
//https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/exec.go
//func ExecPodFromName(clientset *kubernetes.Clientset, cfg *rest.Config, namespace string, name string, cmd []string) (string, string, error) {
func ExecPodFromName(namespace string, name string, cmd []string) (string, string, error) <span class="cov0" title="0">{
        pod, err := clientset.CoreV1().Pods(namespace).Get(name, metav1.GetOptions{})
        if err != nil </span><span class="cov0" title="0">{
                return "", "", fmt.Errorf("could not get pod info: %v", err)
        }</span>

        <span class="cov0" title="0">if pod.Status.Phase == corev1.PodSucceeded || pod.Status.Phase == corev1.PodFailed </span><span class="cov0" title="0">{
                return "", "", fmt.Errorf("cannot exec into a container in a completed pod; current phase is %s", pod.Status.Phase)
        }</span>

        //        return ExecPod(clientset, cfg, namespace, pod, cmd)
        <span class="cov0" title="0">return ExecPod(namespace, pod, cmd)</span>

}

//inspiration
//https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/exec.go
//func ExecPod(clientset *kubernetes.Clientset, cfg *rest.Config, namespace string, pod *corev1.Pod, cmd []string) (string, string, error) {
func ExecPod(namespace string, pod *corev1.Pod, cmd []string) (string, string, error) <span class="cov0" title="0">{

        if len(pod.Spec.Containers) != 1 </span><span class="cov0" title="0">{
                return "", "", fmt.Errorf("could not determine which container to use")
        }</span>

        // build the remoteexec
        <span class="cov0" title="0">req := clientset.CoreV1().RESTClient().Post().
                Resource("pods").
                Name(pod.Name).
                Namespace(namespace).
                SubResource("exec")

        req.VersionedParams(&amp;corev1.PodExecOptions{
                Container: pod.Spec.Containers[0].Name,
                Command:   cmd,
                Stdin:     false,
                Stdout:    true,
                Stderr:    true,
                TTY:       false,
        }, scheme.ParameterCodec)

        exec, err := remotecommand.NewSPDYExecutor(cfg, "POST", req.URL())
        if err != nil </span><span class="cov0" title="0">{
                return "", "", fmt.Errorf("could not init remote executor: %v", err)
        }</span>

        <span class="cov0" title="0">var stdout, stderr bytes.Buffer
        err = exec.Stream(remotecommand.StreamOptions{
                Stdout: &amp;stdout,
                Stderr: &amp;stderr,
                Tty:    false,
        })

        return stdout.String(), stderr.String(), err</span>

}
</pre>
		
		<pre class="file" id="file17" style="display: none">// Copyright 2019 Orange
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
//         You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//         See the License for the specific language governing permissions and
// limitations under the License.

package k8s

import (
        "fmt"
        "regexp"
        "time"

        api "github.com/Orange-OpenSource/casskop/pkg/apis/db/v1alpha1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// Regex to extract date from label
var ReLabelTime = regexp.MustCompile(`(?P&lt;y&gt;\d{4})(?P&lt;m&gt;\d{2})(?P&lt;d&gt;\d{2})T(?P&lt;hh&gt;\d{2})(?P&lt;mm&gt;\d{2})(?P&lt;ss&gt;\d{2})`)

// addOwnerRefToObject appends the desired OwnerReference to the object
func AddOwnerRefToObject(o metav1.Object, r metav1.OwnerReference) <span class="cov0" title="0">{
        o.SetOwnerReferences(append(o.GetOwnerReferences(), r))
}</span>

// labelsForCassandra returns the labels for selecting the resources
// belonging to the given name.
func LabelsForCassandraDCRack(cc *api.CassandraCluster, dcName string, rackName string) map[string]string <span class="cov8" title="1">{
        m := map[string]string{
                "app":                                  "cassandracluster",
                "cassandracluster":                     cc.GetName(),
                "dc-rack":                              cc.GetDCRackName(dcName, rackName),
                "cassandraclusters.db.orange.com.dc":   dcName,
                "cassandraclusters.db.orange.com.rack": rackName,
        }
        return MergeLabels(cc.GetLabels(), m)
}</span>

func LabelsForCassandraDC(cc *api.CassandraCluster, dcName string) map[string]string <span class="cov0" title="0">{
        m := map[string]string{
                "app":                                "cassandracluster",
                "cassandracluster":                   cc.GetName(),
                "cassandraclusters.db.orange.com.dc": dcName,
        }
        return MergeLabels(cc.GetLabels(), m)
}</span>

func LabelsForCassandra(cc *api.CassandraCluster) map[string]string <span class="cov0" title="0">{
        m := map[string]string{
                "app":              "cassandracluster",
                "cassandracluster": cc.GetName(),
        }
        return MergeLabels(cc.GetLabels(), m)
}</span>

//RemoveString remove a string from a slice
//s := []string{"one", "two", "three"}
//s = RemoveString(s, "two")
//fmt.Println(s) // Prints [one three]
func RemoveString(s []string, r string) []string <span class="cov0" title="0">{
        for i, v := range s </span><span class="cov0" title="0">{
                if v == r </span><span class="cov0" title="0">{
                        return append(s[:i], s[i+1:]...)
                }</span>
        }
        <span class="cov0" title="0">return s</span>
}

func Contains(s []string, e string) bool <span class="cov8" title="1">{
        for _, a := range s </span><span class="cov8" title="1">{
                if a == e </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

//ContainSlice return true if each element of n exists in ref
func ContainSlice(ref []string, n []string) bool <span class="cov8" title="1">{
        for i := range n </span><span class="cov8" title="1">{
                if !Contains(ref, n[i]) </span><span class="cov8" title="1">{
                        return false
                }</span>
        }
        <span class="cov8" title="1">return true</span>
}

//MergeSlice will add ad the end of old any elements of new which is missing
//we want to keep the order of elements in old
func MergeSlice(old []string, new []string) []string <span class="cov8" title="1">{
        var result []string

        //result start from old but we don't add if elem don't exists in new
        for i := range old </span><span class="cov8" title="1">{
                if Contains(new, old[i]) </span><span class="cov8" title="1">{
                        result = append(result, old[i])
                }</span>
        }
        //Do we need to add more elements from new ?
        <span class="cov8" title="1">for i := range new </span><span class="cov8" title="1">{
                if !Contains(old, new[i]) </span><span class="cov8" title="1">{
                        result = append(result, new[i])
                }</span>
        }

        <span class="cov8" title="1">return result</span>
}

// MergeLabels merges all the label maps received as argument into a single new label map.
func MergeLabels(allLabels ...map[string]string) map[string]string <span class="cov8" title="1">{
        res := map[string]string{}

        for _, labels := range allLabels </span><span class="cov8" title="1">{
                for k, v := range labels </span><span class="cov8" title="1">{
                        res[k] = v
                }</span>
        }
        <span class="cov8" title="1">return res</span>
}

// asOwner returns an owner reference set as the cassandra cluster CRD
func AsOwner(cc *api.CassandraCluster) metav1.OwnerReference <span class="cov0" title="0">{
        trueVar := true
        return metav1.OwnerReference{
                APIVersion: api.SchemeGroupVersion.String(),
                Kind:       "CassandraCluster",
                Name:       cc.Name,
                UID:        cc.UID,
                Controller: &amp;trueVar,
        }
}</span>

// LabelTime returns a supported label string containing the current date and time
func LabelTime() string <span class="cov8" title="1">{
        t := metav1.Now()
        return fmt.Sprintf("%d%02d%02dT%02d%02d%02d",
                t.Year(), t.Month(), t.Day(),
                t.Hour(), t.Minute(), t.Second())
}</span>

// LabelTime2Time converts a label string containing a time into a Time
func LabelTime2Time(label string) (time.Time, error) <span class="cov8" title="1">{
        reformattedLabel := ReLabelTime.ReplaceAllString(label, `${y}-${m}-${d}T${hh}:${mm}:${ss}`)
        return time.Parse("2006-01-02T15:04:05", reformattedLabel)
}</span>

// GetDCRackLabelsForStatefulSet function return a map with the labels DC &amp; Rack to deploy
// on the statefulset.
// dc and int are the indice of respectively the dc and the rack in the CassandraCluster configuration
func GetDCRackLabelsAndNodeSelectorForStatefulSet(cc *api.CassandraCluster, dc int, rack int) (map[string]string, map[string]string) <span class="cov8" title="1">{
        var dcName, rackName string
        var nodeSelector = map[string]string{}

        dcsize := len(cc.Spec.Topology.DC)
        if dcsize &lt; 1 || dc &gt; dcsize-1 </span><span class="cov8" title="1">{
                dcName = api.DefaultCassandraDC
                rackName = api.DefaultCassandraRack
        }</span> else<span class="cov8" title="1"> {
                nodeSelector = MergeLabels(cc.Spec.Topology.DC[dc].Labels)
                dcName = cc.Spec.Topology.DC[dc].Name
                racksize := len(cc.Spec.Topology.DC[dc].Rack)
                if racksize &lt; 1 || rack &gt; racksize-1 </span><span class="cov0" title="0">{
                        rackName = "Rack-1"
                }</span> else<span class="cov8" title="1"> {
                        nodeSelector = MergeLabels(nodeSelector, cc.Spec.Topology.DC[dc].Rack[rack].Labels)
                        rackName = cc.Spec.Topology.DC[dc].Rack[rack].Name
                }</span>
        }

        <span class="cov8" title="1">labels := MergeLabels(LabelsForCassandraDCRack(cc, dcName, rackName), map[string]string{
                "cassandraclusters.db.orange.com.dc":   dcName,
                "cassandraclusters.db.orange.com.rack": rackName,
        })

        return labels, nodeSelector</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
